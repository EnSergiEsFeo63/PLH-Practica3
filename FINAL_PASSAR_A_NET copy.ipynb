{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecaa654b",
   "metadata": {},
   "source": [
    "# Extracción de Entidades Nombradas con CRF\n",
    "\n",
    "En esta práctica, implementaremos un sistema de reconocimiento de entidades nombradas utilizando Conditional Random Fields (CRF). Utilizaremos el corpus CONLL2002 que contiene textos en español y holandés, con anotaciones para cuatro tipos de entidades: LOC (ubicaciones), MISC (miscelánea), ORG (organizaciones), y PER (personas).\n",
    "\n",
    "Seguiremos un enfoque incremental:\n",
    "1. Exploraremos el dataset y su estructura\n",
    "2. Definiremos diferentes conjuntos de características (features)\n",
    "3. Entrenaremos modelos CRF con diferentes configuraciones\n",
    "4. Evaluaremos el rendimiento utilizando diversas métricas\n",
    "5. Optimizaremos nuestro modelo mediante experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe037f9",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial\n",
    "\n",
    "Primero, importaremos las bibliotecas necesarias y cargaremos los datos básicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209b903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargadas 106150 ubicaciones en el gazetteer\n",
      "Cargados 123466 nombres en el gazetteer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Descargar los datos necesarios\n",
    "nltk.download('conll2002')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar gazetteers (listas de entidades conocidas)\n",
    "with open(\"locations.txt\", encoding=\"utf-8\") as f:\n",
    "    locations_set = set(line.strip().lower() for line in f if line.strip())\n",
    "    print(f\"Cargadas {len(locations_set)} ubicaciones en el gazetteer\")\n",
    "\n",
    "with open(\"person_names.txt\", encoding=\"utf-8\") as f:\n",
    "    person_names_set = set(line.strip().lower() for line in f if line.strip())\n",
    "    print(f\"Cargados {len(person_names_set)} nombres en el gazetteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eda511",
   "metadata": {},
   "source": [
    "### Exploración de los datos\n",
    "\n",
    "Veamos cómo están estructurados los datos de CONLL2002 y analicemos algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b35dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Español - Train: 8323 oraciones, Dev: 1915 oraciones, Test: 1517 oraciones\n",
      "Holandés - Train: 15806 oraciones, Dev: 2895 oraciones, Test: 5195 oraciones\n",
      "\n",
      "Ejemplo de oración en español:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Distribución de etiquetas en español (train):\n",
      "O: 231920\n",
      "B-ORG: 7390\n",
      "I-ORG: 4992\n",
      "B-LOC: 4913\n",
      "B-PER: 4321\n",
      "I-PER: 3903\n",
      "I-MISC: 3212\n",
      "B-MISC: 2173\n",
      "I-LOC: 1891\n"
     ]
    }
   ],
   "source": [
    "# Cargar conjuntos de datos en español\n",
    "esp_train = list(conll2002.iob_sents('esp.train'))\n",
    "esp_dev = list(conll2002.iob_sents('esp.testa'))\n",
    "esp_test = list(conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "# Cargar conjuntos de datos en holandés\n",
    "ned_train = list(conll2002.iob_sents('ned.train'))\n",
    "ned_dev = list(conll2002.iob_sents('ned.testa'))\n",
    "ned_test = list(conll2002.iob_sents('ned.testb'))\n",
    "\n",
    "# Mostrar información sobre los conjuntos de datos\n",
    "print(f\"Español - Train: {len(esp_train)} oraciones, Dev: {len(esp_dev)} oraciones, Test: {len(esp_test)} oraciones\")\n",
    "print(f\"Holandés - Train: {len(ned_train)} oraciones, Dev: {len(ned_dev)} oraciones, Test: {len(ned_test)} oraciones\")\n",
    "\n",
    "# Ver un ejemplo de oración\n",
    "print(\"\\nEjemplo de oración en español:\")\n",
    "print(esp_train[0])\n",
    "\n",
    "# Analizar la distribución de etiquetas\n",
    "def count_tags(dataset):\n",
    "    tag_counts = {}\n",
    "    for sent in dataset:\n",
    "        for _, _, tag in sent:\n",
    "            tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "    return tag_counts\n",
    "\n",
    "esp_tags = count_tags(esp_train)\n",
    "print(\"\\nDistribución de etiquetas en español (train):\")\n",
    "for tag, count in sorted(esp_tags.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eaa32c",
   "metadata": {},
   "source": [
    "## 2. Clase para Manejo de Datos\n",
    "\n",
    "Ahora implementaremos la clase `NERDataProcessor` que facilitará la carga y transformación de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cafd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataProcessor:\n",
    "    def __init__(self, language: str = \"spanish\"):\n",
    "        \"\"\"\n",
    "        Inicializa el procesador de datos NER.\n",
    "        \n",
    "        Args:\n",
    "            language: Idioma de los datos ('spanish' o 'dutch')\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga los conjuntos de datos train, dev y test.\n",
    "        \n",
    "        Returns:\n",
    "            Tupla de (train, dev, test)\n",
    "        \"\"\"\n",
    "        if self.language == \"spanish\":\n",
    "            return (\n",
    "                conll2002.iob_sents('esp.train'),\n",
    "                conll2002.iob_sents('esp.testa'),\n",
    "                conll2002.iob_sents('esp.testb')\n",
    "            )\n",
    "        return (\n",
    "            conll2002.iob_sents('ned.train'),\n",
    "            conll2002.iob_sents('ned.testa'),\n",
    "            conll2002.iob_sents('ned.testb')\n",
    "        )\n",
    "    \n",
    "    def convert_to_features(self, data):\n",
    "        \"\"\"\n",
    "        Convierte los datos a formato de características (solo palabra y POS).\n",
    "        \n",
    "        Args:\n",
    "            data: Datos en formato [(palabra, pos, etiqueta), ...]\n",
    "            \n",
    "        Returns:\n",
    "            Lista de oraciones con tokens [(palabra, pos), ...]\n",
    "        \"\"\"\n",
    "        return [[(word, pos) for word, pos, _ in sent] for sent in data]\n",
    "\n",
    "    def get_labels(self, data):\n",
    "        \"\"\"\n",
    "        Extrae las etiquetas de los datos.\n",
    "        \n",
    "        Args:\n",
    "            data: Datos en formato [(palabra, pos, etiqueta), ...]\n",
    "            \n",
    "        Returns:\n",
    "            Lista de oraciones con etiquetas [etiqueta, ...]\n",
    "        \"\"\"\n",
    "        return [[tag for _, _, tag in sent] for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33262b75",
   "metadata": {},
   "source": [
    "### Demostración del procesador de datos\n",
    "\n",
    "Veamos cómo funciona nuestro `NERDataProcessor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fdfd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de oraciones en train: 8323\n",
      "Ejemplo de features para una oración:\n",
      "[('Melbourne', 'NP'), ('(', 'Fpa'), ('Australia', 'NP'), (')', 'Fpt'), (',', 'Fc')]\n",
      "Ejemplo de etiquetas para la misma oración:\n",
      "['B-LOC', 'O', 'B-LOC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Instanciar el procesador para español\n",
    "processor_es = NERDataProcessor(\"spanish\")\n",
    "\n",
    "# Cargar los datos\n",
    "train, dev, test = processor_es.load_data()\n",
    "\n",
    "# Convertir a features y etiquetas\n",
    "X_train = processor_es.convert_to_features(train)\n",
    "y_train = processor_es.get_labels(train)\n",
    "\n",
    "print(f\"Número de oraciones en train: {len(X_train)}\")\n",
    "print(f\"Ejemplo de features para una oración:\")\n",
    "print(X_train[0][:5])  # Primeros 5 tokens de la primera oración\n",
    "print(f\"Ejemplo de etiquetas para la misma oración:\")\n",
    "print(y_train[0][:5])  # Etiquetas correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f62408",
   "metadata": {},
   "source": [
    "## 3. Generador de Características\n",
    "\n",
    "Las características son cruciales para el rendimiento de un modelo CRF. Implementaremos un generador de características flexible que nos permitirá experimentar con diferentes combinaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00454934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFFeatureGenerator:\n",
    "    def __init__(self, feature_config: Dict):\n",
    "        \"\"\"\n",
    "        Inicializa el generador de características.\n",
    "        \n",
    "        Args:\n",
    "            feature_config: Diccionario de configuración con banderas para activar/desactivar grupos de características\n",
    "        \"\"\"\n",
    "        self.config = feature_config\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "        \n",
    "    def get_features(self, tokens: List[Tuple[str, str]], index: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Genera características para un token en una posición específica.\n",
    "        \n",
    "        Args:\n",
    "            tokens: Lista de tokens de la oración [(palabra, pos), ...]\n",
    "            index: Índice del token actual\n",
    "            \n",
    "        Returns:\n",
    "            Lista de características para el token\n",
    "        \"\"\"\n",
    "        word, pos = tokens[index]\n",
    "        features = [\"bias\"]  # Característica base siempre presente\n",
    "        \n",
    "        # --- Características de la palabra ---\n",
    "        if self.config.get(\"word_form\", True):\n",
    "            features.append(f\"word={word}\")\n",
    "            features.append(f\"word.lower={word.lower()}\")\n",
    "\n",
    "        # --- Características de POS y lematización ---\n",
    "        if self.config.get(\"pos\", True):\n",
    "            features.append(f\"pos={pos}\")\n",
    "            features.append(f\"lemma={self.lemmatizer.lemmatize(word.lower())}\")\n",
    "\n",
    "        # --- Características morfológicas ---\n",
    "        if self.config.get(\"morphology\", True):\n",
    "            features.append(f\"is_title={word.istitle()}\")\n",
    "            features.append(f\"is_upper={word.isupper()}\")\n",
    "            features.append(f\"is_digit={word.isdigit()}\")\n",
    "            features.append(f\"has_digit={any(c.isdigit() for c in word)}\")\n",
    "            features.append(f\"has_symbol={not word.isalnum()}\")\n",
    "\n",
    "        # --- Prefijos y sufijos ---\n",
    "        if self.config.get(\"prefix_suffix\", True):\n",
    "            if len(word) >= 3:\n",
    "                features.append(f\"prefix3={word[:3]}\")\n",
    "                features.append(f\"suffix3={word[-3:]}\")\n",
    "            if len(word) >= 2:\n",
    "                features.append(f\"prefix2={word[:2]}\")\n",
    "                features.append(f\"suffix2={word[-2:]}\")\n",
    "\n",
    "        # --- Longitud de la palabra ---\n",
    "        if self.config.get(\"length\", True):\n",
    "            features.append(f\"length={len(word)}\")\n",
    "\n",
    "        # --- Posición en la oración ---\n",
    "        if self.config.get(\"position\", True):\n",
    "            features.append(f\"position={index}\")\n",
    "            features.append(f\"is_first={index == 0}\")\n",
    "            features.append(f\"is_last={index == len(tokens)-1}\")\n",
    "\n",
    "        # --- Contexto circundante ---\n",
    "        if self.config.get(\"context\", True):\n",
    "            # Palabra anterior\n",
    "            if index > 0:\n",
    "                prev_word, prev_pos = tokens[index-1]\n",
    "                features.append(f\"prev_word.lower={prev_word.lower()}\")\n",
    "                features.append(f\"prev_word.istitle={prev_word.istitle()}\")\n",
    "                features.append(f\"prev_word.isdigit={prev_word.isdigit()}\")\n",
    "                features.append(f\"prev_pos={prev_pos}\")\n",
    "            else:\n",
    "                features.append(\"BOS\")  # Beginning of sentence\n",
    "                \n",
    "            # Palabra siguiente\n",
    "            if index < len(tokens)-1:\n",
    "                next_word, next_pos = tokens[index+1]\n",
    "                features.append(f\"next_word.lower={next_word.lower()}\")\n",
    "                features.append(f\"next_word.istitle={next_word.istitle()}\")\n",
    "                features.append(f\"next_word.isdigit={next_word.isdigit()}\")\n",
    "                features.append(f\"next_pos={next_pos}\")\n",
    "            else:\n",
    "                features.append(\"EOS\")  # End of sentence\n",
    "\n",
    "        # --- Características de gazetteers ---\n",
    "        if self.config.get(\"gazetteers\", True):\n",
    "            features.append(f\"in_location_gazetteer={word.lower() in locations_set}\")\n",
    "            features.append(f\"in_person_gazetteer={word.lower() in person_names_set}\")\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dffd12",
   "metadata": {},
   "source": [
    "### Demostración del generador de características\n",
    "\n",
    "Veamos las características que genera nuestro `CRFFeatureGenerator` para algunos tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c63056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra de ejemplo: Melbourne\n",
      "\n",
      "Características completas:\n",
      "  - bias\n",
      "  - word=Melbourne\n",
      "  - word.lower=melbourne\n",
      "  - pos=NP\n",
      "  - lemma=melbourne\n",
      "  - is_title=True\n",
      "  - is_upper=False\n",
      "  - is_digit=False\n",
      "  - has_digit=False\n",
      "  - has_symbol=False\n",
      "  - prefix3=Mel\n",
      "  - suffix3=rne\n",
      "  - prefix2=Me\n",
      "  - suffix2=ne\n",
      "  - length=9\n",
      "\n",
      "Total de características completas: 25\n"
     ]
    }
   ],
   "source": [
    "# Configuración completa de características\n",
    "full_config = {\n",
    "    \"word_form\": True,\n",
    "    \"pos\": True,\n",
    "    \"morphology\": True,\n",
    "    \"prefix_suffix\": True,\n",
    "    \"length\": True,\n",
    "    \"position\": True,\n",
    "    \"context\": True,\n",
    "    \"gazetteers\": True,\n",
    "}\n",
    "\n",
    "# Crear generadores de características\n",
    "full_feature_gen = CRFFeatureGenerator(full_config)\n",
    "# Ejemplo de oración\n",
    "example_tokens = X_train[0]\n",
    "example_word_index = 0  # Índice de ejemplo\n",
    "\n",
    "print(\"Palabra de ejemplo:\", example_tokens[example_word_index][0])\n",
    "\n",
    "# Generar características con ambas configuraciones\n",
    "full_features = full_feature_gen.get_features(example_tokens, example_word_index)\n",
    "\n",
    "print(\"\\nCaracterísticas completas:\")\n",
    "for feat in full_features[:15]:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nTotal de características completas: {len(full_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20995a62",
   "metadata": {},
   "source": [
    "## 4. Modelo CRF y Entrenamiento\n",
    "\n",
    "Ahora implementaremos la clase `CRFModel` que encapsula la lógica de entrenamiento y predicción con CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f02faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFModel:\n",
    "    def __init__(self, feature_generator: CRFFeatureGenerator):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo CRF.\n",
    "        \n",
    "        Args:\n",
    "            feature_generator: Generador de características a utilizar\n",
    "        \"\"\"\n",
    "        self.ct = CRFTagger(feature_func=feature_generator.get_features)\n",
    "        \n",
    "    def train(self, train_sents, train_labels, model_file='model.crf'):\n",
    "        \"\"\"\n",
    "        Entrena el modelo CRF.\n",
    "        \n",
    "        Args:\n",
    "            train_sents: Oraciones de entrenamiento [(palabra, pos), ...]\n",
    "            train_labels: Etiquetas correspondientes [etiqueta, ...]\n",
    "            model_file: Nombre del archivo donde guardar el modelo\n",
    "        \"\"\"\n",
    "        formatted_data = self._format_data(train_sents, train_labels)\n",
    "        self.ct.train(formatted_data, model_file)\n",
    "        print(f\"Modelo entrenado y guardado como '{model_file}'\")\n",
    "        \n",
    "    def predict(self, test_sents):\n",
    "        \"\"\"\n",
    "        Predice etiquetas para oraciones.\n",
    "        \n",
    "        Args:\n",
    "            test_sents: Oraciones para predecir [(palabra, pos), ...]\n",
    "            \n",
    "        Returns:\n",
    "            Lista de oraciones con etiquetas predichas [etiqueta, ...]\n",
    "        \"\"\"\n",
    "        tagged_sents = self.ct.tag_sents(test_sents)\n",
    "        # Extraer solo las etiquetas de las tuplas (palabra, etiqueta)\n",
    "        return [[tag for _, tag in sent] for sent in tagged_sents]\n",
    "    \n",
    "    def _format_data(self, sents, labels):\n",
    "        \"\"\"\n",
    "        Formatea los datos para el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            sents: Oraciones [(palabra, pos), ...]\n",
    "            labels: Etiquetas [etiqueta, ...]\n",
    "            \n",
    "        Returns:\n",
    "            Lista de oraciones formateadas para CRFTagger [(palabra, etiqueta), ...]\n",
    "        \"\"\"\n",
    "        return [list(zip(sent, label)) for sent, label in zip(sents, labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdb16a",
   "metadata": {},
   "source": [
    "### Entrenamiento de un modelo básico\n",
    "\n",
    "Entrenemos un modelo CRF básico para ver cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33b66fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como 'model.crf'\n",
      "\n",
      "Oración 1:\n",
      "Imagínense\tO\n",
      "ustedes\tO\n",
      "que\tO\n",
      "entre\tO\n",
      "aquellos\tO\n",
      "españoles\tO\n",
      ",\tO\n",
      "que\tO\n",
      "fueron\tO\n",
      "quienes\tO\n",
      "llevaron\tO\n",
      "a\tO\n",
      "Europa\tB-LOC\n",
      "esos\tO\n",
      "dones\tO\n",
      "americanos\tO\n",
      ",\tO\n",
      "se\tO\n",
      "hubiera\tO\n",
      "impuesto\tO\n",
      "la\tO\n",
      "patriotería\tO\n",
      "gastronómica\tO\n",
      ":\tO\n",
      "patatas\tO\n",
      "y\tO\n",
      "tomates\tO\n",
      "se\tO\n",
      "hubieran\tO\n",
      "quedado\tO\n",
      "en\tO\n",
      "curiosidades\tO\n",
      "botánicas\tO\n",
      ".\tO\n",
      "\n",
      "Oración 2:\n",
      "No\tO\n",
      "hay\tO\n",
      ",\tO\n",
      "no\tO\n",
      "puede\tO\n",
      "haber\tO\n",
      ",\tO\n",
      "una\tO\n",
      "cocina\tO\n",
      "universal\tO\n",
      ";\tO\n",
      "recuerden\tO\n",
      "ustedes\tO\n",
      "ese\tO\n",
      "horror\tO\n",
      "conocido\tO\n",
      "como\tO\n",
      "'\tO\n",
      "cocina\tO\n",
      "internacional\tO\n",
      "'\tO\n",
      "tan\tO\n",
      "frecuente\tO\n",
      "en\tO\n",
      "restaurantes\tO\n",
      "de\tO\n",
      "hotel\tO\n",
      ".\tO\n",
      "\n",
      "Oración 3:\n",
      "Pero\tO\n",
      "tampoco\tO\n",
      "puede\tO\n",
      "haber\tO\n",
      "una\tO\n",
      "cocina\tO\n",
      "encerrada\tO\n",
      "en\tO\n",
      "sí\tO\n",
      "misma\tO\n",
      ".\tO\n",
      "\n",
      "Oración 4:\n",
      "No\tO\n",
      "cabe\tO\n",
      "preguntarse\tO\n",
      ",\tO\n",
      "ante\tO\n",
      "un\tO\n",
      "nuevo\tO\n",
      "alimento\tO\n",
      ",\tO\n",
      "una\tO\n",
      "nueva\tO\n",
      "especia\tO\n",
      ",\tO\n",
      "¿\tO\n",
      "de\tO\n",
      "dónde\tO\n",
      "viene\tO\n",
      "esto\tO\n",
      "?\tO\n",
      ",\tO\n",
      "sin\tO\n",
      "investigar\tO\n",
      "antes\tO\n",
      "lo\tO\n",
      "verdaderamente\tO\n",
      "importante\tO\n",
      ":\tO\n",
      "¿\tO\n",
      "está\tO\n",
      "rico\tO\n",
      "?\tO\n",
      "\n",
      "Oración 5:\n",
      "Pues\tO\n",
      ",\tO\n",
      "si\tO\n",
      "está\tO\n",
      "rico\tO\n",
      "...\tO\n",
      "adelante\tO\n",
      ",\tO\n",
      "viniere\tO\n",
      "de\tO\n",
      "donde\tO\n",
      "viniere\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "# Configura un modelo básico (solo forma de palabra)\n",
    "basic_config = {\n",
    "    \"word_form\": True,\n",
    "    \"pos\": True,\n",
    "    \"morphology\": True,\n",
    "    \"prefix_suffix\": True,\n",
    "    \"length\": True,\n",
    "    \"position\": True,\n",
    "    \"context\": True,\n",
    "    \"gazetteers\": True,\n",
    "}\n",
    "\n",
    "# Crear generador de características y modelo\n",
    "feature_gen = CRFFeatureGenerator(basic_config)\n",
    "model = CRFModel(feature_gen)\n",
    "\n",
    "# Usar un subconjunto pequeño para la demostración\n",
    "small_X_train = X_train[:100]  # primeras 100 oraciones\n",
    "small_y_train = y_train[:100]\n",
    "\n",
    "# Entrena el modelo\n",
    "model.train(small_X_train, small_y_train)\n",
    "\n",
    "# Predice sobre algunas oraciones\n",
    "small_X_dev = X_train[100:105]  # 5 oraciones adicionales\n",
    "predictions = model.predict(small_X_dev)\n",
    "\n",
    "# Muestra las predicciones\n",
    "for i, (sentence, pred_tags) in enumerate(zip(small_X_dev, predictions)):\n",
    "    print(f\"\\nOración {i+1}:\")\n",
    "    for (word, _), tag in zip(sentence, pred_tags):\n",
    "        print(f\"{word}\\t{tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3da686",
   "metadata": {},
   "source": [
    "## 5. Funciones de Evaluación\n",
    "\n",
    "Para evaluar el rendimiento de nuestros modelos, implementaremos funciones que calculan diversas métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8421f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tags_to_IO(sent_tags):\n",
    "    \"\"\"\n",
    "    Convierte etiquetas al formato IO (B-X -> I-X).\n",
    "    \n",
    "    Args:\n",
    "        sent_tags: Lista de oraciones con etiquetas\n",
    "        \n",
    "    Returns:\n",
    "        Lista de oraciones con etiquetas en formato IO\n",
    "    \"\"\"\n",
    "    return [[tag.replace(\"B-\", \"I-\") for tag in sent] for sent in sent_tags]\n",
    "\n",
    "def entity_finder(sent_tags):\n",
    "    \"\"\"\n",
    "    Encuentra entidades en las oraciones basándose en etiquetas.\n",
    "    \n",
    "    Args:\n",
    "        sent_tags: Lista de oraciones con etiquetas\n",
    "        \n",
    "    Returns:\n",
    "        Lista de entidades encontradas por oración [(tipo, (inicio, fin)), ...]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for sent in sent_tags:\n",
    "        sent_entities = []  # Lista para la oración actual\n",
    "        entities.append(sent_entities)\n",
    "        \n",
    "        current_entity = None\n",
    "        start_idx = None\n",
    "        entity_type = None\n",
    "        \n",
    "        for i, tag in enumerate(sent):\n",
    "            if tag.startswith(\"I-\"):\n",
    "                if current_entity is None:  # Nueva entidad\n",
    "                    current_entity = tag[2:]\n",
    "                    start_idx = i\n",
    "                    entity_type = tag[2:]\n",
    "                elif tag[2:] != entity_type:  # Cambio de tipo\n",
    "                    if current_entity:\n",
    "                        sent_entities.append((entity_type, (start_idx, i-1)))\n",
    "                    current_entity = tag[2:]\n",
    "                    start_idx = i\n",
    "                    entity_type = tag[2:]\n",
    "            else:  # \"O\" u otra etiqueta no-entidad\n",
    "                if current_entity is not None:  # Finalizar entidad\n",
    "                    sent_entities.append((entity_type, (start_idx, i-1)))\n",
    "                    current_entity = None\n",
    "                    start_idx = None\n",
    "                    entity_type = None\n",
    "        \n",
    "        # Si hay una entidad al final de la oración\n",
    "        if current_entity is not None:\n",
    "            sent_entities.append((entity_type, (start_idx, len(sent)-1)))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def evaluate_model(y_true, y_pred, errors=False):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo usando múltiples métricas.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Etiquetas reales\n",
    "        y_pred: Etiquetas predichas\n",
    "        errors: Si es True, devuelve también lista de errores\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con métricas de rendimiento\n",
    "    \"\"\"\n",
    "    info = {'Balanced accuracy': 0.0, 'F1 Score': 0.0, 'Precision': 0.0, 'Recall': 0.0}\n",
    "    \n",
    "    # Convertir a formato IO para análisis consistente\n",
    "    y_true_io = sent_tags_to_IO(y_true)\n",
    "    y_pred_io = sent_tags_to_IO(y_pred)\n",
    "\n",
    "    # Calcular balanced accuracy usando solo la primera letra de la etiqueta (I/O)\n",
    "    def join_sent_tags(sent_tags):\n",
    "        return [tag for sent in sent_tags for tag in sent]\n",
    "\n",
    "    info['Balanced accuracy'] = balanced_accuracy_score(join_sent_tags(y_true_io), join_sent_tags(y_pred_io))\n",
    "\n",
    "    # Encontrar entidades\n",
    "    true_entities = entity_finder(y_true_io)\n",
    "    pred_entities = entity_finder(y_pred_io)\n",
    "\n",
    "    # Contar entidades reales y correctas por tipo\n",
    "    counts = {'LOC': 0, 'MISC': 0, 'ORG': 0, 'PER': 0}\n",
    "    correct_counts = {'LOC': 0, 'MISC': 0, 'ORG': 0, 'PER': 0}\n",
    "    invented = 0\n",
    "\n",
    "    for i, sent in enumerate(true_entities):\n",
    "        sent_true = set(sent)\n",
    "        sent_pred = set(pred_entities[i])\n",
    "        \n",
    "        # Contar entidades reales por tipo\n",
    "        for ent in sent:\n",
    "            counts[ent[0]] += 1\n",
    "            \n",
    "        # Contar entidades correctamente predichas\n",
    "        for ent in sent_pred & sent_true:\n",
    "            correct_counts[ent[0]] += 1\n",
    "            \n",
    "        # Contar entidades inventadas (falsos positivos)\n",
    "        invented += len(sent_pred - sent_true)\n",
    "\n",
    "    # Calcular precisión por tipo de entidad\n",
    "    for ent_type in counts:\n",
    "        total = counts[ent_type]\n",
    "        correct = correct_counts[ent_type]\n",
    "        info[f'{ent_type} correct'] = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # Calcular métricas globales\n",
    "    total_entities = sum(counts.values())\n",
    "    true_positives = sum(correct_counts.values())\n",
    "    false_positives = invented\n",
    "    false_negatives = total_entities - true_positives\n",
    "    \n",
    "    # Precision, Recall y F1\n",
    "    info['Precision'] = true_positives / (true_positives + false_positives) if (true_positives + false_positives) else 0\n",
    "    info['Recall'] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) else 0\n",
    "    \n",
    "    if info['Precision'] + info['Recall'] > 0:\n",
    "        info['F1 Score'] = 2 * (info['Precision'] * info['Recall']) / (info['Precision'] + info['Recall'])\n",
    "    else:\n",
    "        info['F1 Score'] = 0\n",
    "\n",
    "    # Devolver lista de errores si se solicita\n",
    "    if errors:\n",
    "        error_list = []\n",
    "        for i in range(len(true_entities)):\n",
    "            error_list.extend([\n",
    "                (i, ent) for ent in pred_entities[i] if ent not in true_entities[i]\n",
    "            ])\n",
    "        return info, error_list\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de7ed8",
   "metadata": {},
   "source": [
    "### Demostración de evaluación\n",
    "\n",
    "Veamos cómo funcionan nuestras funciones de evaluación con un pequeño ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5dee633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resultats de l'avaluació</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <td>0.516775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.375622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.457576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.318565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC correct</th>\n",
       "      <td>0.466019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC correct</th>\n",
       "      <td>0.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG correct</th>\n",
       "      <td>0.219124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER correct</th>\n",
       "      <td>0.460674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Resultats de l'avaluació\n",
       "Balanced accuracy                  0.516775\n",
       "F1 Score                           0.375622\n",
       "Precision                          0.457576\n",
       "Recall                             0.318565\n",
       "LOC correct                        0.466019\n",
       "MISC correct                       0.225806\n",
       "ORG correct                        0.219124\n",
       "PER correct                        0.460674"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtener algunas etiquetas reales\n",
    "# Usar un ejemplo más grande: 200 oraciones\n",
    "true_tags = y_train[100:300]\n",
    "# Volver a predecir para el mismo rango\n",
    "pred_tags = model.predict(X_train[100:300])\n",
    "\n",
    "# Evaluación\n",
    "eval_results = evaluate_model(true_tags, pred_tags)\n",
    "display(pd.DataFrame([eval_results]).T.rename(columns={0: \"Resultats de l'avaluació\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499a7b4",
   "metadata": {},
   "source": [
    "## 6. Pipeline de Experimentación\n",
    "\n",
    "Ahora crearemos una función que encapsule todo el proceso de experimentación: entrenamiento, predicción y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bb23415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config: Dict, language: str = \"spanish\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Ejecuta un experimento completo: entrenamiento, predicción y evaluación.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuración de características\n",
    "        language: Idioma ('spanish' o 'dutch')\n",
    "        sample_size: Número de oraciones a usar (None para usar todas)\n",
    "        \n",
    "    Returns:\n",
    "        Resultados de evaluación\n",
    "    \"\"\"\n",
    "    processor = NERDataProcessor(language)\n",
    "    train, dev, test = processor.load_data()\n",
    "    \n",
    "    # Limitar el tamaño de la muestra si es necesario\n",
    "    if sample_size:\n",
    "        train = list(train)[:sample_size]\n",
    "        dev = list(dev)[:sample_size//10]\n",
    "    \n",
    "    # Convertir datos\n",
    "    X_train = processor.convert_to_features(train)\n",
    "    y_train = processor.get_labels(train)\n",
    "    X_dev = processor.convert_to_features(dev)\n",
    "    y_dev = processor.get_labels(dev)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    feature_gen = CRFFeatureGenerator(config)\n",
    "    model = CRFModel(feature_gen)\n",
    "\n",
    "    config_str = \"_\".join([k for k, v in config.items() if v])\n",
    "    model_file = f\"model_{config_str}.crf\"\n",
    "    \n",
    "    # Entrenar y predecir\n",
    "    model.train(X_train, y_train, model_file=model_file)\n",
    "    y_pred = model.predict(X_dev)\n",
    "    \n",
    "    # Evaluar\n",
    "    results = evaluate_model(y_dev, y_pred)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6821c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be230c58",
   "metadata": {},
   "source": [
    "### Experimento base\n",
    "\n",
    "Ejecutemos un experimento base con características mínimas para establecer un punto de referencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7596d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como 'model_word_form_morphology_position.crf'\n",
      "Resultados del experimento base:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <td>0.554371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.464832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.484076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC correct</th>\n",
       "      <td>0.462963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC correct</th>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG correct</th>\n",
       "      <td>0.540984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER correct</th>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "Balanced accuracy  0.554371\n",
       "F1 Score           0.464832\n",
       "Precision          0.484076\n",
       "Recall             0.447059\n",
       "LOC correct        0.462963\n",
       "MISC correct       0.100000\n",
       "ORG correct        0.540984\n",
       "PER correct        0.457143"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración básica\n",
    "basic_config = {\n",
    "    \"word_form\": True,\n",
    "    \"morphology\": True,\n",
    "    \"position\": True,\n",
    "    \"pos\": False,\n",
    "    \"prefix_suffix\": False,\n",
    "    \"length\": False,\n",
    "    \"context\": False,\n",
    "    \"gazetteers\": False,\n",
    "}\n",
    "\n",
    "# Ejecutar experimento con muestra pequeña para demostración\n",
    "baseline_results = run_experiment(basic_config, sample_size=1000)\n",
    "\n",
    "print(\"Resultados del experimento base:\")\n",
    "pd.DataFrame([baseline_results]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781acf1",
   "metadata": {},
   "source": [
    "## 7. Experimentación con Diferentes Características\n",
    "\n",
    "Ahora exploraremos cómo afectan diferentes características al rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82b19e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando configuración base...\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology.crf'\n",
      "Probando características individuales...\n",
      "Evaluando configuración con pos\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos.crf'\n",
      "Evaluando configuración con context\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_context.crf'\n",
      "Evaluando configuración con prefix_suffix\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_prefix_suffix.crf'\n",
      "Evaluando configuración con length\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_length.crf'\n",
      "Evaluando configuración con gazetteers\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_gazetteers.crf'\n",
      "Evaluando configuración completa...\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos_context_prefix_suffix_length_gazetteers.crf'\n",
      "Realizando ablación (quitando una característica a la vez)...\n",
      "Evaluando configuración sin pos\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_context_prefix_suffix_length_gazetteers.crf'\n",
      "Evaluando configuración sin context\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos_prefix_suffix_length_gazetteers.crf'\n",
      "Evaluando configuración sin prefix_suffix\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos_context_length_gazetteers.crf'\n",
      "Evaluando configuración sin length\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos_context_prefix_suffix_gazetteers.crf'\n",
      "Evaluando configuración sin gazetteers\n",
      "Modelo entrenado y guardado como 'model_word_form_position_morphology_pos_context_prefix_suffix_length.crf'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tiempo (s)</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>LOC correct</th>\n",
       "      <th>MISC correct</th>\n",
       "      <th>ORG correct</th>\n",
       "      <th>PER correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Configuración</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base</th>\n",
       "      <td>4.987299</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.464832</td>\n",
       "      <td>0.484076</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + pos</th>\n",
       "      <td>5.440558</td>\n",
       "      <td>0.611477</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.535294</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + context</th>\n",
       "      <td>6.913379</td>\n",
       "      <td>0.624429</td>\n",
       "      <td>0.538922</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + prefix_suffix</th>\n",
       "      <td>5.637598</td>\n",
       "      <td>0.609596</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.560510</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + length</th>\n",
       "      <td>5.091350</td>\n",
       "      <td>0.627444</td>\n",
       "      <td>0.513433</td>\n",
       "      <td>0.521212</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + gazetteers</th>\n",
       "      <td>5.464594</td>\n",
       "      <td>0.594577</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Configuración completa</th>\n",
       "      <td>8.390991</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - pos</th>\n",
       "      <td>8.402137</td>\n",
       "      <td>0.704162</td>\n",
       "      <td>0.631268</td>\n",
       "      <td>0.633136</td>\n",
       "      <td>0.629412</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - context</th>\n",
       "      <td>6.760878</td>\n",
       "      <td>0.669249</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.627329</td>\n",
       "      <td>0.594118</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - prefix_suffix</th>\n",
       "      <td>8.180427</td>\n",
       "      <td>0.723128</td>\n",
       "      <td>0.664671</td>\n",
       "      <td>0.676829</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - length</th>\n",
       "      <td>8.503610</td>\n",
       "      <td>0.699094</td>\n",
       "      <td>0.642643</td>\n",
       "      <td>0.656442</td>\n",
       "      <td>0.629412</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - gazetteers</th>\n",
       "      <td>7.909311</td>\n",
       "      <td>0.679585</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Tiempo (s)  Balanced accuracy  F1 Score  Precision  \\\n",
       "Configuración                                                                  \n",
       "Base                        4.987299           0.554371  0.464832   0.484076   \n",
       "Base + pos                  5.440558           0.611477  0.553191   0.572327   \n",
       "Base + context              6.913379           0.624429  0.538922   0.548780   \n",
       "Base + prefix_suffix        5.637598           0.609596  0.538226   0.560510   \n",
       "Base + length               5.091350           0.627444  0.513433   0.521212   \n",
       "Base + gazetteers           5.464594           0.594577  0.536585   0.556962   \n",
       "Configuración completa      8.390991           0.679353  0.634731   0.646341   \n",
       "Completa - pos              8.402137           0.704162  0.631268   0.633136   \n",
       "Completa - context          6.760878           0.669249  0.610272   0.627329   \n",
       "Completa - prefix_suffix    8.180427           0.723128  0.664671   0.676829   \n",
       "Completa - length           8.503610           0.699094  0.642643   0.656442   \n",
       "Completa - gazetteers       7.909311           0.679585  0.612613   0.625767   \n",
       "\n",
       "                            Recall  LOC correct  MISC correct  ORG correct  \\\n",
       "Configuración                                                                \n",
       "Base                      0.447059     0.462963          0.10     0.540984   \n",
       "Base + pos                0.535294     0.574074          0.10     0.672131   \n",
       "Base + context            0.529412     0.537037          0.10     0.639344   \n",
       "Base + prefix_suffix      0.517647     0.555556          0.10     0.622951   \n",
       "Base + length             0.505882     0.518519          0.15     0.606557   \n",
       "Base + gazetteers         0.517647     0.481481          0.05     0.721311   \n",
       "Configuración completa    0.623529     0.666667          0.05     0.803279   \n",
       "Completa - pos            0.629412     0.703704          0.10     0.786885   \n",
       "Completa - context        0.594118     0.685185          0.05     0.737705   \n",
       "Completa - prefix_suffix  0.652941     0.703704          0.15     0.803279   \n",
       "Completa - length         0.629412     0.666667          0.05     0.803279   \n",
       "Completa - gazetteers     0.600000     0.611111          0.15     0.754098   \n",
       "\n",
       "                          PER correct  \n",
       "Configuración                          \n",
       "Base                         0.457143  \n",
       "Base + pos                   0.485714  \n",
       "Base + context               0.571429  \n",
       "Base + prefix_suffix         0.514286  \n",
       "Base + length                0.514286  \n",
       "Base + gazetteers            0.485714  \n",
       "Configuración completa       0.571429  \n",
       "Completa - pos               0.542857  \n",
       "Completa - context           0.514286  \n",
       "Completa - prefix_suffix     0.600000  \n",
       "Completa - length            0.600000  \n",
       "Completa - gazetteers        0.571429  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from time import time\n",
    "\n",
    "def feature_ablation_study(language=\"spanish\", sample_size=1000):\n",
    "    \"\"\"\n",
    "    Realiza un estudio de ablación de características.\n",
    "    \n",
    "    Args:\n",
    "        language: Idioma a utilizar\n",
    "        sample_size: Tamaño de la muestra\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con resultados\n",
    "    \"\"\"\n",
    "    # Características a probar\n",
    "    feature_groups = [\n",
    "        \"pos\",\n",
    "        \"context\",\n",
    "        \"prefix_suffix\",\n",
    "        \"length\",\n",
    "        \"gazetteers\",\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Configuración base siempre presente\n",
    "    base_config = {\n",
    "        \"word_form\": True,  # Siempre incluimos la forma de palabra\n",
    "        \"position\": True,\n",
    "        \"morphology\": True,\n",
    "    }\n",
    "    \n",
    "    # Inicializar todas las características adicionales como False\n",
    "    for feature in feature_groups:\n",
    "        base_config[feature] = False\n",
    "    \n",
    "    # 1. Probar la configuración base\n",
    "    print(\"Evaluando configuración base...\")\n",
    "    start_time = time()\n",
    "    result = run_experiment(base_config.copy(), language=language, sample_size=sample_size)\n",
    "    end_time = time()\n",
    "    \n",
    "    result_with_config = {\n",
    "        \"Configuración\": \"Base\",\n",
    "        \"Tiempo (s)\": end_time - start_time\n",
    "    }\n",
    "    result_with_config.update(result)\n",
    "    results.append(result_with_config)\n",
    "    \n",
    "    # 2. Probar cada característica individual añadida a la base\n",
    "    print(\"Probando características individuales...\")\n",
    "    for feature in feature_groups:\n",
    "        config = base_config.copy()\n",
    "        config[feature] = True\n",
    "        \n",
    "        print(f\"Evaluando configuración con {feature}\")\n",
    "        start_time = time()\n",
    "        result = run_experiment(config, language=language, sample_size=sample_size)\n",
    "        end_time = time()\n",
    "        \n",
    "        result_with_config = {\n",
    "            \"Configuración\": f\"Base + {feature}\",\n",
    "            \"Tiempo (s)\": end_time - start_time\n",
    "        }\n",
    "        result_with_config.update(result)\n",
    "        results.append(result_with_config)\n",
    "    \n",
    "    # 3. Probar la configuración completa\n",
    "    full_config = base_config.copy()\n",
    "    for feature in feature_groups:\n",
    "        full_config[feature] = True\n",
    "    \n",
    "    print(\"Evaluando configuración completa...\")\n",
    "    start_time = time()\n",
    "    result = run_experiment(full_config, language=language, sample_size=sample_size)\n",
    "    end_time = time()\n",
    "    \n",
    "    result_with_config = {\n",
    "        \"Configuración\": \"Configuración completa\",\n",
    "        \"Tiempo (s)\": end_time - start_time\n",
    "    }\n",
    "    result_with_config.update(result)\n",
    "    results.append(result_with_config)\n",
    "    \n",
    "    # 4. Ablación real: quitar una característica a la vez de la configuración completa\n",
    "    print(\"Realizando ablación (quitando una característica a la vez)...\")\n",
    "    for feature in feature_groups:\n",
    "        config = full_config.copy()\n",
    "        config[feature] = False\n",
    "        \n",
    "        print(f\"Evaluando configuración sin {feature}\")\n",
    "        start_time = time()\n",
    "        result = run_experiment(config, language=language, sample_size=sample_size)\n",
    "        end_time = time()\n",
    "        \n",
    "        result_with_config = {\n",
    "            \"Configuración\": f\"Completa - {feature}\",\n",
    "            \"Tiempo (s)\": end_time - start_time\n",
    "        }\n",
    "        result_with_config.update(result)\n",
    "        results.append(result_with_config)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.set_index(\"Configuración\")\n",
    "\n",
    "# Ejecutar estudio de ablación con muestra pequeña\n",
    "ablation_results = feature_ablation_study(sample_size=1000)\n",
    "ablation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f419f8",
   "metadata": {},
   "source": [
    "### Análisis de los resultados de ablación\n",
    "\n",
    "Analicemos qué características individuales aportan más al rendimiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "863d3d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Configuración</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Completa - prefix_suffix</th>\n",
       "      <td>0.723128</td>\n",
       "      <td>0.664671</td>\n",
       "      <td>0.676829</td>\n",
       "      <td>0.652941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - pos</th>\n",
       "      <td>0.704162</td>\n",
       "      <td>0.631268</td>\n",
       "      <td>0.633136</td>\n",
       "      <td>0.629412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - length</th>\n",
       "      <td>0.699094</td>\n",
       "      <td>0.642643</td>\n",
       "      <td>0.656442</td>\n",
       "      <td>0.629412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - gazetteers</th>\n",
       "      <td>0.679585</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Configuración completa</th>\n",
       "      <td>0.679353</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.623529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completa - context</th>\n",
       "      <td>0.669249</td>\n",
       "      <td>0.610272</td>\n",
       "      <td>0.627329</td>\n",
       "      <td>0.594118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + length</th>\n",
       "      <td>0.627444</td>\n",
       "      <td>0.513433</td>\n",
       "      <td>0.521212</td>\n",
       "      <td>0.505882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + context</th>\n",
       "      <td>0.624429</td>\n",
       "      <td>0.538922</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + pos</th>\n",
       "      <td>0.611477</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.535294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + prefix_suffix</th>\n",
       "      <td>0.609596</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.560510</td>\n",
       "      <td>0.517647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base + gazetteers</th>\n",
       "      <td>0.594577</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.517647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Base</th>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.464832</td>\n",
       "      <td>0.484076</td>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Balanced accuracy  F1 Score  Precision    Recall\n",
       "Configuración                                                             \n",
       "Completa - prefix_suffix           0.723128  0.664671   0.676829  0.652941\n",
       "Completa - pos                     0.704162  0.631268   0.633136  0.629412\n",
       "Completa - length                  0.699094  0.642643   0.656442  0.629412\n",
       "Completa - gazetteers              0.679585  0.612613   0.625767  0.600000\n",
       "Configuración completa             0.679353  0.634731   0.646341  0.623529\n",
       "Completa - context                 0.669249  0.610272   0.627329  0.594118\n",
       "Base + length                      0.627444  0.513433   0.521212  0.505882\n",
       "Base + context                     0.624429  0.538922   0.548780  0.529412\n",
       "Base + pos                         0.611477  0.553191   0.572327  0.535294\n",
       "Base + prefix_suffix               0.609596  0.538226   0.560510  0.517647\n",
       "Base + gazetteers                  0.594577  0.536585   0.556962  0.517647\n",
       "Base                               0.554371  0.464832   0.484076  0.447059"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizar los resultados clave\n",
    "key_metrics = [\"Balanced accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
    "ablation_results[key_metrics].sort_values(by=\"Balanced accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a8b2d",
   "metadata": {},
   "source": [
    "## 8. Experimentación con Contexto\n",
    "\n",
    "Ahora exploraremos cómo afecta la inclusión de información contextual al rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9adcbcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando: Sin contexto\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length.crf'\n",
      "Evaluando: Con contexto\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_context.crf'\n",
      "Evaluando: Con contexto + posición\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_position_context.crf'\n",
      "Evaluando: Completo (con gazetteers)\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_position_context_gazetteers.crf'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Configuración</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sin contexto</th>\n",
       "      <td>0.616659</td>\n",
       "      <td>0.549849</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.535294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Con contexto</th>\n",
       "      <td>0.661735</td>\n",
       "      <td>0.602985</td>\n",
       "      <td>0.612121</td>\n",
       "      <td>0.594118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Con contexto + posición</th>\n",
       "      <td>0.679585</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completo (con gazetteers)</th>\n",
       "      <td>0.679353</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.623529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Balanced accuracy  F1 Score  Precision    Recall\n",
       "Configuración                                                              \n",
       "Sin contexto                        0.616659  0.549849   0.565217  0.535294\n",
       "Con contexto                        0.661735  0.602985   0.612121  0.594118\n",
       "Con contexto + posición             0.679585  0.612613   0.625767  0.600000\n",
       "Completo (con gazetteers)           0.679353  0.634731   0.646341  0.623529"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def context_experiment(language=\"spanish\", sample_size=500):\n",
    "    \"\"\"\n",
    "    Realiza experimentos con diferentes configuraciones de contexto.\n",
    "    \n",
    "    Args:\n",
    "        language: Idioma a utilizar\n",
    "        sample_size: Tamaño de la muestra\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con resultados\n",
    "    \"\"\"\n",
    "    # Configuración base con mejores características\n",
    "    base_config = {\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": False,\n",
    "        \"context\": False,\n",
    "        \"gazetteers\": False\n",
    "    }\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"Sin contexto\", \"config\": base_config.copy()},\n",
    "        {\"name\": \"Con contexto\", \"config\": {**base_config, \"context\": True}},\n",
    "        {\"name\": \"Con contexto + posición\", \"config\": {**base_config, \"context\": True, \"position\": True}},\n",
    "        {\"name\": \"Completo (con gazetteers)\", \"config\": {**base_config, \"context\": True, \"position\": True, \"gazetteers\": True}}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        print(f\"Evaluando: {cfg['name']}\")\n",
    "        start_time = time()\n",
    "        result = run_experiment(cfg[\"config\"], language=language, sample_size=sample_size)\n",
    "        end_time = time()\n",
    "        \n",
    "        result_with_config = {\n",
    "            \"Configuración\": cfg[\"name\"],\n",
    "            \"Tiempo (s)\": end_time - start_time\n",
    "        }\n",
    "        result_with_config.update(result)\n",
    "        \n",
    "        results.append(result_with_config)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.set_index(\"Configuración\")\n",
    "\n",
    "# Ejecutar experimento de contexto\n",
    "context_results = context_experiment(sample_size=1000)\n",
    "context_results[[\"Balanced accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058d49e",
   "metadata": {},
   "source": [
    "## 9. Comparación entre Idiomas\n",
    "\n",
    "Comparemos el rendimiento de nuestro mejor modelo en español y holandés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6445518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo en español...\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_position_context_gazetteers.crf'\n",
      "Evaluando modelo en holandés...\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_position_context_gazetteers.crf'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tiempo (s)</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>LOC correct</th>\n",
       "      <th>MISC correct</th>\n",
       "      <th>ORG correct</th>\n",
       "      <th>PER correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idioma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Español</th>\n",
       "      <td>8.752378</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holandés</th>\n",
       "      <td>2.508224</td>\n",
       "      <td>0.688394</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tiempo (s)  Balanced accuracy  F1 Score  Precision    Recall  \\\n",
       "Idioma                                                                   \n",
       "Español     8.752378           0.679353  0.634731   0.646341  0.623529   \n",
       "Holandés    2.508224           0.688394  0.608696   0.653333  0.569767   \n",
       "\n",
       "          LOC correct  MISC correct  ORG correct  PER correct  \n",
       "Idioma                                                         \n",
       "Español      0.666667      0.050000     0.803279     0.571429  \n",
       "Holandés     0.615385      0.541667     0.478261     0.692308  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def language_comparison(sample_size=500):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento del modelo en español y holandés.\n",
    "    \n",
    "    Args:\n",
    "        sample_size: Tamaño de la muestra\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con resultados\n",
    "    \"\"\"\n",
    "    # Mejor configuración encontrada\n",
    "    best_config = {\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": True,\n",
    "        \"context\": True,\n",
    "        \"gazetteers\": True\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Español\n",
    "    print(\"Evaluando modelo en español...\")\n",
    "    start_time = time()\n",
    "    es_result = run_experiment(best_config, language=\"spanish\", sample_size=sample_size)\n",
    "    end_time = time()\n",
    "    \n",
    "    es_result_with_meta = {\n",
    "        \"Idioma\": \"Español\",\n",
    "        \"Tiempo (s)\": end_time - start_time\n",
    "    }\n",
    "    es_result_with_meta.update(es_result)\n",
    "    results.append(es_result_with_meta)\n",
    "    \n",
    "    # Holandés\n",
    "    print(\"Evaluando modelo en holandés...\")\n",
    "    start_time = time()\n",
    "    nl_result = run_experiment(best_config, language=\"dutch\", sample_size=sample_size)\n",
    "    end_time = time()\n",
    "    \n",
    "    nl_result_with_meta = {\n",
    "        \"Idioma\": \"Holandés\",\n",
    "        \"Tiempo (s)\": end_time - start_time\n",
    "    }\n",
    "    nl_result_with_meta.update(nl_result)\n",
    "    results.append(nl_result_with_meta)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.set_index(\"Idioma\")\n",
    "\n",
    "# Comparar idiomas\n",
    "language_results = language_comparison(sample_size=1000)\n",
    "language_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b863b6",
   "metadata": {},
   "source": [
    "## 10. Experimento con Datos Completos\n",
    "\n",
    "Finalmente, entrenemos y evaluemos nuestro mejor modelo usando el conjunto completo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d83af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo completo en spanish...\n",
      "Modelo entrenado y guardado como 'model_word_form_pos_morphology_prefix_suffix_length_position_context_gazetteers.crf'\n",
      "Tiempo total: 159.84 segundos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <td>0.786713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.738690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.752882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.725023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC correct</th>\n",
       "      <td>0.794450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC correct</th>\n",
       "      <td>0.418919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG correct</th>\n",
       "      <td>0.706335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER correct</th>\n",
       "      <td>0.807061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "Balanced accuracy  0.786713\n",
       "F1 Score           0.738690\n",
       "Precision          0.752882\n",
       "Recall             0.725023\n",
       "LOC correct        0.794450\n",
       "MISC correct       0.418919\n",
       "ORG correct        0.706335\n",
       "PER correct        0.807061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def full_experiment(language=\"spanish\"):\n",
    "    \"\"\"\n",
    "    Realiza un experimento completo con todos los datos.\n",
    "    \n",
    "    Args:\n",
    "        language: Idioma a utilizar\n",
    "        \n",
    "    Returns:\n",
    "        Resultados de evaluación\n",
    "    \"\"\"\n",
    "    best_config = {\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": True,\n",
    "        \"context\": True,\n",
    "        \"gazetteers\": True\n",
    "    }\n",
    "    \n",
    "    print(f\"Entrenando modelo completo en {language}...\")\n",
    "    \n",
    "    start_time = time()\n",
    "    results = run_experiment(best_config, language=language)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"Tiempo total: {end_time - start_time:.2f} segundos\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar experimento completo (comentado por tiempo de ejecución)\n",
    "full_results = full_experiment()\n",
    "df_full = pd.DataFrame([full_results])\n",
    "display(df_full.sort_values(by=\"F1 Score\", ascending=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce03a58",
   "metadata": {},
   "source": [
    "## 11. Análisis de Errores\n",
    "\n",
    "Analicemos algunos de los errores típicos que comete nuestro modelo para entender sus limitaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29213192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando cargar modelo pre-entrenado...\n",
      "Rendimiento global (F1): 0.6436\n",
      "Entidades con mayor dificultad: ('MISC correct', 0.10256410256410256)\n",
      "\n",
      "Análisis de errores comunes:\n",
      "\n",
      "Error 1:\n",
      "Entidad incorrectamente predicha: ('MISC', (11, 15))\n",
      "Contexto:\n",
      "  de (Real: I-MISC, Pred: O)\n",
      "  la (Real: I-MISC, Pred: O)\n",
      "→ Feria (Real: I-MISC, Pred: B-MISC)\n",
      "→ de (Real: I-MISC, Pred: I-MISC)\n",
      "→ Alfarería. (Real: I-MISC, Pred: I-MISC)\n",
      "→ el (Real: I-MISC, Pred: I-MISC)\n",
      "→ Barro (Real: I-MISC, Pred: I-MISC)\n",
      "  . (Real: O, Pred: O)\n",
      "\n",
      "Error 2:\n",
      "Entidad incorrectamente predicha: ('LOC', (36, 37))\n",
      "Contexto:\n",
      "  Telecom (Real: I-ORG, Pred: I-ORG)\n",
      "  en (Real: O, Pred: O)\n",
      "→ Telesp (Real: B-ORG, Pred: B-LOC)\n",
      "→ Celular (Real: I-ORG, Pred: I-LOC)\n",
      "  , (Real: O, Pred: O)\n",
      "  la (Real: O, Pred: O)\n",
      "\n",
      "Error 3:\n",
      "Entidad incorrectamente predicha: ('PER', (23, 24))\n",
      "Contexto:\n",
      "  Brasil (Real: I-MISC, Pred: I-ORG)\n",
      "  , (Real: O, Pred: O)\n",
      "→ Joao (Real: B-PER, Pred: B-PER)\n",
      "→ Pimienta (Real: I-PER, Pred: I-PER)\n",
      "  da (Real: I-PER, Pred: O)\n",
      "  Veiga (Real: I-PER, Pred: B-PER)\n",
      "\n",
      "Error 4:\n",
      "Entidad incorrectamente predicha: ('PER', (32, 33))\n",
      "Contexto:\n",
      "  Tech (Real: I-ORG, Pred: I-PER)\n",
      "  , (Real: O, Pred: O)\n",
      "→ Florida (Real: B-ORG, Pred: B-PER)\n",
      "→ State (Real: I-ORG, Pred: I-PER)\n",
      "  y (Real: O, Pred: O)\n",
      "  la (Real: O, Pred: O)\n",
      "\n",
      "Error 5:\n",
      "Entidad incorrectamente predicha: ('LOC', (38, 41))\n",
      "Contexto:\n",
      "  Valencia (Real: B-LOC, Pred: I-LOC)\n",
      "  y (Real: O, Pred: O)\n",
      "→ La (Real: B-MISC, Pred: B-LOC)\n",
      "→ Magdalena (Real: I-MISC, Pred: I-LOC)\n",
      "→ de (Real: O, Pred: I-LOC)\n",
      "→ Castellón (Real: B-LOC, Pred: I-LOC)\n",
      "  . (Real: O, Pred: O)\n"
     ]
    }
   ],
   "source": [
    "def error_analysis(language=\"spanish\", sample_size=500):\n",
    "    \"\"\"\n",
    "    Analiza errores típicos del modelo.\n",
    "    \n",
    "    Args:\n",
    "        language: Idioma a utilizar\n",
    "        sample_size: Tamaño de la muestra\n",
    "    \"\"\"\n",
    "    processor = NERDataProcessor(language)\n",
    "    train, dev, test = processor.load_data()\n",
    "    \n",
    "    # Limitar el tamaño para análisis\n",
    "    dev = list(dev)[:sample_size]\n",
    "    \n",
    "    # Convertir datos\n",
    "    X_dev = processor.convert_to_features(dev)\n",
    "    y_dev = processor.get_labels(dev)\n",
    "    \n",
    "    # Mejor configuración\n",
    "    best_config = {\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": True,\n",
    "        \"context\": True,\n",
    "        \"gazetteers\": True\n",
    "    }\n",
    "    \n",
    "    # Configurar y entrenar modelo\n",
    "    feature_gen = CRFFeatureGenerator(best_config)\n",
    "    model = CRFModel(feature_gen)\n",
    "    \n",
    "    # Cargar modelo pre-entrenado o entrenar con muestra pequeña\n",
    "    try:\n",
    "        print(\"Intentando cargar modelo pre-entrenado...\")\n",
    "        model.ct.set_model_file('model.crf')\n",
    "    except:\n",
    "        print(\"Entrenando nuevo modelo...\")\n",
    "        X_train = processor.convert_to_features(list(train)[:1000])\n",
    "        y_train = processor.get_labels(list(train)[:1000])\n",
    "        model.train(X_train, y_train)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred = model.predict(X_dev)\n",
    "    \n",
    "    # Obtener métricas y errores\n",
    "    results, errors = evaluate_model(y_dev, y_pred, errors=True)\n",
    "    \n",
    "    print(f\"Rendimiento global (F1): {results['F1 Score']:.4f}\")\n",
    "    print(f\"Entidades con mayor dificultad: {min(results.items(), key=lambda x: x[1] if x[0].endswith('correct') else 1.0)}\")\n",
    "    \n",
    "    # Analizar algunos errores específicos\n",
    "    print(\"\\nAnálisis de errores comunes:\")\n",
    "    \n",
    "    if not errors:\n",
    "        print(\"No se encontraron errores en la muestra analizada.\")\n",
    "        return\n",
    "    \n",
    "    # Mostrar 5 errores aleatorios\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    sample_errors = random.sample(errors, min(5, len(errors)))\n",
    "    \n",
    "    for i, (sent_idx, entity) in enumerate(sample_errors):\n",
    "        print(f\"\\nError {i+1}:\")\n",
    "        print(f\"Entidad incorrectamente predicha: {entity}\")\n",
    "        \n",
    "        # Reconstruir la oración original\n",
    "        original_sent = dev[sent_idx]\n",
    "        words = [word for word, _, _ in original_sent]\n",
    "        true_tags = [tag for _, _, tag in original_sent]\n",
    "        pred_tags = y_pred[sent_idx]\n",
    "        \n",
    "        # Mostrar contexto de error\n",
    "        start_idx = max(0, entity[1][0] - 2)\n",
    "        end_idx = min(len(words), entity[1][1] + 3)\n",
    "        \n",
    "        print(\"Contexto:\")\n",
    "        for j in range(start_idx, end_idx):\n",
    "            prefix = \"→ \" if (j >= entity[1][0] and j <= entity[1][1]) else \"  \"\n",
    "            print(f\"{prefix}{words[j]} (Real: {true_tags[j]}, Pred: {pred_tags[j]})\")\n",
    "\n",
    "# Ejecutar análisis de errores\n",
    "error_analysis(sample_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6386577",
   "metadata": {},
   "source": [
    "## 12. Conclusiones y Trabajo Futuro\n",
    "\n",
    "En esta práctica, hemos implementado y evaluado un sistema de reconocimiento de entidades nombradas utilizando Conditional Random Fields. Nuestros experimentos han demostrado:\n",
    "\n",
    "1. **Importancia de las características**: Las características morfológicas, prefijos/sufijos y POS han demostrado ser muy importantes para el rendimiento del modelo.\n",
    "\n",
    "2. **Contexto**: La información contextual (palabras anteriores y siguientes) mejora significativamente la precisión de las predicciones.\n",
    "\n",
    "3. **Gazetteers**: El uso de listas de entidades conocidas ayuda, especialmente para tipos específicos como personas y ubicaciones.\n",
    "\n",
    "4. **Diferencias entre idiomas**: Hemos observado patrones diferentes de rendimiento entre español y holandés, lo que sugiere la necesidad de adaptar las características al idioma.\n",
    "\n",
    "**Trabajo futuro**:\n",
    "\n",
    "- Explorar modelos más avanzados como BiLSTM-CRF o transformers\n",
    "- Expandir las listas de gazetteers\n",
    "- Implementar técnicas de validación cruzada\n",
    "- Probar diferentes esquemas de codificación (BIO, BIOES)\n",
    "- Optimizar hiperparámetros del CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe41572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen de experimentos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Configuración básica</th>\n",
       "      <th>Con contexto</th>\n",
       "      <th>Modelo completo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <td>0.611477</td>\n",
       "      <td>0.661735</td>\n",
       "      <td>0.679353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.602985</td>\n",
       "      <td>0.634731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.612121</td>\n",
       "      <td>0.646341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.535294</td>\n",
       "      <td>0.594118</td>\n",
       "      <td>0.623529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Configuración básica  Con contexto  Modelo completo\n",
       "Balanced accuracy              0.611477      0.661735         0.679353\n",
       "F1 Score                       0.553191      0.602985         0.634731\n",
       "Precision                      0.572327      0.612121         0.646341\n",
       "Recall                         0.535294      0.594118         0.623529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resumen de resultados\n",
    "print(\"Resumen de experimentos:\")\n",
    "\n",
    "# Intentar recuperar resultados de experimentos anteriores si están disponibles\n",
    "try:\n",
    "    results_summary = pd.DataFrame({\n",
    "        \"Configuración básica\": ablation_results.loc[\"Base + pos\"][key_metrics],\n",
    "        \"Con contexto\": context_results.loc[\"Con contexto\"][key_metrics],\n",
    "        \"Modelo completo\": context_results.loc[\"Completo (con gazetteers)\"][key_metrics]\n",
    "    })\n",
    "    display(results_summary)\n",
    "except:\n",
    "    print(\"Ejecuta los experimentos anteriores para ver un resumen comparativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7730336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de oración original (español):\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Ejemplo de oración en formato IO:\n",
      "[('Melbourne', 'NP', 'I-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'I-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'I-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def train_to_IO(train):\n",
    "    '''\n",
    "    Convert the train to IO format while preserving POS tags\n",
    "    '''\n",
    "    train_io = []\n",
    "    for sent in train:\n",
    "        sent_io = []\n",
    "        for word, pos, tag in sent:\n",
    "            new_tag = re.sub(r'\\bB-', 'I-', tag)\n",
    "            sent_io.append((word, pos, new_tag))\n",
    "        train_io.append(sent_io)\n",
    "    return train_io\n",
    "\n",
    "esp_train_IO = train_to_IO(esp_train)\n",
    "ned_train_IO = train_to_IO(ned_train)\n",
    "\n",
    "# Imprimir un ejemplo de la conversión a IO\n",
    "print(\"Ejemplo de oración original (español):\")\n",
    "print(esp_train[0])\n",
    "print(\"\\nEjemplo de oración en formato IO:\")\n",
    "print(esp_train_IO[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "372b7341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de oración original (español):\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Ejemplo de oración en formato BIOE:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def train_to_BIOE(train):\n",
    "    '''\n",
    "    Convert the train to BIOE format while preserving POS tags\n",
    "    '''\n",
    "    train_bioe = []\n",
    "    for sent in train:\n",
    "        sent_bioe = []\n",
    "        for i in range(len(sent)):\n",
    "            word, pos, tag = sent[i]\n",
    "            if tag.startswith('I-') and (i == len(sent) - 1 or not sent[i + 1][2].startswith('I-')):\n",
    "                tag = re.sub(r'\\bI-', 'E-', tag)\n",
    "            sent_bioe.append((word, pos, tag))\n",
    "        train_bioe.append(sent_bioe)\n",
    "    return train_bioe\n",
    "\n",
    "esp_train_BIOE = train_to_BIOE(esp_train)  # Original BIO data\n",
    "esp_train_BIOS = train_to_BIOE(esp_train)  # Original BIO data\n",
    "\n",
    "# Imprimir un ejemplo de la conversión a BIOE\n",
    "print(\"Ejemplo de oración original (español):\")\n",
    "print(esp_train[0])\n",
    "print(\"\\nEjemplo de oración en formato BIOE:\")\n",
    "print(esp_train_BIOE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "863d14ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de oración original (español):\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Ejemplo de oración en formato BIOS:\n",
      "[('Melbourne', 'NP', 'S-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'S-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'S-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def train_to_BIOS(train):\n",
    "    '''\n",
    "    Convert the train to BIOS format while preserving POS tags\n",
    "    '''\n",
    "    train_bios = []\n",
    "    for sent in train:\n",
    "        sent_bios = []\n",
    "        for i in range(len(sent)):\n",
    "            word, pos, tag = sent[i]\n",
    "            if tag.startswith('B-') and (i == len(sent) - 1 or sent[i + 1][2] == 'O'):\n",
    "                tag = re.sub(r'\\bB-', 'S-', tag)\n",
    "            sent_bios.append((word, pos, tag))\n",
    "        train_bios.append(sent_bios)\n",
    "    return train_bios\n",
    "\n",
    "esp_train_BIOS = train_to_BIOS(esp_train)\n",
    "ned_train_BIOS = train_to_BIOS(ned_train)\n",
    "\n",
    "# Imprimir un ejemplo de la conversión a BIOS\n",
    "print(\"Ejemplo de oración original (español):\")\n",
    "print(esp_train[0])\n",
    "print(\"\\nEjemplo de oración en formato BIOS:\")\n",
    "print(esp_train_BIOS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d226a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de oración original (español):\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "\n",
      "Ejemplo de oración en formato BIOES:\n",
      "[('Melbourne', 'NP', 'S-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'S-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'S-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def train_to_BIOES(train):\n",
    "    '''Convert BIO to BIOES directly'''\n",
    "    train_bioes = []\n",
    "    for sent in train:\n",
    "        sent_bioes = []\n",
    "        for i in range(len(sent)):\n",
    "            word, pos, tag = sent[i]\n",
    "            # Single-token entity (B-X followed by non-entity)\n",
    "            if tag.startswith('B-') and (i == len(sent) - 1 or not sent[i + 1][2].startswith('I-')):\n",
    "                tag = re.sub(r'\\bB-', 'S-', tag)\n",
    "            # End of multi-token entity\n",
    "            elif tag.startswith('I-') and (i == len(sent) - 1 or not sent[i + 1][2].startswith('I-')):\n",
    "                tag = re.sub(r'\\bI-', 'E-', tag)\n",
    "            sent_bioes.append((word, pos, tag))\n",
    "        train_bioes.append(sent_bioes)\n",
    "    return train_bioes\n",
    "\n",
    "esp_train_BIOES = train_to_BIOES(esp_train)\n",
    "ned_train_BIOES = train_to_BIOES(ned_train)\n",
    "\n",
    "# Imprimir un ejemplo de la conversión a BIOES\n",
    "print(\"Ejemplo de oración original (español):\")\n",
    "print(esp_train[0])\n",
    "print(\"\\nEjemplo de oración en formato BIOES:\")\n",
    "print(esp_train_BIOES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9e149f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_finder_universal(sent_tags):\n",
    "    \"\"\"\n",
    "    Finds entities in sentences based on tags in any encoding scheme (BIO, IO, BIOE, BIOS, BIOES).\n",
    "    \n",
    "    Args:\n",
    "        sent_tags: List of sentences with tags\n",
    "        \n",
    "    Returns:\n",
    "        List of entities found per sentence [(type, (start, end)), ...]\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for sent in sent_tags:\n",
    "        sent_entities = []  # List for current sentence\n",
    "        entities.append(sent_entities)\n",
    "        \n",
    "        current_entity = None\n",
    "        start_idx = None\n",
    "        entity_type = None\n",
    "        \n",
    "        for i, tag in enumerate(sent):\n",
    "            # Check if this is part of an entity (any entity tag except \"O\")\n",
    "            if tag != \"O\" and (\"-\" in tag):\n",
    "                prefix, entity = tag.split(\"-\", 1)\n",
    "                \n",
    "                # Start of entity\n",
    "                if prefix in [\"B\", \"S\"] or (prefix == \"I\" and current_entity is None):\n",
    "                    # If we were already tracking an entity, add it to results\n",
    "                    if current_entity is not None:\n",
    "                        sent_entities.append((entity_type, (start_idx, i-1)))\n",
    "                    \n",
    "                    # Start tracking new entity\n",
    "                    current_entity = entity\n",
    "                    start_idx = i\n",
    "                    entity_type = entity\n",
    "                \n",
    "                # Inside entity but type changed\n",
    "                elif prefix == \"I\" and entity != current_entity:\n",
    "                    if current_entity is not None:\n",
    "                        sent_entities.append((entity_type, (start_idx, i-1)))\n",
    "                    current_entity = entity\n",
    "                    start_idx = i\n",
    "                    entity_type = entity\n",
    "                \n",
    "                # End of entity\n",
    "                elif prefix == \"E\":\n",
    "                    if current_entity is not None:\n",
    "                        sent_entities.append((entity_type, (start_idx, i)))\n",
    "                        current_entity = None\n",
    "                        start_idx = None\n",
    "                        entity_type = None\n",
    "                    \n",
    "                # Single token entity\n",
    "                if prefix == \"S\":\n",
    "                    sent_entities.append((entity, (i, i)))\n",
    "                    current_entity = None\n",
    "                    start_idx = None\n",
    "                    entity_type = None\n",
    "                    \n",
    "            # Not part of an entity\n",
    "            else:  \n",
    "                if current_entity is not None:  # End previous entity\n",
    "                    sent_entities.append((entity_type, (start_idx, i-1)))\n",
    "                    current_entity = None\n",
    "                    start_idx = None\n",
    "                    entity_type = None\n",
    "        \n",
    "        # If there's an entity at the end of the sentence\n",
    "        if current_entity is not None:\n",
    "            sent_entities.append((entity_type, (start_idx, len(sent)-1)))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def evaluate_model_universal(y_true, y_pred, errors=False):\n",
    "    \"\"\"\n",
    "    Evaluates model performance using multiple metrics with any encoding scheme.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        errors: If True, also returns error list\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    info = {'Balanced accuracy': 0.0, 'F1 Score': 0.0, 'Precision': 0.0, 'Recall': 0.0}\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    def join_sent_tags(sent_tags):\n",
    "        return [tag for sent in sent_tags for tag in sent]\n",
    "\n",
    "    info['Balanced accuracy'] = balanced_accuracy_score(join_sent_tags(y_true), join_sent_tags(y_pred))\n",
    "\n",
    "    # Find entities using the universal entity finder\n",
    "    true_entities = entity_finder_universal(y_true)\n",
    "    pred_entities = entity_finder_universal(y_pred)\n",
    "\n",
    "    # Count real and correctly predicted entities by type\n",
    "    counts = {'LOC': 0, 'MISC': 0, 'ORG': 0, 'PER': 0}\n",
    "    correct_counts = {'LOC': 0, 'MISC': 0, 'ORG': 0, 'PER': 0}\n",
    "    invented = 0\n",
    "\n",
    "    for i, sent in enumerate(true_entities):\n",
    "        sent_true = set(sent)\n",
    "        sent_pred = set(pred_entities[i])\n",
    "        \n",
    "        # Count real entities by type\n",
    "        for ent in sent:\n",
    "            counts[ent[0]] += 1\n",
    "            \n",
    "        # Count correctly predicted entities\n",
    "        for ent in sent_pred & sent_true:\n",
    "            correct_counts[ent[0]] += 1\n",
    "            \n",
    "        # Count invented entities (false positives)\n",
    "        invented += len(sent_pred - sent_true)\n",
    "\n",
    "    # Calculate precision by entity type\n",
    "    for ent_type in counts:\n",
    "        total = counts[ent_type]\n",
    "        correct = correct_counts[ent_type]\n",
    "        info[f'{ent_type} correct'] = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # Calculate global metrics\n",
    "    total_entities = sum(counts.values())\n",
    "    true_positives = sum(correct_counts.values())\n",
    "    false_positives = invented\n",
    "    false_negatives = total_entities - true_positives\n",
    "    \n",
    "    # Precision, Recall and F1\n",
    "    info['Precision'] = true_positives / (true_positives + false_positives) if (true_positives + false_positives) else 0\n",
    "    info['Recall'] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) else 0\n",
    "    \n",
    "    if info['Precision'] + info['Recall'] > 0:\n",
    "        info['F1 Score'] = 2 * (info['Precision'] * info['Recall']) / (info['Precision'] + info['Recall'])\n",
    "    else:\n",
    "        info['F1 Score'] = 0\n",
    "\n",
    "    # Return error list if requested\n",
    "    if errors:\n",
    "        error_list = []\n",
    "        for i in range(len(true_entities)):\n",
    "            error_list.extend([\n",
    "                (i, ent) for ent in pred_entities[i] if ent not in true_entities[i]\n",
    "            ])\n",
    "        return info, error_list\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "00174b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>LOC correct</th>\n",
       "      <th>MISC correct</th>\n",
       "      <th>ORG correct</th>\n",
       "      <th>PER correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IO</th>\n",
       "      <td>0.367282</td>\n",
       "      <td>0.609563</td>\n",
       "      <td>0.638721</td>\n",
       "      <td>0.582950</td>\n",
       "      <td>0.658883</td>\n",
       "      <td>0.204494</td>\n",
       "      <td>0.572941</td>\n",
       "      <td>0.673486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOE</th>\n",
       "      <td>0.547884</td>\n",
       "      <td>0.628394</td>\n",
       "      <td>0.652324</td>\n",
       "      <td>0.606158</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.624706</td>\n",
       "      <td>0.667758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOS</th>\n",
       "      <td>0.561564</td>\n",
       "      <td>0.620277</td>\n",
       "      <td>0.646560</td>\n",
       "      <td>0.596048</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.641571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOES</th>\n",
       "      <td>0.526705</td>\n",
       "      <td>0.622408</td>\n",
       "      <td>0.646607</td>\n",
       "      <td>0.599954</td>\n",
       "      <td>0.681218</td>\n",
       "      <td>0.182022</td>\n",
       "      <td>0.633529</td>\n",
       "      <td>0.639935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Balanced accuracy  F1 Score  Precision    Recall  LOC correct  \\\n",
       "IO              0.367282  0.609563   0.638721  0.582950     0.658883   \n",
       "BIOE            0.547884  0.628394   0.652324  0.606158     0.680203   \n",
       "BIOS            0.561564  0.620277   0.646560  0.596048     0.680203   \n",
       "BIOES           0.526705  0.622408   0.646607  0.599954     0.681218   \n",
       "\n",
       "       MISC correct  ORG correct  PER correct  \n",
       "IO         0.204494     0.572941     0.673486  \n",
       "BIOE       0.202247     0.624706     0.667758  \n",
       "BIOS       0.202247     0.617647     0.641571  \n",
       "BIOES      0.182022     0.633529     0.639935  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n",
      "Modelo entrenado y guardado como 'model.crf'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>LOC correct</th>\n",
       "      <th>MISC correct</th>\n",
       "      <th>ORG correct</th>\n",
       "      <th>PER correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IO</th>\n",
       "      <td>0.314452</td>\n",
       "      <td>0.585576</td>\n",
       "      <td>0.633052</td>\n",
       "      <td>0.544725</td>\n",
       "      <td>0.682672</td>\n",
       "      <td>0.449198</td>\n",
       "      <td>0.380466</td>\n",
       "      <td>0.712660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOE</th>\n",
       "      <td>0.306811</td>\n",
       "      <td>0.294614</td>\n",
       "      <td>0.617827</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.010438</td>\n",
       "      <td>0.050802</td>\n",
       "      <td>0.166181</td>\n",
       "      <td>0.496444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOS</th>\n",
       "      <td>0.488673</td>\n",
       "      <td>0.581514</td>\n",
       "      <td>0.623632</td>\n",
       "      <td>0.544725</td>\n",
       "      <td>0.693111</td>\n",
       "      <td>0.461230</td>\n",
       "      <td>0.332362</td>\n",
       "      <td>0.739687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIOES</th>\n",
       "      <td>0.417516</td>\n",
       "      <td>0.581087</td>\n",
       "      <td>0.613424</td>\n",
       "      <td>0.551988</td>\n",
       "      <td>0.697286</td>\n",
       "      <td>0.482620</td>\n",
       "      <td>0.332362</td>\n",
       "      <td>0.741110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Balanced accuracy  F1 Score  Precision    Recall  LOC correct  \\\n",
       "IO              0.314452  0.585576   0.633052  0.544725     0.682672   \n",
       "BIOE            0.306811  0.294614   0.617827  0.193425     0.010438   \n",
       "BIOS            0.488673  0.581514   0.623632  0.544725     0.693111   \n",
       "BIOES           0.417516  0.581087   0.613424  0.551988     0.697286   \n",
       "\n",
       "       MISC correct  ORG correct  PER correct  \n",
       "IO         0.449198     0.380466     0.712660  \n",
       "BIOE       0.050802     0.166181     0.496444  \n",
       "BIOS       0.461230     0.332362     0.739687  \n",
       "BIOES      0.482620     0.332362     0.741110  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir los diferentes esquemas de codificación y sus datasets\n",
    "train_sets_es = [esp_train_IO, esp_train_BIOE, esp_train_BIOS, esp_train_BIOES]\n",
    "train_sets_nl = [ned_train_IO, ned_train_BIOE, ned_train_BIOS, ned_train_BIOES]\n",
    "\n",
    "# Usar un subconjunto pequeño para acelerar la comparación\n",
    "small_train_sets_es = [train_set[:1000] for train_set in train_sets_es]\n",
    "small_train_sets_nl = [train_set[:1000] for train_set in train_sets_nl]\n",
    "\n",
    "# Evaluar el modelo para cada esquema de codificación en español\n",
    "results_es = []\n",
    "processor_es = NERDataProcessor(\"spanish\")\n",
    "dev_es = list(esp_dev)\n",
    "\n",
    "\n",
    "X_dev_es = processor_es.convert_to_features(dev_es)\n",
    "y_dev_es = processor_es.get_labels(dev_es)\n",
    "\n",
    "for train_set in small_train_sets_es:\n",
    "    X_train_es = processor_es.convert_to_features(train_set)\n",
    "    y_train_es = processor_es.get_labels(train_set)\n",
    "    feature_gen = CRFFeatureGenerator({\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": True,\n",
    "        \"context\": True,\n",
    "        \"gazetteers\": True\n",
    "    })\n",
    "    model = CRFModel(feature_gen)\n",
    "    model.train(X_train_es, y_train_es)\n",
    "    y_pred_es = model.predict(X_dev_es)\n",
    "\n",
    "    idx = small_train_sets_es.index(train_set)\n",
    "    if idx == 1:  # BIOE\n",
    "        dev_es_scheme = train_to_BIOE(list(esp_dev))\n",
    "    elif idx == 2:  # BIOS\n",
    "        dev_es_scheme = train_to_BIOS(list(esp_dev))\n",
    "    elif idx == 3:  # BIOES\n",
    "        dev_es_scheme = train_to_BIOES(list(esp_dev))\n",
    "    else:  # IO\n",
    "        dev_es_scheme = list(esp_dev)\n",
    "    X_dev_es = processor_es.convert_to_features(dev_es_scheme)\n",
    "    y_dev_es = processor_es.get_labels(dev_es_scheme)\n",
    "    # Actualizar las características y etiquetas de dev\n",
    "    eval_result = evaluate_model_universal(y_dev_es, y_pred_es)\n",
    "    results_es.append(eval_result)\n",
    "\n",
    "df_results_es = pd.DataFrame(results_es, index=[\"IO\", \"BIOE\", \"BIOS\", \"BIOES\"])\n",
    "display(df_results_es)\n",
    "\n",
    "# Evaluar el modelo para cada esquema de codificación en holandés\n",
    "results_nl = []\n",
    "processor_nl = NERDataProcessor(\"dutch\")\n",
    "dev_nl = list(ned_dev)\n",
    "X_dev_nl = processor_nl.convert_to_features(dev_nl)\n",
    "y_dev_nl = processor_nl.get_labels(dev_nl)\n",
    "\n",
    "for train_set in small_train_sets_nl:\n",
    "    X_train_nl = processor_nl.convert_to_features(train_set)\n",
    "    y_train_nl = processor_nl.get_labels(train_set)\n",
    "    feature_gen = CRFFeatureGenerator({\n",
    "        \"word_form\": True,\n",
    "        \"pos\": True,\n",
    "        \"morphology\": True,\n",
    "        \"prefix_suffix\": True,\n",
    "        \"length\": True,\n",
    "        \"position\": True,\n",
    "        \"context\": True,\n",
    "        \"gazetteers\": True\n",
    "    })\n",
    "    model = CRFModel(feature_gen)\n",
    "    model.train(X_train_nl, y_train_nl)\n",
    "    y_pred_nl = model.predict(X_dev_nl)\n",
    "\n",
    "    # Convert dev data to match training data scheme for Dutch\n",
    "    idx = small_train_sets_nl.index(train_set)\n",
    "    if idx == 1:  # BIOE\n",
    "        dev_nl_scheme = train_to_BIOE(list(ned_dev))\n",
    "    elif idx == 2:  # BIOS\n",
    "        dev_nl_scheme = train_to_BIOS(list(ned_dev))\n",
    "    elif idx == 3:  # BIOES\n",
    "        dev_nl_scheme = train_to_BIOES(list(ned_dev))\n",
    "    else:  # IO\n",
    "        dev_nl_scheme = list(ned_dev)\n",
    "\n",
    "    X_dev_nl = processor_nl.convert_to_features(dev_nl_scheme)\n",
    "    y_dev_nl = processor_nl.get_labels(dev_nl_scheme)\n",
    "\n",
    "    eval_result = evaluate_model_universal(y_dev_nl, y_pred_nl)\n",
    "    results_nl.append(eval_result)\n",
    "\n",
    "df_results_nl = pd.DataFrame(results_nl, index=[\"IO\", \"BIOE\", \"BIOS\", \"BIOES\"])\n",
    "display(df_results_nl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
