{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d191de4",
   "metadata": {},
   "source": [
    "# Extracció d'entitats anomenades (Pràctica 3)\n",
    "\n",
    "En aquesta pràctica implementarem un sistema de reconeixement d'entitats anomenades (NER) utilitzant Conditional Random Fields (CRF) amb el conjunt de dades CONLL2002 per a espanyol i neerlandès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5edf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Carreguem les dades\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd68b3",
   "metadata": {},
   "source": [
    "## 1. Exploració de les dades\n",
    "\n",
    "Primer analitzarem l'estructura de les dades CONLL2002 per entendre millor el format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0718f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de dades en espanyol:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O')]\n",
      "\n",
      "Exemple de dades en neerlandès:\n",
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O')]\n",
      "\n",
      "Exemple de dades en neerlandès:\n",
      "[('De', 'Art', 'O'), ('tekst', 'N', 'O'), ('van', 'Prep', 'O'), ('het', 'Art', 'O'), ('arrest', 'N', 'O'), ('is', 'V', 'O'), ('nog', 'Adv', 'O'), ('niet', 'Adv', 'O'), ('schriftelijk', 'Adj', 'O'), ('beschikbaar', 'Adj', 'O')]\n",
      "\n",
      "Mida dels datasets:\n",
      "[('De', 'Art', 'O'), ('tekst', 'N', 'O'), ('van', 'Prep', 'O'), ('het', 'Art', 'O'), ('arrest', 'N', 'O'), ('is', 'V', 'O'), ('nog', 'Adv', 'O'), ('niet', 'Adv', 'O'), ('schriftelijk', 'Adj', 'O'), ('beschikbaar', 'Adj', 'O')]\n",
      "\n",
      "Mida dels datasets:\n",
      "Espanyol - Train: 8323 frases, Dev: 1915 frases, Test: 1517 frases\n",
      "Espanyol - Train: 8323 frases, Dev: 1915 frases, Test: 1517 frases\n",
      "Neerlandès - Train: 15806 frases, Dev: 2895 frases, Test: 5195 frases\n",
      "Neerlandès - Train: 15806 frases, Dev: 2895 frases, Test: 5195 frases\n"
     ]
    }
   ],
   "source": [
    "# Mostrem un exemple de les dades d'espanyol\n",
    "print(\"Exemple de dades en espanyol:\")\n",
    "print(list(train_es)[0][:10])  # Primers 10 tokens de la primera frase\n",
    "\n",
    "# Mostrem un exemple de les dades de neerlandès\n",
    "print(\"\\nExemple de dades en neerlandès:\")\n",
    "print(list(train_ned)[0][:10])  # Primers 10 tokens de la primera frase\n",
    "\n",
    "# Comprovem la mida dels datasets\n",
    "print(\"\\nMida dels datasets:\")\n",
    "print(f\"Espanyol - Train: {len(list(train_es))} frases, Dev: {len(list(dev_es))} frases, Test: {len(list(test_es))} frases\")\n",
    "print(f\"Neerlandès - Train: {len(list(train_ned))} frases, Dev: {len(list(dev_ned))} frases, Test: {len(list(test_ned))} frases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c4e65",
   "metadata": {},
   "source": [
    "## 2. Comprenent l'etiquetatge IOB\n",
    "\n",
    "El dataset CONLL2002 utilitza l'esquema d'etiquetatge IOB (Inside, Outside, Beginning):\n",
    "\n",
    "- **B-XXX**: Inici d'una entitat anomenada de tipus XXX\n",
    "- **I-XXX**: Interior (continuació) d'una entitat anomenada de tipus XXX\n",
    "- **O**: Fora de qualsevol entitat anomenada\n",
    "\n",
    "Analitzarem la distribució dels tipus d'entitats en els nostres datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4b3e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m train_ned_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_ned)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Analitzem les dades d'espanyol\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m es_entity_counts, es_total \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_entity_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_es_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTipus d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentitats en espanyol:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity_type, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(es_entity_counts\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36manalyze_entity_types\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      4\u001b[0m total_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, tag \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# Si és una entitat anomenada\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             entity_type \u001b[38;5;241m=\u001b[39m tag[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Eliminem el prefix 'B-' o 'I-'\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Funció per extreure tipus d'entitats i comptar les seves aparicions\n",
    "def analyze_entity_types(dataset):\n",
    "    entity_counts = {}\n",
    "    total_entities = 0\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        for _, tag in sentence:\n",
    "            if tag != 'O':  # Si és una entitat anomenada\n",
    "                entity_type = tag[2:]  # Eliminem el prefix 'B-' o 'I-'\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "                total_entities += 1\n",
    "    \n",
    "    return entity_counts, total_entities\n",
    "\n",
    "# Convertim LazyMap a llista per als datasets espanyol i neerlandès\n",
    "train_es_list = list(train_es)\n",
    "train_ned_list = list(train_ned)\n",
    "\n",
    "# Analitzem les dades d'espanyol\n",
    "es_entity_counts, es_total = analyze_entity_types(train_es_list)\n",
    "print(\"Tipus d'entitats en espanyol:\")\n",
    "for entity_type, count in sorted(es_entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{entity_type}: {count} ({count/es_total*100:.2f}%)\")\n",
    "\n",
    "# Analitzem les dades de neerlandès\n",
    "ned_entity_counts, ned_total = analyze_entity_types(train_ned_list)\n",
    "print(\"\\nTipus d'entitats en neerlandès:\")\n",
    "for entity_type, count in sorted(ned_entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{entity_type}: {count} ({count/ned_total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d965c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display a Spanish example with multiple entity types\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_es:\n\u001b[1;32m----> 9\u001b[0m     entity_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([tag[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m _, tag \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(entity_types) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# Find a sentence with at least 3 different entity types\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         display_tagged_sentence(sent)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Visualitzem una frase etiquetada per entendre millor l'etiquetatge IOB\n",
    "def display_tagged_sentence(sentence, lang=\"Espanyol\"):\n",
    "    print(f\"Exemple de frase etiquetada en {lang}:\")\n",
    "    for word, tag in sentence:\n",
    "        print(f\"{word:<20} {tag}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Mostrem un exemple d'espanyol amb múltiples tipus d'entitats\n",
    "for idx, sent in enumerate(train_es_list):\n",
    "    entity_types = set([tag[2:] for _, tag in sent if tag != 'O'])\n",
    "    if len(entity_types) >= 3:  # Busquem una frase amb almenys 3 tipus d'entitats diferents\n",
    "        display_tagged_sentence(sent)\n",
    "        break\n",
    "\n",
    "# Mostrem un exemple de neerlandès\n",
    "for idx, sent in enumerate(train_ned_list):\n",
    "    entity_types = set([tag[2:] for _, tag in sent if tag != 'O'])\n",
    "    if len(entity_types) >= 3:  # Busquem una frase amb almenys 3 tipus d'entitats diferents\n",
    "        display_tagged_sentence(sent, \"Neerlandès\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9d314",
   "metadata": {},
   "source": [
    "## 3. Funcions de característiques (Feature Functions)\n",
    "\n",
    "Un aspecte clau en els sistemes NER basats en CRF és l'enginyeria de característiques. Definirem diverses funcions de característiques amb diferents nivells de complexitat per avaluar el seu impacte en el rendiment del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc996bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Funció de característiques bàsiques\n",
    "def basic_features(tokens, idx):\n",
    "    \"\"\"Extreu característiques bàsiques per a un token.\"\"\"\n",
    "    token = tokens[idx][0]  # Obtenim el token actual\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'word.lower': token.lower(),\n",
    "        'word.istitle': token.istitle(),\n",
    "        'word.isupper': token.isupper(),\n",
    "        'word.isdigit': token.isdigit()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# 2. Funció de característiques intermèdies\n",
    "def intermediate_features(tokens, idx):\n",
    "    \"\"\"Extreu característiques intermèdies amb context de finestra.\"\"\"\n",
    "    token = tokens[idx][0]\n",
    "    features = basic_features(tokens, idx)\n",
    "    \n",
    "    # Afegim prefix i sufix\n",
    "    features['word.prefix2'] = token[:2] if len(token) > 1 else token\n",
    "    features['word.prefix3'] = token[:3] if len(token) > 2 else token\n",
    "    features['word.suffix2'] = token[-2:] if len(token) > 1 else token\n",
    "    features['word.suffix3'] = token[-3:] if len(token) > 2 else token\n",
    "    \n",
    "    # Característiques de context (finestra)\n",
    "    if idx > 0:\n",
    "        prev_token = tokens[idx-1][0]\n",
    "        features['prev_word'] = prev_token\n",
    "        features['prev_word.lower'] = prev_token.lower()\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "    \n",
    "    if idx < len(tokens) - 1:\n",
    "        next_token = tokens[idx+1][0]\n",
    "        features['next_word'] = next_token\n",
    "        features['next_word.lower'] = next_token.lower()\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 3. Funció de característiques avançades\n",
    "def advanced_features(tokens, idx):\n",
    "    \"\"\"Extreu característiques avançades amb context més ampli i patrons.\"\"\"\n",
    "    token = tokens[idx][0]\n",
    "    features = intermediate_features(tokens, idx)\n",
    "    \n",
    "    # Forma de la paraula\n",
    "    if token.isdigit():\n",
    "        features['word.shape'] = 'number'\n",
    "    elif all(c.isupper() for c in token if c.isalpha()):\n",
    "        features['word.shape'] = 'allcaps'\n",
    "    elif token.istitle():\n",
    "        features['word.shape'] = 'title'\n",
    "    else:\n",
    "        features['word.shape'] = 'other'\n",
    "    \n",
    "    # Patrons de puntuació i caràcters especials\n",
    "    features['word.has_hyphen'] = '-' in token\n",
    "    features['word.has_period'] = '.' in token\n",
    "    features['word.has_number'] = any(c.isdigit() for c in token)\n",
    "    \n",
    "    # Context més ampli (finestra de 2)\n",
    "    if idx > 1:\n",
    "        prev2_token = tokens[idx-2][0]\n",
    "        features['prev2_word'] = prev2_token\n",
    "        features['prev2_word.istitle'] = prev2_token.istitle()\n",
    "    \n",
    "    if idx < len(tokens) - 2:\n",
    "        next2_token = tokens[idx+2][0]\n",
    "        features['next2_word'] = next2_token\n",
    "        features['next2_word.istitle'] = next2_token.istitle()\n",
    "    \n",
    "    # Bigrames\n",
    "    if idx > 0:\n",
    "        features['bigram-1'] = tokens[idx-1][0] + '_' + token\n",
    "    if idx < len(tokens) - 1:\n",
    "        features['bigram+1'] = token + '_' + tokens[idx+1][0]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 4. Funció de característiques específiques per llengua\n",
    "def language_specific_features(tokens, idx, language=\"spanish\"):\n",
    "    \"\"\"Extreu característiques adaptades a una llengua específica.\"\"\"\n",
    "    features = advanced_features(tokens, idx)\n",
    "    token = tokens[idx][0]\n",
    "    \n",
    "    if language == \"spanish\":\n",
    "        # Característiques específiques per espanyol\n",
    "        spanish_common_suffixes = ['ción', 'sión', 'dad', 'tad', 'ismo', 'ista', 'miento']\n",
    "        for suffix in spanish_common_suffixes:\n",
    "            features[f'es_suffix_{suffix}'] = token.lower().endswith(suffix)\n",
    "        \n",
    "        # Possibles indicadors de noms propis en espanyol\n",
    "        features['es_possible_name'] = token.istitle() and not idx == 0\n",
    "        \n",
    "    elif language == \"dutch\":\n",
    "        # Característiques específiques per neerlandès\n",
    "        dutch_common_suffixes = ['ing', 'heid', 'teit', 'atie', 'iek', 'isme']\n",
    "        for suffix in dutch_common_suffixes:\n",
    "            features[f'nl_suffix_{suffix}'] = token.lower().endswith(suffix)\n",
    "        \n",
    "        # Articles i preposicions comunes en neerlandès\n",
    "        common_nl_words = ['de', 'het', 'een', 'van', 'in', 'op', 'aan']\n",
    "        features['nl_common_word'] = token.lower() in common_nl_words\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Creació de funcions d'envoltura per a llengües específiques\n",
    "def spanish_features(tokens, idx):\n",
    "    return language_specific_features(tokens, idx, \"spanish\")\n",
    "\n",
    "def dutch_features(tokens, idx):\n",
    "    return language_specific_features(tokens, idx, \"dutch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55017b2d",
   "metadata": {},
   "source": [
    "## 4. Entrenar models CRF per espanyol\n",
    "\n",
    "Entrarem diversos models CRF per a espanyol utilitzant diferents funcions de característiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25137013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertim les dades IOB al format esperat pel CRFTagger\n",
    "train_es_tagged = list(conll2002.tagged_sents('esp.train'))\n",
    "dev_es_tagged = list(conll2002.tagged_sents('esp.testa'))\n",
    "test_es_tagged = list(conll2002.tagged_sents('esp.testb'))\n",
    "\n",
    "# Utilitzem una mostra més petita per agilitzar l'entrenament\n",
    "train_sample_size = 1000\n",
    "train_es_sample = train_es_tagged[:train_sample_size]\n",
    "\n",
    "print(f\"Mida de la mostra d'entrenament: {len(train_es_sample)} frases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f32fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Funció per entrenar i avaluar un model\n",
    "def train_evaluate_model(feature_func, train_data, test_data, model_name, language=\"Espanyol\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Inicialitzem i entrenem el model\n",
    "    model = nltk.tag.CRFTagger(feature_func=feature_func)\n",
    "    print(f\"Entrenant model {model_name} per a {language}...\")\n",
    "    model.train(train_data, f'{model_name}.mdl')\n",
    "    \n",
    "    # Mesurem el temps d'entrenament\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Entrenament completat en {training_time:.2f} segons\")\n",
    "    \n",
    "    # Avaluem el model\n",
    "    accuracy = model.accuracy(test_data)\n",
    "    print(f\"Precisió del model {model_name}: {accuracy:.4f}\\n\")\n",
    "    \n",
    "    return model, accuracy, training_time\n",
    "\n",
    "# Entrenem models amb diferents funcions de característiques per a espanyol\n",
    "es_models = {\n",
    "    \"es_basic\": basic_features,\n",
    "    \"es_intermediate\": intermediate_features,\n",
    "    \"es_advanced\": advanced_features,\n",
    "    \"es_language_specific\": spanish_features\n",
    "}\n",
    "\n",
    "es_results = {}\n",
    "\n",
    "for name, feature_func in es_models.items():\n",
    "    model, accuracy, train_time = train_evaluate_model(\n",
    "        feature_func, \n",
    "        train_es_sample,\n",
    "        test_es_tagged[:100],  # Utilitzem una mostra petita per a avaluació ràpida\n",
    "        name\n",
    "    )\n",
    "    es_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_time\": train_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098376c3",
   "metadata": {},
   "source": [
    "## 5. Entrenar models CRF per neerlandès\n",
    "\n",
    "Ara aplicarem el mateix enfocament a les dades de neerlandès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12563f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertim les dades IOB de neerlandès al format esperat per CRFTagger\n",
    "train_ned_tagged = list(conll2002.tagged_sents('ned.train'))\n",
    "dev_ned_tagged = list(conll2002.tagged_sents('ned.testa'))\n",
    "test_ned_tagged = list(conll2002.tagged_sents('ned.testb'))\n",
    "\n",
    "# Utilitzem una mostra per a l'entrenament\n",
    "train_ned_sample = train_ned_tagged[:train_sample_size]\n",
    "\n",
    "print(f\"Mida de la mostra d'entrenament: {len(train_ned_sample)} frases\")\n",
    "\n",
    "# Entrenem models amb diferents funcions de característiques per a neerlandès\n",
    "nl_models = {\n",
    "    \"nl_basic\": basic_features,\n",
    "    \"nl_intermediate\": intermediate_features,\n",
    "    \"nl_advanced\": advanced_features,\n",
    "    \"nl_language_specific\": dutch_features\n",
    "}\n",
    "\n",
    "nl_results = {}\n",
    "\n",
    "for name, feature_func in nl_models.items():\n",
    "    model, accuracy, train_time = train_evaluate_model(\n",
    "        feature_func, \n",
    "        train_ned_sample,\n",
    "        test_ned_tagged[:100],  # Utilitzem una mostra petita per a avaluació ràpida\n",
    "        name,\n",
    "        \"Neerlandès\"\n",
    "    )\n",
    "    nl_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_time\": train_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa4cdc",
   "metadata": {},
   "source": [
    "## 6. Implementació de diferents esquemes de codificació\n",
    "\n",
    "Experimentarem amb diferents esquemes de codificació (BIO, BIOW, IO) i analitzarem com afecten el rendiment del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcions per convertir entre diferents esquemes de codificació\n",
    "def convert_bio_to_io(bio_tags):\n",
    "    \"\"\"Converteix codificació BIO a IO\"\"\"\n",
    "    io_tags = []\n",
    "    for tag in bio_tags:\n",
    "        if tag.startswith('B-'):\n",
    "            io_tags.append('I-' + tag[2:])  # Convertim B-TAG a I-TAG\n",
    "        else:\n",
    "            io_tags.append(tag)  # Mantenim I-TAG i O\n",
    "    return io_tags\n",
    "\n",
    "def convert_bio_to_biowe(bio_tags):\n",
    "    \"\"\"Converteix codificació BIO a BIOWE (Beginning, Inside, Outside, End, Whole)\"\"\"\n",
    "    biowe_tags = []\n",
    "    for i, tag in enumerate(bio_tags):\n",
    "        if tag == 'O':\n",
    "            biowe_tags.append('O')\n",
    "        elif tag.startswith('B-'):\n",
    "            entity_type = tag[2:]\n",
    "            # Comprovem si és una entitat d'un sol token\n",
    "            if (i == len(bio_tags) - 1) or not bio_tags[i+1].startswith('I-' + entity_type):\n",
    "                biowe_tags.append('W-' + entity_type)  # Whole entity\n",
    "            else:\n",
    "                biowe_tags.append(tag)  # Beginning\n",
    "        elif tag.startswith('I-'):\n",
    "            entity_type = tag[2:]\n",
    "            # Comprovem si és el final d'una entitat\n",
    "            if (i == len(bio_tags) - 1) or not bio_tags[i+1].startswith('I-' + entity_type):\n",
    "                biowe_tags.append('E-' + entity_type)  # End of entity\n",
    "            else:\n",
    "                biowe_tags.append(tag)  # Inside\n",
    "    return biowe_tags\n",
    "\n",
    "# Funció per convertir un dataset sencer a un esquema diferent\n",
    "def convert_dataset_encoding(dataset, conversion_func):\n",
    "    \"\"\"Converteix un dataset sencer a un esquema de codificació diferent\"\"\"\n",
    "    converted_dataset = []\n",
    "    for sentence in dataset:\n",
    "        words = [word for word, tag in sentence]\n",
    "        tags = [tag for _, tag in sentence]\n",
    "        new_tags = conversion_func(tags)\n",
    "        converted_sentence = list(zip(words, new_tags))\n",
    "        converted_dataset.append(converted_sentence)\n",
    "    return converted_dataset\n",
    "\n",
    "# Convertim una mostra de dades d'espanyol a codificació IO i BIOWE\n",
    "train_es_io = convert_dataset_encoding(train_es_sample, convert_bio_to_io)\n",
    "train_es_biowe = convert_dataset_encoding(train_es_sample, convert_bio_to_biowe)\n",
    "\n",
    "# Convertim les dades de test per a avaluació\n",
    "test_es_io = convert_dataset_encoding(test_es_tagged[:100], convert_bio_to_io)\n",
    "test_es_biowe = convert_dataset_encoding(test_es_tagged[:100], convert_bio_to_biowe)\n",
    "\n",
    "# Entrenem models amb diferents esquemes de codificació\n",
    "encoding_models = {\n",
    "    \"es_io\": (train_es_io, test_es_io),\n",
    "    \"es_biowe\": (train_es_biowe, test_es_biowe)\n",
    "}\n",
    "\n",
    "encoding_results = {}\n",
    "\n",
    "for name, (train_data, test_data) in encoding_models.items():\n",
    "    # Utilitzarem l'extractor de característiques avançat\n",
    "    model, accuracy, train_time = train_evaluate_model(\n",
    "        advanced_features,\n",
    "        train_data,\n",
    "        test_data,\n",
    "        name,\n",
    "        f\"Espanyol ({name.split('_')[1].upper()})\"\n",
    "    )\n",
    "    encoding_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_time\": train_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651e276",
   "metadata": {},
   "source": [
    "## 7. Comparació i visualització de resultats\n",
    "\n",
    "Ara visualitzarem els resultats de tots els nostres experiments per extreure conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollim els resultats de diverses configuracions per visualitzar-los\n",
    "models = []\n",
    "accuracies = []\n",
    "train_times = []\n",
    "descriptions = []\n",
    "languages = []\n",
    "\n",
    "# Resultats espanyol (feature functions)\n",
    "for name, result in es_results.items():\n",
    "    models.append(name)\n",
    "    accuracies.append(result['accuracy'])\n",
    "    train_times.append(result['train_time'])\n",
    "    descriptions.append(name.split('_')[1].capitalize())\n",
    "    languages.append('Espanyol')\n",
    "\n",
    "# Resultats neerlandès (feature functions)\n",
    "for name, result in nl_results.items():\n",
    "    models.append(name)\n",
    "    accuracies.append(result['accuracy'])\n",
    "    train_times.append(result['train_time'])\n",
    "    descriptions.append(name.split('_')[1].capitalize())\n",
    "    languages.append('Neerlandès')\n",
    "\n",
    "# Resultats esquemes de codificació\n",
    "for name, result in encoding_results.items():\n",
    "    models.append(name)\n",
    "    accuracies.append(result['accuracy'])\n",
    "    train_times.append(result['train_time'])\n",
    "    descriptions.append(f\"Advanced ({name.split('_')[1].upper()})\")\n",
    "    languages.append('Espanyol')\n",
    "\n",
    "# Creem un dataframe pels resultats\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracies,\n",
    "    'Training Time (s)': train_times,\n",
    "    'Features': descriptions,\n",
    "    'Language': languages\n",
    "})\n",
    "\n",
    "# Mostrem la taula de resultats\n",
    "display(HTML(results_df.to_html(index=False)))\n",
    "\n",
    "# Gràfic de barres per a la precisió\n",
    "plt.figure(figsize=(14, 6))\n",
    "bar_colors = ['blue' if lang == 'Espanyol' else 'green' for lang in languages]\n",
    "\n",
    "# Utilitzem noms més descriptius pels eixos x\n",
    "x_labels = [f\"{desc} ({lang[:2]})\" for desc, lang in zip(descriptions, languages)]\n",
    "\n",
    "bars = plt.bar(x_labels, accuracies, color=bar_colors, alpha=0.7)\n",
    "\n",
    "# Afegim etiquetes\n",
    "plt.xlabel('Model i característiques')\n",
    "plt.ylabel('Precisió')\n",
    "plt.title('Comparativa de la precisió dels models NER')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Afegim els valors a les barres\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gràfic temps d'entrenament vs precisió\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(train_times, accuracies, c=['blue' if lang == 'Espanyol' else 'green' for lang in languages], \n",
    "                      s=100, alpha=0.7)\n",
    "\n",
    "# Afegim etiquetes als punts\n",
    "for i, (time, acc, name) in enumerate(zip(train_times, accuracies, descriptions)):\n",
    "    plt.annotate(f\"{name} ({languages[i][:2]})\", \n",
    "                 (time, acc), \n",
    "                 xytext=(5, 5), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Temps d\\'entrenament (segons)')\n",
    "plt.ylabel('Precisió')\n",
    "plt.title('Relació entre temps d\\'entrenament i precisió')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fd400",
   "metadata": {},
   "source": [
    "## 8. Avaluació detallada dels millors models\n",
    "\n",
    "Ara farem una avaluació més detallada dels millors models, analitzant les mètriques per tipus d'entitat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funció per avaluar un model més enllà de la simple precisió\n",
    "def detailed_evaluation(model, test_data, lang=\"Espanyol\"):\n",
    "    # Predim etiquetes pels test data\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for sentence in test_data:\n",
    "        # Extraiem només els tokens per a la predicció\n",
    "        tokens = [token for token, _ in sentence]\n",
    "        # Obtenim les etiquetes reals\n",
    "        true_tags = [tag for _, tag in sentence]\n",
    "        # Predim etiquetes\n",
    "        predicted_tags = model.tag([tokens])[0]\n",
    "        # Extraiem només les etiquetes de la predicció\n",
    "        pred_tags = [tag for _, tag in predicted_tags]\n",
    "        \n",
    "        # Ampliem les nostres llistes\n",
    "        y_true.extend(true_tags)\n",
    "        y_pred.extend(pred_tags)\n",
    "    \n",
    "    # Mostrem l'informe de classificació\n",
    "    print(f\"Avaluació detallada del model ({lang}):\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    \n",
    "    # Calculem la precisió\n",
    "    accuracy = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n",
    "    print(f\"Precisió global: {accuracy:.4f}\")\n",
    "    \n",
    "    # Retornem les etiquetes predites per a una anàlisi més detallada\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Avaluem el millor model d'espanyol (escollim el model amb característiques avançades)\n",
    "best_es_model = es_results[\"es_advanced\"][\"model\"]\n",
    "es_true, es_pred = detailed_evaluation(best_es_model, test_es_tagged[:200], \"Espanyol\")\n",
    "\n",
    "# Avaluem el millor model de neerlandès\n",
    "best_nl_model = nl_results[\"nl_advanced\"][\"model\"]\n",
    "nl_true, nl_pred = detailed_evaluation(best_nl_model, test_ned_tagged[:200], \"Neerlandès\")\n",
    "\n",
    "# Avaluem el model amb esquema de codificació BIOWE\n",
    "biowe_model = encoding_results[\"es_biowe\"][\"model\"]\n",
    "biowe_true, biowe_pred = detailed_evaluation(biowe_model, test_es_biowe[:200], \"Espanyol (BIOWE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e922242",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "Basant-nos en els nostres experiments:\n",
    "\n",
    "1. **Impacte de l'enginyeria de característiques**: Les característiques avançades com la forma de la paraula, els afixos i les finestres de context milloren significativament el rendiment dels models NER en ambdues llengües.\n",
    "\n",
    "2. **Diferències entre llengües**: Els models d'espanyol i neerlandès mostren patrons de rendiment diferents, probablement a causa de diferències lingüístiques i propietats específiques de cada idioma.\n",
    "\n",
    "3. **Esquemes de codificació**: La selecció de l'esquema de codificació (BIO vs IO vs BIOWE) afecta el rendiment del model. En general:\n",
    "   - L'esquema BIO proporciona més informació sobre els límits d'entitat\n",
    "   - L'esquema BIOWE ofereix granularitat més fina però augmenta la complexitat\n",
    "   - L'esquema IO és més simple però pot perdre informació sobre els inicis d'entitats\n",
    "\n",
    "4. **Treball futur**: Les millores podrien incloure:\n",
    "   - Experimentar amb conjunts de característiques més complexos\n",
    "   - Utilitzar embeddings de paraules pre-entrenats\n",
    "   - Provar models més avançats d'etiquetatge de seqüències com BiLSTM-CRF\n",
    "   - Implementar tècniques d'augmentació de dades per millorar la generalització"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podem explorar més exemples de prediccions\n",
    "def show_predictions(model, sentences, n=5, language=\"Espanyol\"):\n",
    "    \"\"\"Mostra exemples de prediccions del model\"\"\"\n",
    "    print(f\"\\nExemples de prediccions ({language}):\\n\")\n",
    "    for i, sentence in enumerate(sentences[:n]):\n",
    "        tokens = [word for word, _ in sentence]\n",
    "        true_tags = [tag for _, tag in sentence]\n",
    "        predicted = model.tag([tokens])[0]\n",
    "        pred_tags = [tag for _, tag in predicted]\n",
    "        \n",
    "        print(f\"\\nOració {i+1}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Token':<20} {'Etiqueta real':<15} {'Predicció':<15} {'Correcte':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        for j, (token, true, pred) in enumerate(zip(tokens, true_tags, pred_tags)):\n",
    "            correct = \"✓\" if true == pred else \"✗\"\n",
    "            print(f\"{token:<20} {true:<15} {pred:<15} {correct:<8}\")\n",
    "\n",
    "# Mostrem exemples de prediccions pels millors models\n",
    "show_predictions(best_es_model, test_es_tagged[10:15], language=\"Espanyol\")\n",
    "show_predictions(best_nl_model, test_ned_tagged[10:15], language=\"Neerlandès\")\n",
    "show_predictions(biowe_model, test_es_biowe[10:15], language=\"Espanyol (BIOWE)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
