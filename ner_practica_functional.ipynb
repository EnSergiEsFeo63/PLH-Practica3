{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b556ed92",
   "metadata": {},
   "source": [
    "# Extracció d'entitats anomenades\n",
    "\n",
    "L'objectiu d'aquesta pràctica és fer un reconeixedor d'entitats anomenades amb conditional random fields. També experimentar amb diferents \"feature functions\" i amb diferents tipus de codificacions. L'experimentació s'estructura de la següent forma:\n",
    "\n",
    "- Primer provem diferents tipus de funcions per decidir quines utilitzar\n",
    "- En segon lloc, provem diferents contexts en els quals aplicar aquestes funcions\n",
    "- Després posem a prova ambdós models a les diferents codificacions per escollir-ne una per cada idioma\n",
    "- Un cop els models estiguin fets, els provem a la partició de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5d545",
   "metadata": {},
   "source": [
    "## 1 - Experimentació de \"feature functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3890bd2a",
   "metadata": {},
   "source": [
    "### Import de les dades i preprocessament\n",
    "\n",
    "En primer lloc, importem les dades d'entrenament, validació i test de cada un dels dos idiomes. Aquestes dades venen en la codificació \"BIO\". Més endavant aquests tags es tractaran per provar altres codificacions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b22b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar datasets en español y holandés\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es = conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af62ad3",
   "metadata": {},
   "source": [
    "El preprocessing el dissenyem per experimentar amb diferents combinacions de \"feature functions\", i aplicar-les a diferents contextos dels tokens.\n",
    "\n",
    "Per agilitzar-ho i fer-ho de manera eficient, calcularem les \"feature functions\" de cada token durant el preprocessament. Això té dos avantatges principals:\n",
    "\n",
    "1. En el cas de voler informació del context d'un token, no cal recalcular les funcions, només cal accedir a un índex diferent.\n",
    "2. Per fer \"grid search\" estalviem temps de computació, ja que les funcions es calculen un sol cop.\n",
    "\n",
    "Utilitzarem les 5 funcions bàsiques i afegirem 5 funcions noves:\n",
    "\n",
    "1. Prefixos fins a longitud 3\n",
    "2. El Pos-Tag del token\n",
    "3. La longitud del token\n",
    "4. Si el token és una stop-word\n",
    "5. Si el token es troba en llistes externes de noms, organitzacions o localitzacions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11dbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gazetters():\n",
    "    \"\"\"\n",
    "    Define un conjunt de gazetteers per reconeixement d'entitats.\n",
    "    Aquestes gazetteers inclouen localitzacions comunes, noms de persona, i organitzacions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Llista de localitzacions, noms de persona, i organitzacions\n",
    "    locations_list = {\n",
    "        # Ciutats principals (España i Països Baixos)\n",
    "        \"madrid\", \"barcelona\", \"valencia\", \"sevilla\", \"zaragoza\", \"bilbao\", \"málaga\", \"murcia\", \"palma\",\n",
    "        \"amsterdam\", \"rotterdam\", \"utrecht\", \"eindhoven\", \"groningen\", \"nijmegen\",\n",
    "\n",
    "        # Regions i províncies\n",
    "        \"andalucía\", \"cataluña\", \"galicia\", \"castilla\", \"canarias\", \"baleares\",\n",
    "        \"noord-holland\", \"zuid-holland\", \"gelderland\", \"limburg\",\n",
    "\n",
    "        # Països (hispanoparlants + rellevants propers)\n",
    "        \"españa\", \"méxico\", \"argentina\", \"colombia\", \"perú\", \"chile\", \"ecuador\",\n",
    "        \"nederland\", \"belgië\", \"duitsland\", \"francia\", \"italia\", \"portugal\", \"reino unido\", \"estados unidos\",\n",
    "\n",
    "        # Geografia física\n",
    "        \"mediterráneo\", \"atlántico\", \"pirineos\", \"cantábrico\", \"guadalquivir\", \"ebro\",\n",
    "        \"noordzee\", \"rijn\", \"maas\", \"veluwe\"\n",
    "    }\n",
    "\n",
    "    person_names_list = {\n",
    "        # Noms masculins comuns\n",
    "        \"juan\", \"josé\", \"david\", \"javier\", \"miguel\", \"pedro\", \"sergio\", \"pablo\", \"antonio\",\n",
    "        \"jan\", \"peter\", \"willem\", \"tim\", \"thomas\", \"jeroen\", \"lucas\", \"kees\",\n",
    "\n",
    "        # Noms femenins comuns\n",
    "        \"maría\", \"ana\", \"isabel\", \"laura\", \"marta\", \"lucía\", \"paula\", \"cristina\",\n",
    "        \"maria\", \"johanna\", \"emma\", \"sophie\", \"lisa\", \"lotte\", \"iris\", \"eva\",\n",
    "\n",
    "        # Cognoms comuns\n",
    "        \"garcía\", \"gonzález\", \"rodríguez\", \"fernández\", \"lópez\", \"sánchez\", \"martínez\",\n",
    "        \"de jong\", \"jansen\", \"de vries\", \"van den berg\", \"bakker\", \"visser\", \"meijer\"\n",
    "    }\n",
    "\n",
    "    organizations_list = {\n",
    "        # Empreses destacades\n",
    "        \"telefónica\", \"bbva\", \"iberdrola\", \"mercadona\", \"inditex\", \"seat\",\n",
    "        \"philips\", \"shell\", \"unilever\", \"heineken\", \"ing\", \"asml\",\n",
    "\n",
    "        # Institucions governamentals\n",
    "        \"gobierno\", \"ministerio\", \"ayuntamiento\", \"generalitat\", \"policía nacional\", \"guardia civil\",\n",
    "        \"regering\", \"ministerie\", \"gemeente\", \"belastingdienst\", \"politie\", \"koninklijke marechaussee\",\n",
    "\n",
    "        # Educació i cultura\n",
    "        \"universidad\", \"museo del prado\", \"csic\", \"colegio\", \"hospital\", \"asociación\", \"fundación\",\n",
    "        \"universiteit\", \"hogeschool\", \"tno\", \"knaw\", \"rijksmuseum\", \"omroep\", \"kvk\"\n",
    "    }\n",
    "\n",
    "    return locations_list, person_names_list, organizations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120b70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def pos_tag(word):\n",
    "    tagged = nltk.pos_tag([word])\n",
    "    return tagged[0][1]\n",
    "\n",
    "class FuncPreprocessing:\n",
    "    def __init__(self, language='spanish'):\n",
    "        self._pattern = re.compile(r\"\\d\")\n",
    "        if language == 'spanish':\n",
    "            self.stop_words = set(nltk.corpus.stopwords.words('spanish'))\n",
    "        elif language == 'dutch':\n",
    "            self.stop_words = set(nltk.corpus.stopwords.words('dutch'))\n",
    "        else:\n",
    "            self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "            \n",
    "        # Use gazetteers\n",
    "        self.locations_list, self.person_names_list, self.organizations_list = gazetters()\n",
    "\n",
    "    def __call__(self, corpus: List[List[tuple]]) -> List[List[tuple]]:\n",
    "        \"\"\"\n",
    "        Extract features from a corpus\n",
    "\n",
    "        :param corpus: list of list of tuples (word, tag)\n",
    "        :type corpus: list(list(tuple(str, str)))\n",
    "        :return: list of features\n",
    "        :rtype: list(list(list(str)))\n",
    "        \"\"\"\n",
    "        return [self.find_sent_features(sentence) for sentence in corpus]\n",
    "        \n",
    "    def find_sent_features(self, sentence):\n",
    "        \"\"\"\n",
    "        Extract features from a sentence\n",
    "\n",
    "        :param sentence: list of tuples (word, tag)\n",
    "        :type sentence: list(tuple(str, str))\n",
    "        :return: list of features\n",
    "        :rtype: list(list(str))\n",
    "        \"\"\"\n",
    "        new_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            new_sentence.append(((sentence[i][0], self.get_token_features(sentence, i)), sentence[i][1]))\n",
    "        return new_sentence\n",
    "\n",
    "    def get_token_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features and extra feature about this word\n",
    "        \"\"\"\n",
    "        token = tokens[idx][0]\n",
    "\n",
    "        feature_list = [[], [], []]\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Capitalization\n",
    "        if token[0].isupper():\n",
    "            feature_list[0].append(\"CAPITALIZATION\")\n",
    "\n",
    "        # Number\n",
    "        if re.search(self._pattern, token) is not None:\n",
    "            feature_list[0].append(\"HAS_NUM\")\n",
    "\n",
    "        # Punctuation\n",
    "        punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list[0].append(\"PUNCTUATION\")\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list[0].append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list[0].append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list[0].append(\"SUF_\" + token[-3:])\n",
    "\n",
    "        # Word \n",
    "        feature_list[0].append(\"WORD_\" + token)\n",
    "\n",
    "        # Prefix up to length 3\n",
    "        prefixes = []\n",
    "        if len(token) > 1:\n",
    "            prefixes.append(\"PREF_\" + token[:1])\n",
    "        if len(token) > 2:\n",
    "            prefixes.append(\"PREF_\" + token[:2])\n",
    "        if len(token) > 3:\n",
    "            prefixes.append(\"PREF_\" + token[:3])\n",
    "\n",
    "        feature_list[1].append(prefixes)\n",
    "\n",
    "        # POS\n",
    "        feature_list[1].append(\"POS_\" + pos_tag(token))\n",
    "\n",
    "        # Word length\n",
    "        feature_list[1].append(\"WORD_LENGTH_\" + str(len(token)))\n",
    "\n",
    "        # Stop word\n",
    "        if token.lower() in self.stop_words:\n",
    "            feature_list[1].append(\"STOP_WORD\")\n",
    "        else:\n",
    "            feature_list[1].append(None)\n",
    "\n",
    "        # If capitalized\n",
    "        if token[0].isupper():\n",
    "            # Names\n",
    "            if token.lower() in self.person_names_list:\n",
    "                feature_list[2].append(\"NAME\")\n",
    "\n",
    "            # Locations\n",
    "            if token.lower() in self.locations_list:\n",
    "                feature_list[2].append(\"LOCATION\")\n",
    "                \n",
    "            # Organizations\n",
    "            if token.lower() in self.organizations_list:\n",
    "                feature_list[2].append(\"ORGANIZATION\")\n",
    "\n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d75b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train(corpus: List[List[tuple]], language = 'spanish') -> List[List[tuple]]:\n",
    "    '''\n",
    "    Preprocess the corpus for training\n",
    "\n",
    "    :param corpus: list of list of tuples (word, pos, tag)\n",
    "    :type corpus: list(list(tuple(str, str, str)))\n",
    "    :return: list of features\n",
    "    :rtype: list(list(tuple(tuple(str, list(list(str))), str)))\n",
    "    '''\n",
    "    new_corpus = []\n",
    "    for sentence in corpus:\n",
    "        new_sentence = []\n",
    "        for word_tuple in sentence:\n",
    "            new_word_tuple = ((word_tuple[0], word_tuple[1]), word_tuple[2])\n",
    "            new_sentence.append(new_word_tuple)\n",
    "        new_corpus.append(new_sentence)\n",
    "    new_corpus = FuncPreprocessing(language)(new_corpus)\n",
    "    return new_corpus\n",
    "\n",
    "def sep_token_tag(corpus: List[List[tuple]]) -> tuple[List[List[tuple]], List[List[str]]]:\n",
    "    '''\n",
    "    Separate the tokens and tags from the corpus\n",
    "    \n",
    "    :param corpus: list of list of tuples (word, tag)\n",
    "    :type corpus: list(list(tuple(tuple(str, list(list(str))), str)))\n",
    "    :return: tokens and tags\n",
    "    :rtype: list(list(tuple)), list(list(str))\n",
    "    '''\n",
    "    tokens = [[word[0] for word in sentence] for sentence in corpus]\n",
    "    tags = [[word[1] for word in sentence] for sentence in corpus]\n",
    "    return tokens, tags\n",
    "\n",
    "def prep_test(corpus: List[List[tuple]], language='spanish') ->tuple[List[List[tuple]], List[List[str]]]:\n",
    "    '''\n",
    "    Preprocess the corpus for testing\n",
    "    '''\n",
    "    corpus_prep = prep_train(corpus, language)\n",
    "    tokens, tags = sep_token_tag(corpus_prep)\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3f465b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocesar los datos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_es_prep \u001b[38;5;241m=\u001b[39m \u001b[43mprep_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_es\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspanish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dev_es_tokens, dev_es_tags \u001b[38;5;241m=\u001b[39m prep_test(dev_es, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_es_tokens, test_es_tags \u001b[38;5;241m=\u001b[39m prep_test(test_es, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mprep_train\u001b[1;34m(corpus, language)\u001b[0m\n\u001b[0;32m     15\u001b[0m         new_sentence\u001b[38;5;241m.\u001b[39mappend(new_word_tuple)\n\u001b[0;32m     16\u001b[0m     new_corpus\u001b[38;5;241m.\u001b[39mappend(new_sentence)\n\u001b[1;32m---> 17\u001b[0m new_corpus \u001b[38;5;241m=\u001b[39m \u001b[43mFuncPreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_corpus\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mFuncPreprocessing.__call__\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, corpus: List[List[\u001b[38;5;28mtuple\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mtuple\u001b[39m]]:\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    Extract features from a corpus\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    :rtype: list(list(list(str)))\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_sent_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m corpus]\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mFuncPreprocessing.find_sent_features\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     52\u001b[0m new_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m---> 54\u001b[0m     new_sentence\u001b[38;5;241m.\u001b[39mappend(((sentence[i][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m), sentence[i][\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_sentence\n",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m, in \u001b[0;36mFuncPreprocessing.get_token_features\u001b[1;34m(self, tokens, idx)\u001b[0m\n\u001b[0;32m     70\u001b[0m     feature_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAPITALIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Number\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     feature_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHAS_NUM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Punctuation\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:177\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'tuple'"
     ]
    }
   ],
   "source": [
    "# Preprocesar los datos\n",
    "train_es_prep = prep_train(train_es, 'spanish')\n",
    "dev_es_tokens, dev_es_tags = prep_test(dev_es, 'spanish')\n",
    "test_es_tokens, test_es_tags = prep_test(test_es, 'spanish')\n",
    "\n",
    "train_ned_prep = prep_train(train_ned, 'dutch')\n",
    "dev_ned_tokens, dev_ned_tags = prep_test(dev_ned, 'dutch')\n",
    "test_ned_tokens, test_ned_tags = prep_test(test_ned, 'dutch')\n",
    "\n",
    "# Ver cómo son los datos después del preprocesamiento\n",
    "print(\"Ejemplo de oración después del preprocesamiento:\")\n",
    "print(train_es_prep[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e682a9",
   "metadata": {},
   "source": [
    "### Definició de GetFeatures\n",
    "\n",
    "Per aprofitar el format de les dades preprocessades, creem una classe que guarda:\n",
    "- Un vector \"features_vector\" que indica quines de les 4 primeres funcions extra s'utilitzen\n",
    "- Un booleà \"lists\" que indica si s'utilitzen o no les llistes externes\n",
    "- Un \"word_vector\" que indica quin context de la paraula s'utilitza (fins a dues paraules endavant i enrere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59129b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetFeatures:\n",
    "            \n",
    "    def __init__(self, features_vector = [0, 0, 0, 0], lists = False, word_vector = [0, 0, 1, 0, 0]):\n",
    "        self.features_vector = features_vector\n",
    "        self.lists = lists\n",
    "        self.word_vector = word_vector\n",
    "\n",
    "    def __call__(self, tokens: List[tuple], idx: int) -> List[str]:\n",
    "        '''\n",
    "        Extract features from a token\n",
    "        \n",
    "        :param tokens: list of tuples (word, features)\n",
    "        :type tokens: list(tuple(str, list(list(str))))\n",
    "        :param idx: index of the token\n",
    "        :type idx: int\n",
    "        :return: list of features\n",
    "        :rtype: list(str)\n",
    "        '''\n",
    "        feature_list = []\n",
    "\n",
    "        if not tokens[idx][0]:\n",
    "            return feature_list\n",
    "\n",
    "        # Current word features\n",
    "        if self.word_vector[2] == 1:\n",
    "            features = tokens[idx][1]\n",
    "\n",
    "            # Basic features\n",
    "            for feat in features[0]:\n",
    "                feature_list.append(feat)\n",
    "\n",
    "            # Extra features\n",
    "            if self.features_vector[0] == 1:  # Prefixes\n",
    "                for pre in features[1][0]:\n",
    "                    feature_list.append(pre)\n",
    "            if self.features_vector[1] == 1:  # POS tag\n",
    "                feature_list.append(features[1][1])\n",
    "            if self.features_vector[2] == 1:  # Length\n",
    "                feature_list.append(features[1][2])\n",
    "            if self.features_vector[3] == 1:  # Stop word\n",
    "                if features[1][3]:\n",
    "                    feature_list.append(features[1][3])\n",
    "\n",
    "            # External lists\n",
    "            if self.lists:\n",
    "                for feat in features[2]:\n",
    "                    feature_list.append(feat)\n",
    "            \n",
    "        # 2 words before features\n",
    "        if idx > 1 and self.word_vector[0] == 1:\n",
    "            features = tokens[idx - 2][1]\n",
    "\n",
    "            for feat in features[0]:\n",
    "                feature_list.append(\"2PREV\" + feat)\n",
    "\n",
    "            if self.features_vector[0] == 1:\n",
    "                for pre in features[1][0]:\n",
    "                    feature_list.append(\"2PREV\" + pre)\n",
    "            if self.features_vector[1] == 1:\n",
    "                feature_list.append(\"2PREV\" + features[1][1])\n",
    "            if self.features_vector[2] == 1:\n",
    "                feature_list.append(\"2PREV\" + features[1][2])\n",
    "            if self.features_vector[3] == 1:\n",
    "                if features[1][3]:\n",
    "                    feature_list.append(\"2PREV\" + features[1][3])\n",
    "\n",
    "            if self.lists:\n",
    "                for feat in features[2]:\n",
    "                    feature_list.append(\"2PREV\" + feat)\n",
    "\n",
    "        # Previous word features\n",
    "        if idx > 0 and self.word_vector[1] == 1:\n",
    "            features = tokens[idx - 1][1]\n",
    "\n",
    "            for feat in features[0]:\n",
    "                feature_list.append(\"PREV\" + feat)\n",
    "\n",
    "            if self.features_vector[0] == 1:\n",
    "                for pre in features[1][0]:\n",
    "                    feature_list.append(\"PREV\" + pre)\n",
    "            if self.features_vector[1] == 1:\n",
    "                feature_list.append(\"PREV\" + features[1][1])\n",
    "            if self.features_vector[2] == 1:\n",
    "                feature_list.append(\"PREV\" + features[1][2])\n",
    "            if self.features_vector[3] == 1:\n",
    "                if features[1][3]:\n",
    "                    feature_list.append(\"PREV\" + features[1][3])\n",
    "\n",
    "            if self.lists:\n",
    "                for feat in features[2]:\n",
    "                    feature_list.append(\"PREV\" + feat)\n",
    "\n",
    "        # Next word features\n",
    "        if idx < len(tokens) - 1 and self.word_vector[3] == 1:\n",
    "            features = tokens[idx + 1][1]\n",
    "\n",
    "            for feat in features[0]:\n",
    "                feature_list.append(\"NEXT\" + feat)\n",
    "\n",
    "            if self.features_vector[0] == 1:\n",
    "                for pre in features[1][0]:\n",
    "                    feature_list.append(\"NEXT\" + pre)\n",
    "            if self.features_vector[1] == 1:\n",
    "                feature_list.append(\"NEXT\" + features[1][1])\n",
    "            if self.features_vector[2] == 1:\n",
    "                feature_list.append(\"NEXT\" + features[1][2])\n",
    "            if self.features_vector[3] == 1:\n",
    "                if features[1][3]:\n",
    "                    feature_list.append(\"NEXT\" + features[1][3])\n",
    "\n",
    "            if self.lists:\n",
    "                for feat in features[2]:\n",
    "                    feature_list.append(\"NEXT\" + feat)\n",
    "\n",
    "        # 2 words after features\n",
    "        if idx < len(tokens) - 2 and self.word_vector[4] == 1:\n",
    "            features = tokens[idx + 2][1]\n",
    "\n",
    "            for feat in features[0]:\n",
    "                feature_list.append(\"2NEXT\" + feat)\n",
    "\n",
    "            if self.features_vector[0] == 1:\n",
    "                for pre in features[1][0]:\n",
    "                    feature_list.append(\"2NEXT\" + pre)\n",
    "            if self.features_vector[1] == 1:\n",
    "                feature_list.append(\"2NEXT\" + features[1][1])\n",
    "            if self.features_vector[2] == 1:\n",
    "                feature_list.append(\"2NEXT\" + features[1][2])\n",
    "            if self.features_vector[3] == 1:\n",
    "                if features[1][3]:\n",
    "                    feature_list.append(\"2NEXT\" + features[1][3])\n",
    "\n",
    "            if self.lists:\n",
    "                for feat in features[2]:\n",
    "                    feature_list.append(\"2NEXT\" + feat)\n",
    "\n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787035c",
   "metadata": {},
   "source": [
    "### Funció d'avaluació d'un model\n",
    "\n",
    "Abans d'experimentar amb les features, necessitem establir un criteri d'avaluació pels models. Utilitzarem dos enfocaments:\n",
    "\n",
    "1. La \"accuracy\" balancejada: indica quantes etiquetes predites són correctes\n",
    "2. Anàlisi d'entitats: quin percentatge d'entitats es detecten perfectament, quantes entitats \"s'inventa\" el model\n",
    "\n",
    "Prioritzarem detectar entitats perfectament, que és la tasca principal del model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf9f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import re\n",
    "\n",
    "def sent_tags_to_IO(sent_tags):\n",
    "    '''\n",
    "    Convert sentence tags to IO format\n",
    "    \n",
    "    :param sent_tags: list of tags\n",
    "    :type sent_tags: list(str)\n",
    "    :return: list of IO tags\n",
    "    :rtype: list(str)\n",
    "    '''\n",
    "    io_sent_tags = []\n",
    "    for sent in sent_tags:\n",
    "        tags = [re.sub(r'\\b[BES]-', 'I-', tag) for tag in sent]\n",
    "        io_sent_tags.append(tags)\n",
    "    return io_sent_tags\n",
    "\n",
    "def entity_finder(sent_tags):\n",
    "    '''\n",
    "    Find entities in a sentence\n",
    "\n",
    "    :param sent_tags: list of tags\n",
    "    :type sent_tags: list(str)\n",
    "    :return: list of entities\n",
    "    :rtype: list(tuple(str, tuple(int, int)))\n",
    "    '''\n",
    "    sent_tags_io = sent_tags_to_IO(sent_tags)\n",
    "\n",
    "    entities = []\n",
    "    for sent in sent_tags_io:\n",
    "        sent_entities = []\n",
    "        last = sent[0]\n",
    "        ini = None\n",
    "        end = None\n",
    "        type_ent = None\n",
    "        if last[0] == 'I':\n",
    "            ini = 0\n",
    "            end = 0\n",
    "            type_ent = last[2:]\n",
    "        for i in range(1, len(sent)):\n",
    "            tag = sent[i]\n",
    "            if tag[0] == 'I':\n",
    "                if last == 'O':\n",
    "                    ini = i\n",
    "                    end = i\n",
    "                    type_ent = tag[2:]\n",
    "                if last[0] == 'I':\n",
    "                    end = i\n",
    "                last = tag\n",
    "            else:\n",
    "                if last[0] == 'I':\n",
    "                    sent_entities.append((type_ent, (ini, end)))\n",
    "                    ini = None\n",
    "                    end = None\n",
    "                    type_ent = None\n",
    "                last = tag\n",
    "        if ini:\n",
    "            sent_entities.append((type_ent, (ini, end)))\n",
    "        entities.append(sent_entities)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def evaluate_tagger_performance(sent_real, sent_pred, features=None, errors=False):\n",
    "    '''\n",
    "    Evaluate the performance of a tagger\n",
    "\n",
    "    :param sent_real: list of real tags\n",
    "    :type sent_real: list(list(str))\n",
    "    :param sent_pred: list of predicted tags\n",
    "    :type sent_pred: list(list(str))\n",
    "    :param features: identifier for the features used\n",
    "    :param errors: return errors\n",
    "    :type errors: bool\n",
    "    '''\n",
    "\n",
    "    info = {'Codification': features, 'Balanced accuracy': 0, 'Total entities': 0, 'Entities correct': 0, \n",
    "            'LOC correct': 0, 'MISC correct': 0, 'ORG correct': 0, 'PER correct': 0, 'Entities invented': 0}\n",
    "    \n",
    "    sent_io_real = sent_tags_to_IO(sent_real)\n",
    "    sent_io_pred = sent_tags_to_IO(sent_pred)\n",
    "\n",
    "    def join_sent_tags(sent_tags):\n",
    "        return [tag[0] for sent in sent_tags for tag in sent]\n",
    "\n",
    "    # Calculate balanced accuracy\n",
    "    info['Balanced accuracy'] = balanced_accuracy_score(join_sent_tags(sent_io_real), join_sent_tags(sent_io_pred))\n",
    "\n",
    "    real_entities = entity_finder(sent_io_real)\n",
    "    pred_entities = entity_finder(sent_io_pred)\n",
    "\n",
    "    total_ent = 0\n",
    "    total_loc = 0\n",
    "    total_misc = 0\n",
    "    total_org = 0\n",
    "    total_per = 0\n",
    "\n",
    "    for sent in real_entities:\n",
    "        for entity in sent:\n",
    "            total_ent += 1\n",
    "            if entity[0] == 'LOC':\n",
    "                total_loc += 1\n",
    "            if entity[0] == 'MISC':\n",
    "                total_misc += 1\n",
    "            if entity[0] =='ORG':\n",
    "                total_org += 1\n",
    "            if entity[0] == 'PER':\n",
    "                total_per += 1\n",
    "\n",
    "    info['Total entities'] = total_ent\n",
    "\n",
    "    good_ent = 0\n",
    "    good_loc = 0\n",
    "    good_misc = 0\n",
    "    good_org = 0\n",
    "    good_per = 0\n",
    "    invented_ent = 0\n",
    "\n",
    "    for i in range(0, len(real_entities)):\n",
    "        for entity in pred_entities[i]:\n",
    "            if entity in real_entities[i]:\n",
    "                good_ent += 1\n",
    "                if entity[0] == 'LOC':\n",
    "                    good_loc += 1\n",
    "                if entity[0] == 'MISC':\n",
    "                    good_misc += 1\n",
    "                if entity[0] =='ORG':\n",
    "                    good_org += 1\n",
    "                if entity[0] == 'PER':\n",
    "                    good_per += 1\n",
    "            else:\n",
    "                invented_ent += 1\n",
    "\n",
    "    info['Entities correct'] = good_ent/total_ent if total_ent > 0 else 0\n",
    "    info['LOC correct'] = good_loc/total_loc if total_loc > 0 else 0\n",
    "    info['MISC correct'] = good_misc/total_misc if total_misc > 0 else 0\n",
    "    info['ORG correct'] = good_org/total_org if total_org > 0 else 0\n",
    "    info['PER correct'] = good_per/total_per if total_per > 0 else 0\n",
    "    info['Entities invented'] = invented_ent\n",
    "\n",
    "    if errors:\n",
    "        errors_list = []\n",
    "        invented = []\n",
    "        for i in range(0, len(real_entities)):\n",
    "            for entity in pred_entities[i]:\n",
    "                if entity not in real_entities[i]:\n",
    "                    errors_list.append((i, entity))\n",
    "        for i in range(0, len(pred_entities)):\n",
    "            for entity in real_entities[i]:\n",
    "                if entity not in pred_entities[i]:\n",
    "                    invented.append((i, entity))\n",
    "        return info, errors_list, invented\n",
    "                \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2b1d5",
   "metadata": {},
   "source": [
    "### Grid search per definir les quatre primeres funcions extres\n",
    "\n",
    "Ara farem un \"grid search\" per seleccionar quines de les quatre primeres funcions extres utilitzar:\n",
    "\n",
    "1. Prefixos\n",
    "2. Pos-Tag\n",
    "3. Longitud\n",
    "4. Stop-Word\n",
    "\n",
    "Provarem totes les combinacions possibles i crearem una taula per cada idioma amb els resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1db0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "def features_grid_search(train, val_tokens, val_tags):\n",
    "    '''\n",
    "    Perform a grid search over the features\n",
    "    '''\n",
    "    feature_combinations = list(itertools.product([0, 1], repeat=4))\n",
    "\n",
    "    results = pd.DataFrame(columns=['Features', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                    'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "    # Para cada combinación de características\n",
    "    for features in feature_combinations:\n",
    "        model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=features))\n",
    "        model.train(train, 'crfTagger.mdl')\n",
    "        prediction = model.tag_sents(val_tokens)\n",
    "\n",
    "        _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "        feat_results = evaluate_tagger_performance(val_tags, prediction_tags, features)\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6a16e",
   "metadata": {},
   "source": [
    "### Anàlisi de \"feature functions\" en el model de l'idioma espanyol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902e06e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_es_prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m test_features:\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mCRFTagger(feature_func\u001b[38;5;241m=\u001b[39mGetFeatures(features_vector\u001b[38;5;241m=\u001b[39mfeatures))\n\u001b[1;32m---> 11\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\u001b[43mtrain_es_prep\u001b[49m[:\u001b[38;5;241m500\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrfTagger.mdl\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Usando un subconjunto para rapidez\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtag_sents(dev_es_tokens[:\u001b[38;5;241m100\u001b[39m])  \u001b[38;5;66;03m# Evaluamos en un subconjunto\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     _, prediction_tags \u001b[38;5;241m=\u001b[39m sep_token_tag(prediction)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_es_prep' is not defined"
     ]
    }
   ],
   "source": [
    "# Esta operación puede tardar bastante tiempo\n",
    "# results_es = features_grid_search(train_es_prep, dev_es_tokens, dev_es_tags)\n",
    "\n",
    "# Para ahorrar tiempo, probamos solo algunas configuraciones relevantes\n",
    "test_features = [[0,0,0,0], [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1], [1,1,0,0], [1,1,1,0], [1,1,1,1]]\n",
    "results_es = pd.DataFrame(columns=['Features', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                  'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "for features in test_features:\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=features))\n",
    "    model.train(train_es_prep[:500], 'crfTagger.mdl')  # Usando un subconjunto para rapidez\n",
    "    prediction = model.tag_sents(dev_es_tokens[:100])  # Evaluamos en un subconjunto\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(dev_es_tags[:100], prediction_tags, features)\n",
    "\n",
    "    results_es = pd.concat([results_es, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "results_es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a40356",
   "metadata": {},
   "source": [
    "A partir dels resultats podem observar que:\n",
    "\n",
    "- La \"balanced accuracy\" és alta en tots els casos, indicant que el model ubica bastant bé on hi ha entitats\n",
    "- El marge de millora es troba en detectar correctament les entitats i el seu tipus\n",
    "- Les funcions que més milloren el model individualment són els prefixos i el Pos-Tag\n",
    "- La longitud aporta millora en models amb més funcions actives\n",
    "- Les Stop-Words no aporten pràcticament res\n",
    "\n",
    "Per l'espanyol, seleccionem la combinació de funcions [1,1,1,0] (prefixos, pos-tag, longitud)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391d510",
   "metadata": {},
   "source": [
    "### Anàlisi de \"feature functions\" en el model de l'idioma holandés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta operación puede tardar bastante tiempo\n",
    "# results_ned = features_grid_search(train_ned_prep, dev_ned_tokens, dev_ned_tags)\n",
    "\n",
    "# Para ahorrar tiempo, probamos solo algunas configuraciones relevantes\n",
    "test_features = [[0,0,0,0], [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1], [1,1,0,0], [1,1,1,0], [1,1,1,1]]\n",
    "results_ned = pd.DataFrame(columns=['Features', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                  'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "for features in test_features:\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=features))\n",
    "    model.train(train_ned_prep[:500], 'crfTagger.mdl')  # Usando un subconjunto para rapidez\n",
    "    prediction = model.tag_sents(dev_ned_tokens[:100])  # Evaluamos en un subconjunto\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(dev_ned_tags[:100], prediction_tags, features)\n",
    "\n",
    "    results_ned = pd.concat([results_ned, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "results_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afc7d6",
   "metadata": {},
   "source": [
    "Per l'holandès, també observem:\n",
    "\n",
    "- El rang de millora és superior que en espanyol, tant en \"accuracy\" com en detecció d'entitats\n",
    "- Les funcions 1 (prefixos) i 2 (pos-tag) tenen el major impacte individual\n",
    "- La funció 3 (longitud) millora lleugerament, però la 4 (stop-words) no aporta\n",
    "- MISC es prediu millor que en espanyol, mentre que ORG es prediu pitjor\n",
    "\n",
    "Seleccionem també la combinació [1,1,1,0] per l'holandès."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1afed",
   "metadata": {},
   "source": [
    "### Prova d'afegir llistes externes de noms i localitzacions\n",
    "\n",
    "Finalment, provem si afegir la cinquena funció extra (llistes externes) millora els resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c65bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_comparation(train, val_tokens, val_tags):\n",
    "    '''\n",
    "    Compare the performance of the tagger with and without lists\n",
    "    '''\n",
    "    results = pd.DataFrame(columns=['Lists', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                    'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "    # Sin listas\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=False))\n",
    "    model.train(train[:500], 'crfTagger.mdl')  # Subsample para rapidez\n",
    "    prediction = model.tag_sents(val_tokens[:100])\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(val_tags[:100], prediction_tags, \"No lists\")\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "    # Con listas\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True))\n",
    "    model.train(train[:500], 'crfTagger.mdl')\n",
    "    prediction = model.tag_sents(val_tokens[:100])\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(val_tags[:100], prediction_tags, \"Lists\")\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Comparación para español\n",
    "results_lists_es = list_comparation(train_es_prep, dev_es_tokens, dev_es_tags)\n",
    "print(\"Resultados para español con y sin listas:\")\n",
    "print(results_lists_es)\n",
    "\n",
    "# Comparación para holandés\n",
    "results_lists_ned = list_comparation(train_ned_prep, dev_ned_tokens, dev_ned_tags)\n",
    "print(\"\\nResultados para holandés con y sin listas:\")\n",
    "print(results_lists_ned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671f60d",
   "metadata": {},
   "source": [
    "Després d'analitzar els resultats amb i sense llistes, observem:\n",
    "\n",
    "- Les llistes milloren significativament el percentatge d'entitats predites correctament\n",
    "- En espanyol, millora especialment la detecció de localitzacions i persones\n",
    "- En holandès, la millora es concentra en localitzacions\n",
    "- Les llistes també redueixen el nombre d'entitats inventades\n",
    "\n",
    "Per tant, utilitzarem les llistes en ambdós models finals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead90722",
   "metadata": {},
   "source": [
    "## 2 - Experimentació de context\n",
    "\n",
    "Un cop decidides les \"feature functions\", experimentem amb diferents contexts. Fins ara només hem donat informació del token a predir, ara provarem d'incloure informació de les paraules del voltant.\n",
    "\n",
    "Provarem les següents combinacions de context:\n",
    "- Paraula actual\n",
    "- Paraula anterior, paraula actual\n",
    "- Paraula dos cops anterior, paraula anterior, paraula actual\n",
    "- Paraula actual, paraula següent\n",
    "- Paraula actual, paraula següent, paraula dos cops següent\n",
    "- Paraula anterior, paraula actual, paraula següent\n",
    "- Paraula anterior, paraula actual, paraula següent, paraula dos cops següent\n",
    "- Paraula dos cops anterior, paraula anterior, paraula actual, paraula següent\n",
    "- Paraula dos cops anterior, paraula anterior, paraula actual, paraula següent, paraula dos cops següent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2389c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_search(train, val_tokens, val_tags):\n",
    "    '''\n",
    "    Perform a grid search over the word vector\n",
    "    '''\n",
    "    word_combinations = [\n",
    "        [0, 0, 1, 0, 0],  # Paraula actual\n",
    "        [0, 1, 1, 0, 0],  # Paraula anterior, paraula actual\n",
    "        [1, 1, 1, 0, 0],  # Paraula dos cops anterior, paraula anterior, paraula actual\n",
    "        [0, 0, 1, 1, 0],  # Paraula actual, paraula següent\n",
    "        [0, 0, 1, 1, 1],  # Paraula actual, paraula següent, paraula dos cops següent\n",
    "        [0, 1, 1, 1, 0],  # Paraula anterior, paraula actual, paraula següent\n",
    "        [0, 1, 1, 1, 1],  # Paraula anterior, paraula actual, paraula següent, paraula dos cops següent\n",
    "        [1, 1, 1, 1, 0],  # Paraula dos cops anterior, paraula anterior, paraula actual, paraula següent\n",
    "        [1, 1, 1, 1, 1]   # Paraula dos cops anterior, paraula anterior, paraula actual, paraula següent, paraula dos cops següent\n",
    "    ]\n",
    "\n",
    "    results = pd.DataFrame(columns=['Word vector', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                    'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "    # Para cada combinación de word vector\n",
    "    for i, word_vector in enumerate(word_combinations):\n",
    "        model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=word_vector))\n",
    "        model.train(train[:500], 'crfTagger.mdl')\n",
    "        prediction = model.tag_sents(val_tokens[:100])\n",
    "\n",
    "        _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "        feat_results = evaluate_tagger_performance(val_tags[:100], prediction_tags, f\"Context {i+1}\")\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71923de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta operación puede tardar bastante tiempo\n",
    "# Probamos versiones simplificadas para español\n",
    "context_combinations = [\n",
    "    [0, 0, 1, 0, 0],  # Solo palabra actual\n",
    "    [0, 1, 1, 0, 0],  # Palabra anterior + actual\n",
    "    [0, 1, 1, 1, 0],  # Palabra anterior + actual + siguiente\n",
    "    [1, 1, 1, 1, 1]   # Contexto completo\n",
    "]\n",
    "\n",
    "results_context_es = pd.DataFrame(columns=['Word vector', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                          'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "for i, word_vector in enumerate(context_combinations):\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=word_vector))\n",
    "    model.train(train_es_prep[:500], 'crfTagger.mdl')\n",
    "    prediction = model.tag_sents(dev_es_tokens[:100])\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(dev_es_tags[:100], prediction_tags, f\"Context {i+1}\")\n",
    "\n",
    "    results_context_es = pd.concat([results_context_es, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "results_context_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos versiones simplificadas para holandés\n",
    "results_context_ned = pd.DataFrame(columns=['Word vector', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                           'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "for i, word_vector in enumerate(context_combinations):\n",
    "    model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=word_vector))\n",
    "    model.train(train_ned_prep[:500], 'crfTagger.mdl')\n",
    "    prediction = model.tag_sents(dev_ned_tokens[:100])\n",
    "\n",
    "    _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "    feat_results = evaluate_tagger_performance(dev_ned_tags[:100], prediction_tags, f\"Context {i+1}\")\n",
    "\n",
    "    results_context_ned = pd.concat([results_context_ned, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "results_context_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13df7a",
   "metadata": {},
   "source": [
    "Dels resultats observem:\n",
    "\n",
    "- Afegir funcions del context millora el rendiment del model\n",
    "- Donar massa context pot empitjorar els resultats\n",
    "- Per ambdós idiomes, la millor combinació és: **Paraula anterior, paraula actual, paraula següent** [0,1,1,1,0]\n",
    "- Aquest context millora tots els tipus d'entitats i disminueix el nombre d'entitats inventades\n",
    "\n",
    "Els nostres models finals tindran els següents paràmetres:\n",
    "- features_vector = [1, 1, 1, 0] (prefixos, pos-tag, longitud)\n",
    "- lists = True (utilitzem llistes externes)\n",
    "- word_vector = [0, 1, 1, 1, 0] (paraula anterior, actual i següent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad87644",
   "metadata": {},
   "source": [
    "## 3 - Experimentació de les codificacions\n",
    "\n",
    "Ara provarem els models finals amb diferents codificacions per escollir-ne una per cada idioma. Provarem:\n",
    "- IO\n",
    "- BIO\n",
    "- BIOE\n",
    "- BIOS\n",
    "- BIOES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_IO(train):\n",
    "    '''\n",
    "    Convert the train to IO format\n",
    "    '''\n",
    "    train_io = []\n",
    "    for sent in train:\n",
    "        sent_io = []\n",
    "        for word in sent:\n",
    "            new_tag = re.sub(r'\\bB-', 'I-', word[1])\n",
    "            sent_io.append((word[0], new_tag))\n",
    "        train_io.append(sent_io)\n",
    "    return train_io\n",
    "\n",
    "def train_to_BIOE(train):\n",
    "    '''\n",
    "    Convert the train to BIOE format\n",
    "    '''\n",
    "    train_bioe = []\n",
    "    for sent in train:\n",
    "        sent_bioe = []\n",
    "        for i in range(len(sent)):\n",
    "            tag = sent[i][1]\n",
    "            if tag[0] == 'I' and (i == len(sent) - 1 or sent[i + 1][1][0] != 'I' or sent[i+1][1][2:] != tag[2:]):\n",
    "                tag = re.sub(r'\\bI-', 'E-', tag)\n",
    "            sent_bioe.append((sent[i][0], tag))\n",
    "        train_bioe.append(sent_bioe)\n",
    "    return train_bioe\n",
    "\n",
    "def train_to_BIOS(train):\n",
    "    '''\n",
    "    Convert the train to BIOS format\n",
    "    '''\n",
    "    train_bios = []\n",
    "    for sent in train:\n",
    "        sent_bios = []\n",
    "        for i in range(len(sent)):\n",
    "            tag = sent[i][1]\n",
    "            if tag[0] == 'B' and (i == len(sent) - 1 or sent[i + 1][1][0] != 'I' or sent[i+1][1][2:] != tag[2:]):\n",
    "                tag = re.sub(r'\\bB-', 'S-', tag)\n",
    "            sent_bios.append((sent[i][0], tag))\n",
    "        train_bios.append(sent_bios)\n",
    "    return train_bios\n",
    "\n",
    "def train_to_BIOES(train):\n",
    "    '''\n",
    "    Convert the train to BIOES format\n",
    "    '''\n",
    "    train_bios = train_to_BIOS(train)\n",
    "    train_bioes = train_to_BIOE(train_bios)\n",
    "    return train_bioes\n",
    "\n",
    "# Crear versiones con diferentes codificaciones (usando una muestra para rapidez)\n",
    "esp_train_sample = train_es_prep[:500]\n",
    "esp_train_IO = train_to_IO(esp_train_sample)\n",
    "esp_train_BIOE = train_to_BIOE(esp_train_sample)\n",
    "esp_train_BIOS = train_to_BIOS(esp_train_sample)\n",
    "esp_train_BIOES = train_to_BIOES(esp_train_sample)\n",
    "\n",
    "ned_train_sample = train_ned_prep[:500]\n",
    "ned_train_IO = train_to_IO(ned_train_sample)\n",
    "ned_train_BIOE = train_to_BIOE(ned_train_sample)\n",
    "ned_train_BIOS = train_to_BIOS(ned_train_sample)\n",
    "ned_train_BIOES = train_to_BIOES(ned_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cod_grid_search(trains, val_tokens, val_tags, cod):\n",
    "    '''\n",
    "    Perform a grid search over the codification\n",
    "    '''\n",
    "    results = pd.DataFrame(columns=['Codification', 'Balanced accuracy', 'Total entities', 'Entities correct', \n",
    "                                    'LOC correct', 'MISC correct', 'ORG correct', 'PER correct', 'Entities invented'])\n",
    "\n",
    "    for i in range(len(trains)):\n",
    "        model = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=[0, 1, 1, 1, 0]))\n",
    "        model.train(trains[i], 'crfTagger.mdl')\n",
    "        prediction = model.tag_sents(val_tokens[:100])\n",
    "\n",
    "        _, prediction_tags = sep_token_tag(prediction)\n",
    "\n",
    "        feat_results = evaluate_tagger_performance(val_tags[:100], prediction_tags, cod[i])\n",
    "\n",
    "        results = pd.concat([results, pd.DataFrame([feat_results])], ignore_index=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Probamos diferentes codificaciones\n",
    "cod_train_esp = [esp_train_sample, esp_train_IO, esp_train_BIOE, esp_train_BIOS, esp_train_BIOES]\n",
    "cod_train_ned = [ned_train_sample, ned_train_IO, ned_train_BIOE, ned_train_BIOS, ned_train_BIOES]\n",
    "cod = ['BIO','IO', 'BIOE', 'BIOS', 'BIOES']\n",
    "\n",
    "results_cod_es = cod_grid_search(cod_train_esp, dev_es_tokens, dev_es_tags, cod)\n",
    "print(\"Resultados de codificaciones para español:\")\n",
    "print(results_cod_es)\n",
    "\n",
    "results_cod_ned = cod_grid_search(cod_train_ned, dev_ned_tokens, dev_ned_tags, cod)\n",
    "print(\"\\nResultados de codificaciones para holandés:\")\n",
    "print(results_cod_ned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c2528",
   "metadata": {},
   "source": [
    "Després de provar les diferents codificacions observem:\n",
    "\n",
    "**Per a l'espanyol:**\n",
    "- No hi ha pràcticament diferència entre les codificacions\n",
    "- La codificació BIOES dona la millor \"balanced accuracy\" i precisió en entitats\n",
    "- Però BIOES augmenta lleugerament el nombre d'entitats inventades\n",
    "- Decidim mantenir la codificació original BIO per simplicitat\n",
    "\n",
    "**Per a l'holandès:**\n",
    "- Totes les codificacions empitjoren respecte l'original (BIO)\n",
    "- Mantenim també la codificació BIO\n",
    "\n",
    "La conclusió és que per ambdós idiomes utilitzarem la codificació BIO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421a87b",
   "metadata": {},
   "source": [
    "## 4 - Anàlisi dels models definitius\n",
    "\n",
    "Ara que hem definit els models finals, els entrenarem amb tots els paràmetres seleccionats i els avaluarem en la partició de test:\n",
    "\n",
    "- Features: prefixos, pos-tag i longitud [1,1,1,0]\n",
    "- Llistes externes: sí\n",
    "- Context: paraula anterior, actual i següent [0,1,1,1,0]\n",
    "- Codificació: BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos y evaluamos el modelo final para español\n",
    "model_esp = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=[0, 1, 1, 1, 0]))\n",
    "model_esp.train(train_es_prep, 'esp_model.mdl')\n",
    "prediction_esp = model_esp.tag_sents(test_es_tokens)\n",
    "\n",
    "_, prediction_esp_tags = sep_token_tag(prediction_esp)\n",
    "\n",
    "results_esp_final, errors_esp = evaluate_tagger_performance(test_es_tags, prediction_esp_tags, \"Español Final\", errors=True)\n",
    "print(\"Resultados del modelo español en test:\")\n",
    "for key, value in results_esp_final.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos y evaluamos el modelo final para holandés\n",
    "model_ned = nltk.tag.CRFTagger(feature_func=GetFeatures(features_vector=[1, 1, 1, 0], lists=True, word_vector=[0, 1, 1, 1, 0]))\n",
    "model_ned.train(train_ned_prep, 'ned_model.mdl')\n",
    "prediction_ned = model_ned.tag_sents(test_ned_tokens)\n",
    "\n",
    "_, prediction_ned_tags = sep_token_tag(prediction_ned)\n",
    "\n",
    "results_ned_final, errors_ned = evaluate_tagger_performance(test_ned_tags, prediction_ned_tags, \"Holandés Final\", errors=True)\n",
    "print(\"Resultados del modelo holandés en test:\")\n",
    "for key, value in results_ned_final.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32dc672",
   "metadata": {},
   "source": [
    "### Anàlisi d'errors\n",
    "\n",
    "Examinem alguns errors específics per entendre millor les limitacions dels models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(21)\n",
    "# Escogemos un error aleatorio del modelo español\n",
    "if errors_esp and len(errors_esp) > 0:\n",
    "    error = random.choice(errors_esp)\n",
    "    \n",
    "    idx = error[0]\n",
    "    entity = error[1]\n",
    "    \n",
    "    sentence = [token[0] for token in test_es[idx]]\n",
    "    \n",
    "    print(\"Frase original:\", sentence)\n",
    "    print(\"Etiquetas reales:\", [tag for tag in test_es_tags[idx]])\n",
    "    print(\"Etiquetas predichas:\", [tag[1] for tag in prediction_esp[idx]])\n",
    "    print(\"\\nError en la entidad:\", entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(23)\n",
    "# Escogemos un error aleatorio del modelo holandés\n",
    "if errors_ned and len(errors_ned) > 0:\n",
    "    error = random.choice(errors_ned)\n",
    "    \n",
    "    idx = error[0]\n",
    "    entity = error[1]\n",
    "    \n",
    "    sentence = [token[0] for token in test_ned[idx]]\n",
    "    \n",
    "    print(\"Frase original:\", sentence)\n",
    "    print(\"Etiquetas reales:\", [tag for tag in test_ned_tags[idx]])\n",
    "    print(\"Etiquetas predichas:\", [tag[1] for tag in prediction_ned[idx]])\n",
    "    print(\"\\nError en la entidad:\", entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c43b78",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "**Model Espanyol:**\n",
    "- Excel·lent \"accuracy\" balancejada, indicant bona capacitat per ubicar entitats\n",
    "- Aproximadament 4 de cada 5 entitats són detectades perfectament\n",
    "- És especialment bo en detectar persones (90%), seguit de localitzacions i organitzacions\n",
    "- El tipus MISC costa més de detectar (50% d'encert)\n",
    "- Els errors més comuns són:\n",
    "  - Paraules estranyes que es prediuen amb un tipus incorrecte\n",
    "  - Entitats extenses que no s'acaben de predir correctament\n",
    "\n",
    "**Model Holandès:**\n",
    "- Resultats una mica inferiors al model espanyol, però igualment bons\n",
    "- 3 de cada 4 entitats són identificades perfectament\n",
    "- També excel·lent en detectar persones i localitzacions\n",
    "- Més problemes amb organitzacions que el model espanyol\n",
    "- Millor detecció del tipus MISC que el model espanyol\n",
    "- Els errors més comuns continuen sent sobre el tipus d'entitat, no sobre la ubicació\n",
    "\n",
    "En definitiva, hem creat dos models funcionals que detecten la gran majoria d'entitats i, en la majoria dels casos, identifiquen correctament el seu tipus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
