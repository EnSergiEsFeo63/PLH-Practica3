{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f8cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es =conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned =conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af853b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n"
     ]
    }
   ],
   "source": [
    "train = conll2002.tagged_sents('esp.train')\n",
    "test = conll2002.tagged_sents('esp.testb')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766ae304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gazetters():\n",
    "    \"\"\"\n",
    "    Define a set of gazetteers for named entity recognition.\n",
    "    These gazetteers include common locations, person names, and organizations.\n",
    "    \"\"\"\n",
    "\n",
    "    # List of locations, person names, and organizations\n",
    "    locations_list = {\n",
    "        # Ciudades principales (España y Países Bajos)\n",
    "        \"madrid\", \"barcelona\", \"valencia\", \"sevilla\", \"zaragoza\", \"bilbao\", \"málaga\", \"murcia\", \"palma\",\n",
    "        \"amsterdam\", \"rotterdam\", \"utrecht\", \"eindhoven\", \"groningen\", \"nijmegen\",\n",
    "\n",
    "        # Regiones y provincias\n",
    "        \"andalucía\", \"cataluña\", \"galicia\", \"castilla\", \"canarias\", \"baleares\",\n",
    "        \"noord-holland\", \"zuid-holland\", \"gelderland\", \"limburg\",\n",
    "\n",
    "        # Países (hispanohablantes + relevantes cercanos)\n",
    "        \"españa\", \"méxico\", \"argentina\", \"colombia\", \"perú\", \"chile\", \"ecuador\",\n",
    "        \"nederland\", \"belgië\", \"duitsland\", \"francia\", \"italia\", \"portugal\", \"reino unido\", \"estados unidos\",\n",
    "\n",
    "        # Geografía física\n",
    "        \"mediterráneo\", \"atlántico\", \"pirineos\", \"cantábrico\", \"guadalquivir\", \"ebro\",\n",
    "        \"noordzee\", \"rijn\", \"maas\", \"veluwe\"\n",
    "    }\n",
    "\n",
    "    person_names_list = {\n",
    "        # Nombres masculinos comunes\n",
    "        \"juan\", \"josé\", \"david\", \"javier\", \"miguel\", \"pedro\", \"sergio\", \"pablo\", \"antonio\",\n",
    "        \"jan\", \"peter\", \"willem\", \"tim\", \"thomas\", \"jeroen\", \"lucas\", \"kees\",\n",
    "\n",
    "        # Nombres femeninos comunes\n",
    "        \"maría\", \"ana\", \"isabel\", \"laura\", \"marta\", \"lucía\", \"paula\", \"cristina\",\n",
    "        \"maria\", \"johanna\", \"emma\", \"sophie\", \"lisa\", \"lotte\", \"iris\", \"eva\",\n",
    "\n",
    "        # Apellidos comunes\n",
    "        \"garcía\", \"gonzález\", \"rodríguez\", \"fernández\", \"lópez\", \"sánchez\", \"martínez\",\n",
    "        \"de jong\", \"jansen\", \"de vries\", \"van den berg\", \"bakker\", \"visser\", \"meijer\"\n",
    "    }\n",
    "\n",
    "    organizations_list = {\n",
    "        # Empresas destacadas\n",
    "        \"telefónica\", \"bbva\", \"iberdrola\", \"mercadona\", \"inditex\", \"seat\",\n",
    "        \"philips\", \"shell\", \"unilever\", \"heineken\", \"ing\", \"asml\",\n",
    "\n",
    "        # Instituciones gubernamentales\n",
    "        \"gobierno\", \"ministerio\", \"ayuntamiento\", \"generalitat\", \"policía nacional\", \"guardia civil\",\n",
    "        \"regering\", \"ministerie\", \"gemeente\", \"belastingdienst\", \"politie\", \"koninklijke marechaussee\",\n",
    "\n",
    "        # Educación y cultura\n",
    "        \"universidad\", \"museo del prado\", \"csic\", \"colegio\", \"hospital\", \"asociación\", \"fundación\",\n",
    "        \"universiteit\", \"hogeschool\", \"tno\", \"knaw\", \"rijksmuseum\", \"omroep\", \"kvk\"\n",
    "    }\n",
    "\n",
    "    return locations_list, person_names_list, organizations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9844a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def pos_tag(word):\n",
    "    tagged = nltk.pos_tag([word])\n",
    "    return tagged[0][1]\n",
    "\n",
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {\"bias\": 1.0}  # Bias term for the feature vector\n",
    "        word = sentence[index][0]  # Always get the first element (word)\n",
    "        \n",
    "        # Características básicas de la palabra\n",
    "        if feature_selection.get(\"word_form\", True):\n",
    "            features.update({\n",
    "                \"word\": word,\n",
    "                \"word.lower\": word.lower(),\n",
    "            })\n",
    "\n",
    "        # Características morfológicas\n",
    "        if feature_selection.get(\"morphology\", True):\n",
    "            features.update({\n",
    "                \"word.istitle\": word.istitle(),\n",
    "                \"word.isupper\": word.isupper(),\n",
    "                \"word.islower\": word.islower(),\n",
    "                \"word.isdigit\": word.isdigit(),\n",
    "                \"has_digit\": any(c.isdigit() for c in word),\n",
    "                \"capitals_inside\": any(c.isupper() for c in word[1:]),\n",
    "                \"has_symbol\": not word.isalnum()\n",
    "            })\n",
    "        \n",
    "        # POS tagging y lemas\n",
    "        if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "            features[\"pos_tag\"] = pos_tag(word)\n",
    "            features[\"lemma\"] = lemma(word)\n",
    "        \n",
    "        # Prefijos y sufijos\n",
    "        if feature_selection.get(\"prefix_suffix\", True):\n",
    "            features.update({\n",
    "                \"prefix3\": word[:3],\n",
    "                \"suffix3\": word[-3:],\n",
    "                \"prefix2\": word[:2],\n",
    "                \"suffix2\": word[-2:],\n",
    "            })\n",
    "        \n",
    "        # Longitud de la palabra\n",
    "        if feature_selection.get(\"length\", True):\n",
    "            features[\"length\"] = len(word)\n",
    "        \n",
    "        # Posición en la oración\n",
    "        if feature_selection.get(\"position\", True):\n",
    "            features.update({\n",
    "                \"position\": index,\n",
    "                \"is_first\": index == 0,\n",
    "                \"is_last\": index == len(sentence)-1\n",
    "            })\n",
    "        \n",
    "        # Contexto circundante\n",
    "        if feature_selection.get(\"context\", True):\n",
    "            if index > 0:\n",
    "                prev_word = sentence[index-1][0]\n",
    "                features.update({\n",
    "                    \"prev_word.lower\": prev_word.lower(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle(),\n",
    "                    \"prev_word.isupper\": prev_word.isupper(),\n",
    "                    \"prev_word.isdigit\": prev_word.isdigit(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle()})\n",
    "                \n",
    "                if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "                    features[\"prev_lemma\"] = lemma(prev_word)\n",
    "                    features[\"prev_pos_tag\"] = pos_tag(prev_word)\n",
    "\n",
    "            if index < len(sentence)-1:\n",
    "                next_word = sentence[index+1][0]\n",
    "                features.update({\n",
    "                    \"next_word.lower\": next_word.lower(),\n",
    "                    \"next_word.istitle\": next_word.istitle(),\n",
    "                    \"next_word.isupper\": next_word.isupper(),\n",
    "                    \"next_word.isdigit\": next_word.isdigit(),\n",
    "                    \"next_word.istitle\": next_word.istitle()})\n",
    "                \n",
    "                if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "                    features[\"next_lemma\"] = lemma(next_word)\n",
    "                    features[\"next_pos_tag\"] = pos_tag(next_word)\n",
    "        \n",
    "        # Patrones de fecha y hora\n",
    "        if feature_selection.get(\"date_time_patterns\", True):\n",
    "            date_pattern = r\"\\d{1,2}[/-]\\d{1,2}([/-]\\d{2,4})?\"\n",
    "            time_pattern = r\"\\d{1,2}:\\d{2}(:\\d{2})?\"\n",
    "            features[\"has_date_pattern\"] = bool(re.search(date_pattern, word))\n",
    "            features[\"has_time_pattern\"] = bool(re.search(time_pattern, word))\n",
    "\n",
    "        # Caracteres especiales y símbolos\n",
    "        if feature_selection.get(\"symbol_patterns\", True):\n",
    "            features.update({\n",
    "            \"has_hyphen\": \"-\" in word,\n",
    "            \"has_dot\": \".\" in word,\n",
    "            \"has_comma\": \",\" in word,\n",
    "            \"has_slash\": \"/\" in word,\n",
    "            \"has_percent\": \"%\" in word,\n",
    "            \"has_currency\": any(c in word for c in \"$€£¥\"),\n",
    "            \"has_at\": \"@\" in word\n",
    "            })\n",
    "\n",
    "                # En generate_features, añadir:\n",
    "        if feature_selection.get(\"gazetteers\", True):\n",
    "            locations_list, person_names_list, organizations_list = gazetters()\n",
    "            features.update({\n",
    "                \"in_locations\": word.lower() in locations_list,\n",
    "                \"in_person_names\": word.lower() in person_names_list,\n",
    "                \"in_organizations\": word.lower() in organizations_list\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e9cd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6670482991481187"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection = {\n",
    "    \"word_form\": True,           # Basic word characteristics\n",
    "    \"prefix_suffix\": True,       # Prefixes and suffixes\n",
    "    \"morphology\": True,          # Morphological features\n",
    "    \"context\": True,             # Surrounding words\n",
    "    \"lemma_pos_tags\": True,      # POS tagging and lemmas\n",
    "    \"date_time_patterns\": True,  # Date and time patterns\n",
    "    \"symbol_patterns\": True,     # Special characters\n",
    "    \"length\": True,              # Word length features\n",
    "    \"position\": True,            # Position features\n",
    "    \"gazetteers\": True           # Gazetteer features\n",
    "}\n",
    "\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")\n",
    "ct.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe62314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "class CRFModel:\n",
    "    # Clase proporcionada por el usuario (como en el enunciado)\n",
    "    def __init__(self, train, test, model_file: Optional[str] = None, features: Optional[Callable] = None, encoding: str = 'bio') -> None:\n",
    "        self.model = CRFTagger(feature_func=features)\n",
    "        self.model_file = model_file\n",
    "        self.encoding = encoding\n",
    "        self.train_data = self.transformar_dades(train)\n",
    "        self.test_data = self.transformar_dades(test)\n",
    "\n",
    "    def transformar_dades(self, data):\n",
    "        if self.encoding == 'io':\n",
    "            data_encoded = [self.to_io(sent) for sent in data]\n",
    "        elif self.encoding == 'bioes':\n",
    "            data_encoded = [self.to_bioes(sent) for sent in data]\n",
    "        else:\n",
    "            data_encoded = data\n",
    "        return data_encoded\n",
    "\n",
    "    def train(self, train_data: List[List[Tuple[str, str]]]) -> None:\n",
    "        self.model.train(train_data, self.model_file)\n",
    "\n",
    "    def predict(self, sents: List[List[Tuple[str, str]]]) -> List[List[Tuple[str, str]]]:\n",
    "        if self.model_file:\n",
    "            self.model.set_model_file(self.model_file)\n",
    "        words_test = [[word for (word, _) in sent] for sent in sents]\n",
    "        return self.model.tag_sents(words_test)\n",
    "\n",
    "    # Resto de métodos (to_bioes, to_io, extraccio_entitats, evaluacio_entitats, matriu_confusio) como en el enunciado\n",
    "\n",
    "class NERCRF:\n",
    "    def __init__(self, language: str = 'esp', encoding: str = 'bio', feature_func: Optional[Callable] = None, model_file: Optional[str] = None):\n",
    "        self.language = language\n",
    "        self.encoding = encoding\n",
    "        self.feature_func = feature_func\n",
    "        self.model_file = model_file\n",
    "\n",
    "        # Cargar datos\n",
    "        self.train_raw = self._load_conll_data(language+'.train')\n",
    "        self.dev_raw = self._load_conll_data(language+'.testa')\n",
    "        self.test_raw = self._load_conll_data(language+'.testb')\n",
    "\n",
    "        # Inicializar modelo CRF\n",
    "        self.crf_model = CRFModel(\n",
    "            train=self.train_raw,\n",
    "            test=self.test_raw,\n",
    "            model_file=self.model_file,\n",
    "            features=self.feature_func,\n",
    "            encoding=self.encoding\n",
    "        )\n",
    "\n",
    "    def _load_conll_data(self, split: str) -> List[List[Tuple[str, str]]]:\n",
    "        sents = conll2002.iob_sents(f'{self.language}.{split}')\n",
    "        return [[(word, tag) for (word, _, tag) in sent] for sent in sents]\n",
    "\n",
    "    def train_model(self):\n",
    "        self.crf_model.train(self.crf_model.train_data)\n",
    "\n",
    "    def evaluate(self, split: str = 'dev') -> Tuple[float, float, float]:\n",
    "        if split == 'dev':\n",
    "            data = self.dev_raw\n",
    "        elif split == 'test':\n",
    "            data = self.test_raw\n",
    "        else:\n",
    "            raise ValueError(\"split debe ser 'dev' o 'test'\")\n",
    "\n",
    "        # Transformar datos al encoding correcto\n",
    "        data_encoded = self.crf_model.transformar_dades(data)\n",
    "\n",
    "        # Predecir etiquetas\n",
    "        predicted = self.crf_model.predict(data_encoded)\n",
    "\n",
    "        # Extraer entidades\n",
    "        y_true = []\n",
    "        for sent in data_encoded:\n",
    "            entities = self.crf_model.extraccio_entitats([sent])\n",
    "            y_true.extend(entities)\n",
    "\n",
    "        y_pred = []\n",
    "        for sent in predicted:\n",
    "            entities = self.crf_model.extraccio_entitats([sent])\n",
    "            y_pred.extend(entities)\n",
    "\n",
    "        # Evaluar\n",
    "        recall, precision, f1 = self.crf_model.evaluacio_entitats(y_true, y_pred)\n",
    "        return recall, precision, f1\n",
    "\n",
    "    def predict_sentence(self, sentence: List[str]) -> List[Tuple[str, str]]:\n",
    "        formatted_sent = [[(word, 'O') for word in sentence]]\n",
    "        predicted = self.crf_model.predict(formatted_sent)\n",
    "        return predicted[0]\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Función de características de ejemplo\n",
    "    def custom_features(sentence, index):\n",
    "        word = sentence[index][0]\n",
    "        features = {\n",
    "            'word': word,\n",
    "            'word.lower()': word.lower(),\n",
    "            'prefix3': word[:3],\n",
    "            'suffix3': word[-3:],\n",
    "            'prev_word': sentence[index-1][0] if index > 0 else '<START>',\n",
    "            'next_word': sentence[index+1][0] if index < len(sentence)-1 else '<END>',\n",
    "            'is_capitalized': word[0].isupper(),\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    # Entrenar modelo para español con BIOES\n",
    "    spanish_model = NERCRF(\n",
    "        language='esp',\n",
    "        encoding='bioes',\n",
    "        feature_func=custom_features,\n",
    "        model_file='esp_model.crf'\n",
    "    )\n",
    "    spanish_model.train_model()\n",
    "    recall, precision, f1 = spanish_model.evaluate('dev')\n",
    "    print(f\"Resultados para español (BIOES):\\nRecall: {recall:.2f}, Precision: {precision:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "    # Entrenar modelo para neerlandés con IO\n",
    "    dutch_model = NERCRF(\n",
    "        language='ned',\n",
    "        encoding='io',\n",
    "        feature_func=custom_features,\n",
    "        model_file='ned_model.crf'\n",
    "    )\n",
    "    dutch_model.train_model()\n",
    "    recall, precision, f1 = dutch_model.evaluate('test')\n",
    "    print(f\"Resultados para neerlandés (IO):\\nRecall: {recall:.2f}, Precision: {precision:.2f}, F1: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
