{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f8cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es =conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned =conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af853b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n"
     ]
    }
   ],
   "source": [
    "train = conll2002.tagged_sents('esp.train')\n",
    "test = conll2002.tagged_sents('esp.testb')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766ae304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gazetters():\n",
    "    \"\"\"\n",
    "    Define a set of gazetteers for named entity recognition.\n",
    "    These gazetteers include common locations, person names, and organizations.\n",
    "    \"\"\"\n",
    "\n",
    "    # List of locations, person names, and organizations\n",
    "    locations_list = {\n",
    "        # Ciudades principales (España y Países Bajos)\n",
    "        \"madrid\", \"barcelona\", \"valencia\", \"sevilla\", \"zaragoza\", \"bilbao\", \"málaga\", \"murcia\", \"palma\",\n",
    "        \"amsterdam\", \"rotterdam\", \"utrecht\", \"eindhoven\", \"groningen\", \"nijmegen\",\n",
    "\n",
    "        # Regiones y provincias\n",
    "        \"andalucía\", \"cataluña\", \"galicia\", \"castilla\", \"canarias\", \"baleares\",\n",
    "        \"noord-holland\", \"zuid-holland\", \"gelderland\", \"limburg\",\n",
    "\n",
    "        # Países (hispanohablantes + relevantes cercanos)\n",
    "        \"españa\", \"méxico\", \"argentina\", \"colombia\", \"perú\", \"chile\", \"ecuador\",\n",
    "        \"nederland\", \"belgië\", \"duitsland\", \"francia\", \"italia\", \"portugal\", \"reino unido\", \"estados unidos\",\n",
    "\n",
    "        # Geografía física\n",
    "        \"mediterráneo\", \"atlántico\", \"pirineos\", \"cantábrico\", \"guadalquivir\", \"ebro\",\n",
    "        \"noordzee\", \"rijn\", \"maas\", \"veluwe\"\n",
    "    }\n",
    "\n",
    "    person_names_list = {\n",
    "        # Nombres masculinos comunes\n",
    "        \"juan\", \"josé\", \"david\", \"javier\", \"miguel\", \"pedro\", \"sergio\", \"pablo\", \"antonio\",\n",
    "        \"jan\", \"peter\", \"willem\", \"tim\", \"thomas\", \"jeroen\", \"lucas\", \"kees\",\n",
    "\n",
    "        # Nombres femeninos comunes\n",
    "        \"maría\", \"ana\", \"isabel\", \"laura\", \"marta\", \"lucía\", \"paula\", \"cristina\",\n",
    "        \"maria\", \"johanna\", \"emma\", \"sophie\", \"lisa\", \"lotte\", \"iris\", \"eva\",\n",
    "\n",
    "        # Apellidos comunes\n",
    "        \"garcía\", \"gonzález\", \"rodríguez\", \"fernández\", \"lópez\", \"sánchez\", \"martínez\",\n",
    "        \"de jong\", \"jansen\", \"de vries\", \"van den berg\", \"bakker\", \"visser\", \"meijer\"\n",
    "    }\n",
    "\n",
    "    organizations_list = {\n",
    "        # Empresas destacadas\n",
    "        \"telefónica\", \"bbva\", \"iberdrola\", \"mercadona\", \"inditex\", \"seat\",\n",
    "        \"philips\", \"shell\", \"unilever\", \"heineken\", \"ing\", \"asml\",\n",
    "\n",
    "        # Instituciones gubernamentales\n",
    "        \"gobierno\", \"ministerio\", \"ayuntamiento\", \"generalitat\", \"policía nacional\", \"guardia civil\",\n",
    "        \"regering\", \"ministerie\", \"gemeente\", \"belastingdienst\", \"politie\", \"koninklijke marechaussee\",\n",
    "\n",
    "        # Educación y cultura\n",
    "        \"universidad\", \"museo del prado\", \"csic\", \"colegio\", \"hospital\", \"asociación\", \"fundación\",\n",
    "        \"universiteit\", \"hogeschool\", \"tno\", \"knaw\", \"rijksmuseum\", \"omroep\", \"kvk\"\n",
    "    }\n",
    "\n",
    "    return locations_list, person_names_list, organizations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9844a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def pos_tag(word):\n",
    "    tagged = nltk.pos_tag([word])\n",
    "    return tagged[0][1]\n",
    "\n",
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {\"bias\": 1.0}  # Bias term for the feature vector\n",
    "        word = sentence[index][0]  # Always get the first element (word)\n",
    "        \n",
    "        # Características básicas de la palabra\n",
    "        if feature_selection.get(\"word_form\", True):\n",
    "            features.update({\n",
    "                \"word\": word,\n",
    "                \"word.lower\": word.lower(),\n",
    "            })\n",
    "\n",
    "        # Características morfológicas\n",
    "        if feature_selection.get(\"morphology\", True):\n",
    "            features.update({\n",
    "                \"word.istitle\": word.istitle(),\n",
    "                \"word.isupper\": word.isupper(),\n",
    "                \"word.islower\": word.islower(),\n",
    "                \"word.isdigit\": word.isdigit(),\n",
    "                \"has_digit\": any(c.isdigit() for c in word),\n",
    "                \"capitals_inside\": any(c.isupper() for c in word[1:]),\n",
    "                \"has_symbol\": not word.isalnum()\n",
    "            })\n",
    "        \n",
    "        # POS tagging y lemas\n",
    "        if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "            features[\"pos_tag\"] = pos_tag(word)\n",
    "            features[\"lemma\"] = lemma(word)\n",
    "        \n",
    "        # Prefijos y sufijos\n",
    "        if feature_selection.get(\"prefix_suffix\", True):\n",
    "            features.update({\n",
    "                \"prefix3\": word[:3],\n",
    "                \"suffix3\": word[-3:],\n",
    "                \"prefix2\": word[:2],\n",
    "                \"suffix2\": word[-2:],\n",
    "            })\n",
    "        \n",
    "        # Longitud de la palabra\n",
    "        if feature_selection.get(\"length\", True):\n",
    "            features[\"length\"] = len(word)\n",
    "        \n",
    "        # Posición en la oración\n",
    "        if feature_selection.get(\"position\", True):\n",
    "            features.update({\n",
    "                \"position\": index,\n",
    "                \"is_first\": index == 0,\n",
    "                \"is_last\": index == len(sentence)-1\n",
    "            })\n",
    "        \n",
    "        # Contexto circundante\n",
    "        if feature_selection.get(\"context\", True):\n",
    "            if index > 0:\n",
    "                prev_word = sentence[index-1][0]\n",
    "                features.update({\n",
    "                    \"prev_word.lower\": prev_word.lower(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle(),\n",
    "                    \"prev_word.isupper\": prev_word.isupper(),\n",
    "                    \"prev_word.isdigit\": prev_word.isdigit(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle()})\n",
    "                \n",
    "                if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "                    features[\"prev_lemma\"] = lemma(prev_word)\n",
    "                    features[\"prev_pos_tag\"] = pos_tag(prev_word)\n",
    "\n",
    "            if index < len(sentence)-1:\n",
    "                next_word = sentence[index+1][0]\n",
    "                features.update({\n",
    "                    \"next_word.lower\": next_word.lower(),\n",
    "                    \"next_word.istitle\": next_word.istitle(),\n",
    "                    \"next_word.isupper\": next_word.isupper(),\n",
    "                    \"next_word.isdigit\": next_word.isdigit(),\n",
    "                    \"next_word.istitle\": next_word.istitle()})\n",
    "                \n",
    "                if feature_selection.get(\"lemma_pos_tags\", True):\n",
    "                    features[\"next_lemma\"] = lemma(next_word)\n",
    "                    features[\"next_pos_tag\"] = pos_tag(next_word)\n",
    "        \n",
    "        # Patrones de fecha y hora\n",
    "        if feature_selection.get(\"date_time_patterns\", True):\n",
    "            date_pattern = r\"\\d{1,2}[/-]\\d{1,2}([/-]\\d{2,4})?\"\n",
    "            time_pattern = r\"\\d{1,2}:\\d{2}(:\\d{2})?\"\n",
    "            features[\"has_date_pattern\"] = bool(re.search(date_pattern, word))\n",
    "            features[\"has_time_pattern\"] = bool(re.search(time_pattern, word))\n",
    "\n",
    "        # Caracteres especiales y símbolos\n",
    "        if feature_selection.get(\"symbol_patterns\", True):\n",
    "            features.update({\n",
    "            \"has_hyphen\": \"-\" in word,\n",
    "            \"has_dot\": \".\" in word,\n",
    "            \"has_comma\": \",\" in word,\n",
    "            \"has_slash\": \"/\" in word,\n",
    "            \"has_percent\": \"%\" in word,\n",
    "            \"has_currency\": any(c in word for c in \"$€£¥\"),\n",
    "            \"has_at\": \"@\" in word\n",
    "            })\n",
    "\n",
    "                # En generate_features, añadir:\n",
    "        if feature_selection.get(\"gazetteers\", True):\n",
    "            locations_list, person_names_list, organizations_list = gazetters()\n",
    "            features.update({\n",
    "                \"in_locations\": word.lower() in locations_list,\n",
    "                \"in_person_names\": word.lower() in person_names_list,\n",
    "                \"in_organizations\": word.lower() in organizations_list\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e9cd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6670482991481187"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection = {\n",
    "    \"word_form\": True,           # Basic word characteristics\n",
    "    \"prefix_suffix\": True,       # Prefixes and suffixes\n",
    "    \"morphology\": True,          # Morphological features\n",
    "    \"context\": True,             # Surrounding words\n",
    "    \"lemma_pos_tags\": True,      # POS tagging and lemmas\n",
    "    \"date_time_patterns\": True,  # Date and time patterns\n",
    "    \"symbol_patterns\": True,     # Special characters\n",
    "    \"length\": True,              # Word length features\n",
    "    \"position\": True,            # Position features\n",
    "    \"gazetteers\": True           # Gazetteer features\n",
    "}\n",
    "\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")\n",
    "ct.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe62314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002\n",
    "from nltk.tag import CRFTagger\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('conll2002')\n",
    "\n",
    "class CRFModel:\n",
    "    # Clase proporcionada por el usuario (como en el enunciado)\n",
    "    def __init__(self, train, test, model_file: Optional[str] = None, features: Optional[Callable] = None, encoding: str = 'bio') -> None:\n",
    "        self.model = CRFTagger(feature_func=features)\n",
    "        self.model_file = model_file\n",
    "        self.encoding = encoding\n",
    "        self.train_data = self.transformar_dades(train)\n",
    "        self.test_data = self.transformar_dades(test)\n",
    "\n",
    "    def transformar_dades(self, data):\n",
    "        if self.encoding == 'io':\n",
    "            #data_encoded = [self.to_io(sent) for sent in data] \n",
    "            data_encoded = self.to_io(data)\n",
    "        elif self.encoding == 'bioes':\n",
    "            #data_encoded = [self.to_bioes(sent) for sent in data]\n",
    "            data_encoded = self.to_bioes(data)\n",
    "        elif self.encoding == 'biow':\n",
    "            #data_encoded = [self.to_biow(sent) for sent in data]\n",
    "            data_encoded = self.to_biow(data)\n",
    "        else:\n",
    "            data_encoded = data\n",
    "        return data_encoded\n",
    "\n",
    "    def train(self, train_data: List[List[Tuple[str, str]]]) -> None:\n",
    "        self.model.train(train_data, self.model_file)\n",
    "\n",
    "    def predict(self, sents: List[List[Tuple[str, str]]]) -> List[List[Tuple[str, str]]]:\n",
    "        if self.model_file:\n",
    "            self.model.set_model_file(self.model_file)\n",
    "        words_test = [[word for (word, _) in sent] for sent in sents]\n",
    "        return self.model.tag_sents(words_test)\n",
    "    def to_biow(self, tagged_sents):\n",
    "   \n",
    "        biow_tagged_sents = []\n",
    "        \n",
    "        for sent in tagged_sents:\n",
    "            biow_sent = []\n",
    "            i = 0\n",
    "            while i < len(sent):\n",
    "                word = sent[i][0]  # Get the word\n",
    "                tag = sent[i][-1]  # Get the BIO tag\n",
    "                \n",
    "                # If it's a B- tag, check if it's a single token entity\n",
    "                if tag.startswith('B-'):\n",
    "                    entity_type = tag[2:]  # Get entity type (PER, LOC, etc.)\n",
    "                    \n",
    "                    # Check if next token continues this entity\n",
    "                    if i + 1 < len(sent) and sent[i+1][-1] == f'I-{entity_type}':\n",
    "                        # Multi-token entity, keep as B-\n",
    "                        biow_sent.append((word, tag))\n",
    "                    else:\n",
    "                        # Single token entity, convert to W-\n",
    "                        biow_sent.append((word, f'W-{entity_type}'))\n",
    "                else:\n",
    "                    # Keep I- and O tags as they are\n",
    "                    biow_sent.append((word, tag))\n",
    "                    \n",
    "                i += 1\n",
    "                \n",
    "            biow_tagged_sents.append(biow_sent)\n",
    "        \n",
    "        return biow_tagged_sents\n",
    "    \n",
    "    def to_io(self,tagged_sents):\n",
    "        io_tagged_sents = []\n",
    "        \n",
    "        for sent in tagged_sents:\n",
    "            io_sent = []\n",
    "            for token in sent:\n",
    "                word = token[0]  # Get the word\n",
    "                tag = token[-1]  # Get the BIO tag\n",
    "                \n",
    "                # Convert B- to I- (Beginning to Inside)\n",
    "                if tag.startswith('B-'):\n",
    "                    tag = 'I-' + tag[2:]\n",
    "                    \n",
    "                io_sent.append((word, tag))\n",
    "            io_tagged_sents.append(io_sent)\n",
    "        \n",
    "        return io_tagged_sents\n",
    "    def to_bioes(self,tagged_sents):\n",
    "        bioes_tagged_sents = []\n",
    "        \n",
    "        for sent in tagged_sents:\n",
    "            bioes_sent = []\n",
    "            i = 0\n",
    "            while i < len(sent):\n",
    "                word = sent[i][0]  # Get the word\n",
    "                tag = sent[i][-1]  # Get the BIO tag\n",
    "                \n",
    "                # Handle outside tags (O)\n",
    "                if tag == 'O':\n",
    "                    bioes_sent.append((word, tag))\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # Handle entity tags (B-X or I-X)\n",
    "                if tag.startswith('B-'):\n",
    "                    entity_type = tag[2:]  # Get entity type (PER, LOC, etc.)\n",
    "                    \n",
    "                    # Find the end of this entity\n",
    "                    end_idx = i + 1\n",
    "                    while (end_idx < len(sent) and \n",
    "                        sent[end_idx][-1] == f'I-{entity_type}'):\n",
    "                        end_idx += 1\n",
    "                    \n",
    "                    # Single-token entity (B-X not followed by I-X)\n",
    "                    if end_idx == i + 1:\n",
    "                        bioes_sent.append((word, f'S-{entity_type}'))\n",
    "                    else:\n",
    "                        # Add B- tag for beginning\n",
    "                        bioes_sent.append((word, tag))\n",
    "                        \n",
    "                        # Add I- tags for middle tokens (if any)\n",
    "                        for j in range(i+1, end_idx-1):\n",
    "                            word_j = sent[j][0]\n",
    "                            bioes_sent.append((word_j, f'I-{entity_type}'))\n",
    "                        \n",
    "                        # Add E- tag for the end token\n",
    "                        word_end = sent[end_idx-1][0]\n",
    "                        bioes_sent.append((word_end, f'E-{entity_type}'))\n",
    "                    \n",
    "                    i = end_idx\n",
    "                    \n",
    "                elif tag.startswith('I-'):\n",
    "                    # This is an error case - I- tag without preceding B-\n",
    "                    # We'll treat it as B- for robustness\n",
    "                    entity_type = tag[2:]\n",
    "                    \n",
    "                    # Find the end of this entity\n",
    "                    end_idx = i + 1\n",
    "                    while (end_idx < len(sent) and \n",
    "                        sent[end_idx][-1] == f'I-{entity_type}'):\n",
    "                        end_idx += 1\n",
    "                    \n",
    "                    # Single-token entity (just this I-X)\n",
    "                    if end_idx == i + 1:\n",
    "                        bioes_sent.append((word, f'S-{entity_type}'))\n",
    "                    else:\n",
    "                        # Add B- tag for beginning (correcting the I- error)\n",
    "                        bioes_sent.append((word, f'B-{entity_type}'))\n",
    "                        \n",
    "                        # Add I- tags for middle tokens (if any)\n",
    "                        for j in range(i+1, end_idx-1):\n",
    "                            word_j = sent[j][0]\n",
    "                            bioes_sent.append((word_j, f'I-{entity_type}'))\n",
    "                        \n",
    "                        # Add E- tag for the end token\n",
    "                        word_end = sent[end_idx-1][0]\n",
    "                        bioes_sent.append((word_end, f'E-{entity_type}'))\n",
    "                    \n",
    "                    i = end_idx\n",
    "                else:\n",
    "                    # Unknown tag type, pass through unchanged\n",
    "                    bioes_sent.append((word, tag))\n",
    "                    i += 1\n",
    "                    \n",
    "            bioes_tagged_sents.append(bioes_sent)\n",
    "        \n",
    "        return bioes_tagged_sents\n",
    "    # Resto de métodos (to_bioes, to_io, extraccio_entitats, evaluacio_entitats, matriu_confusio) como en el enunciado\n",
    "\n",
    "class NERCRF:\n",
    "    def __init__(self, language: str = 'esp', encoding: str = 'bio', feature_func: Optional[Callable] = None, model_file: Optional[str] = None):\n",
    "        self.language = language\n",
    "        self.encoding = encoding\n",
    "        self.feature_func = feature_func\n",
    "        self.model_file = model_file\n",
    "\n",
    "        # Cargar datos\n",
    "        self.train_raw = self._load_conll_data(language +'.train')\n",
    "        self.dev_raw = self._load_conll_data(language +'.testa')\n",
    "        self.test_raw = self._load_conll_data(language +'.testb')\n",
    "\n",
    "        # Inicializar modelo CRF\n",
    "        self.crf_model = CRFModel(\n",
    "            train=self.train_raw,\n",
    "            test=self.test_raw,\n",
    "            model_file=self.model_file,\n",
    "            features=self.feature_func,\n",
    "            encoding=self.encoding\n",
    "        )\n",
    "\n",
    "    def _load_conll_data(self, split: str) -> List[List[Tuple[str, str]]]:\n",
    "        sents = conll2002.iob_sents(f'{self.language}.{split}')\n",
    "        return [[(word, tag) for (word, _, tag) in sent] for sent in sents]\n",
    "    \n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        self.crf_model.train(self.crf_model.train_data)\n",
    "\n",
    "    def evaluate(self, split: str = 'dev') -> Tuple[float, float, float]:\n",
    "        if split == 'dev':\n",
    "            data = self.dev_raw\n",
    "        elif split == 'test':\n",
    "            data = self.test_raw\n",
    "        else:\n",
    "            raise ValueError(\"split debe ser 'dev' o 'test'\")\n",
    "\n",
    "        # Transformar datos al encoding correcto\n",
    "        data_encoded = self.crf_model.transformar_dades(data)\n",
    "\n",
    "        # Predecir etiquetas\n",
    "        predicted = self.crf_model.predict(data_encoded)\n",
    "\n",
    "        # Extraer entidades\n",
    "        y_true = []\n",
    "        for sent in data_encoded:\n",
    "            entities = self.crf_model.extraccio_entitats([sent])\n",
    "            y_true.extend(entities)\n",
    "\n",
    "        y_pred = []\n",
    "        for sent in predicted:\n",
    "            entities = self.crf_model.extraccio_entitats([sent])\n",
    "            y_pred.extend(entities)\n",
    "\n",
    "        # Evaluar\n",
    "        recall, precision, f1 = self.crf_model.evaluacio_entitats(y_true, y_pred)\n",
    "        return recall, precision, f1\n",
    "\n",
    "    def predict_sentence(self, sentence: List[str]) -> List[Tuple[str, str]]:\n",
    "        formatted_sent = [[(word, 'O') for word in sentence]]\n",
    "        predicted = self.crf_model.predict(formatted_sent)\n",
    "        return predicted[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Función de características de ejemplo\n",
    "    def custom_features(sentence, index):\n",
    "        word = sentence[index][0]\n",
    "        features = {\n",
    "            'word': word,\n",
    "            'word.lower()': word.lower(),\n",
    "            'prefix3': word[:3],\n",
    "            'suffix3': word[-3:],\n",
    "            'prev_word': sentence[index-1][0] if index > 0 else '<START>',\n",
    "            'next_word': sentence[index+1][0] if index < len(sentence)-1 else '<END>',\n",
    "            'is_capitalized': word[0].isupper(),\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    # Entrenar modelo para español con BIOES\n",
    "    spanish_model = NERCRF(\n",
    "        language='esp',\n",
    "        encoding='bioes',\n",
    "        feature_func=custom_features,\n",
    "        model_file='esp_model.crf'\n",
    "    )\n",
    "    spanish_model.train_model()\n",
    "    recall, precision, f1 = spanish_model.evaluate('dev')\n",
    "    print(f\"Resultados para español (BIOES):\\nRecall: {recall:.2f}, Precision: {precision:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "    # Entrenar modelo para neerlandés con IO\n",
    "    dutch_model = NERCRF(\n",
    "        language='ned',\n",
    "        encoding='io',\n",
    "        feature_func=custom_features,\n",
    "        model_file='ned_model.crf'\n",
    "    )\n",
    "    dutch_model.train_model()\n",
    "    recall, precision, f1 = dutch_model.evaluate('test')\n",
    "    print(f\"Resultados para neerlandés (IO):\\nRecall: {recall:.2f}, Precision: {precision:.2f}, F1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef74dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pyconll\n",
      "  Using cached pyconll-3.2.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Using cached pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sambr\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyconll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf2e1e",
   "metadata": {},
   "source": [
    "# Extra: Cadec Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5719 training sentences\n",
      "Loaded 1878 test sentences\n",
      "\n",
      "Sample sentence:\n",
      "pain --> O\n",
      "in --> O\n",
      "my --> O\n",
      "left --> O\n",
      "leg --> O\n",
      "and --> O\n",
      "most --> O\n",
      "of --> O\n",
      "my --> O\n",
      "joints --> O\n",
      ". --> O\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "import os\n",
    "\n",
    "# Adjust working with CADEC-specific data\n",
    "def load_cadec_data(filepath):\n",
    "    \"\"\"Load CADEC data and extract entity types (ADR, Di, Dr, S, F)\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        # Split into sentences (assuming one sentence per line or blank line separation)\n",
    "        raw_sents = [s.strip() for s in raw_text.split('\\n\\n') if s.strip()]\n",
    "        \n",
    "        for sent in raw_sents:\n",
    "            tokens = []\n",
    "            for line in sent.split('\\n'):\n",
    "                if line.strip():\n",
    "                    # Assuming CoNLL format: token pos chunk tag\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 4:  # Ensure we have enough columns\n",
    "                        word = parts[0]\n",
    "                        tag = parts[3]  # Entity tag (O, B-ADR, I-ADR, etc.)\n",
    "                        \n",
    "                        # Strip the identifier, keep only entity type\n",
    "                        if tag != 'O' and '-' in tag:\n",
    "                            prefix, entity_type = tag.split('-', 1)\n",
    "                            # Keep only the entity type (ADR, Di, Dr, S, F), remove identifiers\n",
    "                            if '+' in entity_type:\n",
    "                                entity_type = entity_type.split('+')[0]\n",
    "                            tag = f\"{prefix}-{entity_type}\"\n",
    "                        \n",
    "                        tokens.append((word, tag))\n",
    "            \n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CADEC data: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a44d06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_cadec_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_cadec_data\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cadec/train.conll\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_cadec_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cadec/test.conll\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_cadec_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = load_cadec_data('./cadec/train.conll')\n",
    "test_data = load_cadec_data('./cadec/test.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a182b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
