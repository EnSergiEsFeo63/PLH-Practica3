{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c47af4",
   "metadata": {},
   "source": [
    "# Pràctica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b928f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es =conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned =conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdbb168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n"
     ]
    }
   ],
   "source": [
    "train = conll2002.tagged_sents('esp.train')\n",
    "test = conll2002.tagged_sents('esp.testb')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f56828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CRFTagger\n",
    "model = nltk.tag.CRFTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5193f65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "train_sample = train[0:500] #temps d'entrenament còmicament gran amb totes les dades: pillo un sample més petit\n",
    "print(len(train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aecc1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrfTaggerEs.mdl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\tag\\crf.py:190\u001b[0m, in \u001b[0;36mCRFTagger.train\u001b[1;34m(self, train_data, model_file)\u001b[0m\n\u001b[0;32m    187\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mappend(features, labels)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Now train the model, the output should be model_file\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Save the model file\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model_file(model_file)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:359\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:272\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer._on_message\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pycrfsuite\\_pycrfsuite.pyx:499\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.Trainer.message\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pycrfsuite\\_logparser.py:30\u001b[0m, in \u001b[0;36mTrainLogParser.feed\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog)))\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhandle_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m(line)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(train, 'crfTaggerEs.mdl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549997089243786"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8094b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_features(tokens, idx, features_selection):\n",
    "    \"\"\"\n",
    "    Genera una llista de característiques per a un token específic basat en les característiques seleccionades.\n",
    "\n",
    "    features_selection és un diccionari que indica quines característiques incloure (True o False).\n",
    "\n",
    "    Retorna una llista de característiques per al token donat.\n",
    "    \"\"\"\n",
    "    caracteristiques = []\n",
    "    caracteristiques.append('bias')\n",
    "\n",
    "    token_actual = tokens[idx]\n",
    "\n",
    "    # ------ FORMA DE LA PARAULA ------\n",
    "    if features_selection.get('word_form', True):\n",
    "        caracteristiques.extend([\n",
    "            f'paraula={token_actual}',\n",
    "            f'token_minuscula={token_actual.lower()}'\n",
    "        ])\n",
    "\n",
    "    # ------ PREFIXOS I SUFIXOS ------\n",
    "    if features_selection.get('prefix_suffix', True):\n",
    "        caracteristiques.extend([\n",
    "            f'prefix-1={token_actual[0]}',\n",
    "            f'prefix-2={token_actual[:2]}',\n",
    "            f'prefix-3={token_actual[:3]}',\n",
    "            f'sufix-1={token_actual[-1]}',\n",
    "            f'sufix-2={token_actual[-2:]}',\n",
    "            f'sufix-3={token_actual[-3:]}'\n",
    "        ])\n",
    "\n",
    "    # ------ MORFOLOGIA ------\n",
    "    if features_selection.get('morphology', True):\n",
    "        caracteristiques.extend([\n",
    "            f'està_capitalitzat={token_actual[0].isupper()}',\n",
    "            f'es_majuscules={token_actual.isupper()}',\n",
    "            f'es_minuscules={token_actual.islower()}',\n",
    "            f'té_guion={\"-\" in token_actual}',\n",
    "            f'es_numeric={token_actual.isdigit()}',\n",
    "            f'capitals_internes={token_actual[1:].lower() != token_actual[1:]}'\n",
    "        ])\n",
    "\n",
    "    # ------ LONGITUD ------\n",
    "    if features_selection.get('length', True):\n",
    "        caracteristiques.append(f'longitud={len(token_actual)}')\n",
    "\n",
    "    # ------ POSICIÓ ------\n",
    "    if features_selection.get('position', True):\n",
    "        caracteristiques.extend([\n",
    "            f'es_primer={idx == 0}',\n",
    "            f'es_ultim={idx == len(tokens) - 1}'\n",
    "        ])\n",
    "\n",
    "    # ------ CONTEXT ANTERIOR ------\n",
    "    if idx > 0 and features_selection.get('context', True):\n",
    "        token_anterior = tokens[idx - 1]\n",
    "        caracteristiques.extend([\n",
    "            f'token_anterior={token_anterior.lower()}',\n",
    "            f'anterior_capitalitzat={token_anterior[0].isupper()}',\n",
    "            f'anterior_majuscules={token_anterior.isupper()}',\n",
    "            f'anterior_minuscules={token_anterior.islower()}'\n",
    "        ])\n",
    "\n",
    "    # ------ CONTEXT POSTERIOR ------\n",
    "    if idx < len(tokens) - 1 and features_selection.get('context', True):\n",
    "        token_seguent = tokens[idx + 1]\n",
    "        caracteristiques.extend([\n",
    "            f'token_seguent={token_seguent.lower()}',\n",
    "            f'seguent_capitalitzat={token_seguent[0].isupper()}',\n",
    "            f'seguent_majuscules={token_seguent.isupper()}',\n",
    "            f'seguent_minuscules={token_seguent.islower()}'\n",
    "        ])\n",
    "\n",
    "    return caracteristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a61e13",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generar_features() missing 2 required positional arguments: 'idx' and 'features_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_types \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_form\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_pos_tags\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     }\n\u001b[1;32m---> 10\u001b[0m ct \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mCRFTagger(feature_func\u001b[38;5;241m=\u001b[39m\u001b[43mgenerar_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m ct\u001b[38;5;241m.\u001b[39mtrain(train[:\u001b[38;5;241m100\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnooooooolapolitziiaa.mdl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: generar_features() missing 2 required positional arguments: 'idx' and 'features_selection'"
     ]
    }
   ],
   "source": [
    "feature_types = {\n",
    "        'word_form': True,\n",
    "        'lemma_pos_tags': False,\n",
    "        'prefix_suffix': True,\n",
    "        'morphology': True,\n",
    "        'length': True,\n",
    "        'position': True,\n",
    "        'context': True\n",
    "    }\n",
    "ct = nltk.tag.CRFTagger(feature_func=generar_features(feature_types))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85732d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {}\n",
    "        word = sentence[index][0]  # Always get the first element (word)\n",
    "        pos = sentence[index][1] if len(sentence[index]) > 1 else None  # Get POS if available\n",
    "        \n",
    "        # Características básicas de la palabra\n",
    "        if feature_selection.get(\"word_form\", True):\n",
    "            features.update({\n",
    "                \"word\": word,\n",
    "                \"word.lower\": word.lower(),\n",
    "                \"word.istitle\": word.istitle(),\n",
    "                \"word.isupper\": word.isupper(),\n",
    "                \"word.isdigit\": word.isdigit(),\n",
    "            })\n",
    "        \n",
    "        # POS tagging y lemas\n",
    "        if feature_selection.get(\"lemma_pos_tags\", True) and pos:\n",
    "            features[\"pos\"] = pos\n",
    "        \n",
    "        # Prefijos y sufijos\n",
    "        if feature_selection.get(\"prefix_suffix\", True):\n",
    "            features.update({\n",
    "                \"prefix3\": word[:3],\n",
    "                \"suffix3\": word[-3:],\n",
    "                \"prefix2\": word[:2],\n",
    "                \"suffix2\": word[-2:],\n",
    "            })\n",
    "        \n",
    "        # Características morfológicas\n",
    "        if feature_selection.get(\"morphology\", True):\n",
    "            features.update({\n",
    "                \"hyphen\": \"-\" in word,\n",
    "                \"has_digit\": any(c.isdigit() for c in word),\n",
    "                \"shape\": \"\".join([\n",
    "                    \"X\" if c.isupper() else \n",
    "                    \"x\" if c.islower() else \n",
    "                    \"d\" if c.isdigit() else c \n",
    "                    for c in word\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        # Longitud de la palabra\n",
    "        if feature_selection.get(\"length\", True):\n",
    "            features[\"length\"] = len(word)\n",
    "        \n",
    "        # Posición en la oración\n",
    "        if feature_selection.get(\"position\", True):\n",
    "            features.update({\n",
    "                \"position\": index,\n",
    "                \"is_first\": index == 0,\n",
    "                \"is_last\": index == len(sentence)-1\n",
    "            })\n",
    "        \n",
    "        # Contexto circundante\n",
    "        if feature_selection.get(\"context\", True):\n",
    "            if index > 0:\n",
    "                prev_word = sentence[index-1][0]\n",
    "                features.update({\n",
    "                    \"prev_word\": prev_word,\n",
    "                    \"prev_word.lower\": prev_word.lower(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle(),\n",
    "                })\n",
    "            if index < len(sentence)-1:\n",
    "                next_word = sentence[index+1][0]\n",
    "                features.update({\n",
    "                    \"next_word\": next_word,\n",
    "                    \"next_word.lower\": next_word.lower(),\n",
    "                    \"next_word.istitle\": next_word.istitle(),\n",
    "                })\n",
    "        \n",
    "        # Características adicionales\n",
    "        features.update({\n",
    "            \"bias\": 1.0,  # Término de sesgo\n",
    "            \"word.isalnum\": word.isalnum(),\n",
    "            \"capital_inside\": word[1:].lower() != word[1:] if len(word) > 1 else False\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc763e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7088467583878292"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection = {\n",
    "    \"word_form\": True,           # Basic word characteristics\n",
    "    \"prefix_suffix\": True,       # Prefixes and suffixes\n",
    "    \"morphology\": True,          # Morphological features\n",
    "    \"context\": True,             # Surrounding words\n",
    "    \"extended_context\": True,    # Wider context window (±2)\n",
    "    \"language_specific\": True,   # Spanish/Dutch specific features\n",
    "    \"sentence_position\": True,   # Position in sentence features\n",
    "    \"case_patterns\": True,       # Capitalization patterns\n",
    "    \"date_time_patterns\": True,  # Date and time patterns\n",
    "    \"numeric_patterns\": True,    # Numeric features\n",
    "    \"symbol_patterns\": True,     # Special characters\n",
    "    \"length\": True,              # Word length features\n",
    "    \"orthographic_features\": True, # Complex orthographic patterns\n",
    "    \"position\": True,            # Position features\n",
    "    \"suffix_long\": True,         # Longer suffixes (4+ chars)\n",
    "    \"common_ending\": True        # Common word endings\n",
    "}\n",
    "\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")\n",
    "ct.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BIO tags:\n",
      "Melbourne -> B-LOC\n",
      "( -> O\n",
      "Australia -> B-LOC\n",
      ") -> O\n",
      ", -> O\n",
      "25 -> O\n",
      "may -> O\n",
      "( -> O\n",
      "EFE -> B-ORG\n",
      ") -> O\n",
      "\n",
      "Converted to IO tags:\n",
      "Melbourne -> I-LOC\n",
      "( -> O\n",
      "Australia -> I-LOC\n",
      ") -> O\n",
      ", -> O\n",
      "25 -> O\n",
      "may -> O\n",
      "( -> O\n",
      "EFE -> I-ORG\n",
      ") -> O\n"
     ]
    }
   ],
   "source": [
    "def bio_to_io(tagged_sents):\n",
    "    \"\"\"\n",
    "    Convert BIO tagging to IO tagging.\n",
    "    \n",
    "    In IO format, we don't distinguish between beginning (B-) and inside (I-) of an entity.\n",
    "    All entity tokens are labeled as I-X.\n",
    "    \n",
    "    Args:\n",
    "        tagged_sents: List of sentences with BIO tags\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences with IO tags\n",
    "    \"\"\"\n",
    "    io_tagged_sents = []\n",
    "    \n",
    "    for sent in tagged_sents:\n",
    "        io_sent = []\n",
    "        for token in sent:\n",
    "            word = token[0]  # Get the word\n",
    "            tag = token[-1]  # Get the BIO tag\n",
    "            \n",
    "            # Convert B- to I- (Beginning to Inside)\n",
    "            if tag.startswith('B-'):\n",
    "                tag = 'I-' + tag[2:]\n",
    "                \n",
    "            io_sent.append((word, tag))\n",
    "        io_tagged_sents.append(io_sent)\n",
    "    \n",
    "    return io_tagged_sents\n",
    "\n",
    "# Example usage\n",
    "printable_example = train_es[0][:10]  # First 10 tokens of first sentence\n",
    "print(\"Original BIO tags:\")\n",
    "for token in printable_example:\n",
    "    print(f\"{token[0]} -> {token[-1]}\")\n",
    "\n",
    "# Convert to IO\n",
    "io_example = bio_to_io([printable_example])[0]\n",
    "print(\"\\nConverted to IO tags:\")\n",
    "for token in io_example:\n",
    "    print(f\"{token[0]} -> {token[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959943eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted to BIOW tags:\n",
      "Melbourne -> W-LOC\n",
      "( -> O\n",
      "Australia -> W-LOC\n",
      ") -> O\n",
      ", -> O\n",
      "25 -> O\n",
      "may -> O\n",
      "( -> O\n",
      "EFE -> W-ORG\n",
      ") -> O\n"
     ]
    }
   ],
   "source": [
    "def bio_to_biow(tagged_sents):\n",
    "    \"\"\"\n",
    "    Convert BIO tagging to BIOW tagging.\n",
    "    \n",
    "    In BIOW format, we distinguish between:\n",
    "    - B-X: Beginning of multi-token entity\n",
    "    - I-X: Inside of entity\n",
    "    - W-X: Single-token entity (Word)\n",
    "    - O: Outside any entity\n",
    "    \n",
    "    Args:\n",
    "        tagged_sents: List of sentences with BIO tags\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences with BIOW tags\n",
    "    \"\"\"\n",
    "    biow_tagged_sents = []\n",
    "    \n",
    "    for sent in tagged_sents:\n",
    "        biow_sent = []\n",
    "        i = 0\n",
    "        while i < len(sent):\n",
    "            word = sent[i][0]  # Get the word\n",
    "            tag = sent[i][-1]  # Get the BIO tag\n",
    "            \n",
    "            # If it's a B- tag, check if it's a single token entity\n",
    "            if tag.startswith('B-'):\n",
    "                entity_type = tag[2:]  # Get entity type (PER, LOC, etc.)\n",
    "                \n",
    "                # Check if next token continues this entity\n",
    "                if i + 1 < len(sent) and sent[i+1][-1] == f'I-{entity_type}':\n",
    "                    # Multi-token entity, keep as B-\n",
    "                    biow_sent.append((word, tag))\n",
    "                else:\n",
    "                    # Single token entity, convert to W-\n",
    "                    biow_sent.append((word, f'W-{entity_type}'))\n",
    "            else:\n",
    "                # Keep I- and O tags as they are\n",
    "                biow_sent.append((word, tag))\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "        biow_tagged_sents.append(biow_sent)\n",
    "    \n",
    "    return biow_tagged_sents\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nConverted to BIOW tags:\")\n",
    "biow_example = bio_to_biow([printable_example])[0]\n",
    "for token in biow_example:\n",
    "    print(f\"{token[0]} -> {token[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d8d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5 sentences to IO and BIOW schemes\n",
      "\n",
      "BIO tag counts:\n",
      "B-LOC: 4\n",
      "B-MISC: 2\n",
      "B-ORG: 2\n",
      "B-PER: 3\n",
      "I-ORG: 1\n",
      "I-PER: 5\n",
      "O: 146\n",
      "\n",
      "IO tag counts:\n",
      "I-LOC: 4\n",
      "I-MISC: 2\n",
      "I-ORG: 3\n",
      "I-PER: 8\n",
      "O: 146\n",
      "\n",
      "BIOW tag counts:\n",
      "B-ORG: 1\n",
      "B-PER: 3\n",
      "I-ORG: 1\n",
      "I-PER: 5\n",
      "O: 146\n",
      "W-LOC: 4\n",
      "W-MISC: 2\n",
      "W-ORG: 1\n"
     ]
    }
   ],
   "source": [
    "def convert_tagging_scheme(tagged_sents, source_scheme='bio', target_scheme='io'):\n",
    "    \"\"\"\n",
    "    Convert between different tagging schemes.\n",
    "    \n",
    "    Args:\n",
    "        tagged_sents: List of sentences with tags\n",
    "        source_scheme: Source tagging scheme ('bio', 'io', 'biow')\n",
    "        target_scheme: Target tagging scheme ('bio', 'io', 'biow')\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences with converted tags\n",
    "    \"\"\"\n",
    "    # First convert to BIO if needed (not implemented here)\n",
    "    if source_scheme.lower() != 'bio':\n",
    "        # You would need to implement IO to BIO and BIOW to BIO converters\n",
    "        raise NotImplementedError(f\"Conversion from {source_scheme} to BIO not implemented\")\n",
    "    \n",
    "    # Then convert from BIO to target scheme\n",
    "    if target_scheme.lower() == 'io':\n",
    "        return bio_to_io(tagged_sents)\n",
    "    elif target_scheme.lower() == 'biow':\n",
    "        return bio_to_biow(tagged_sents)\n",
    "    elif target_scheme.lower() == 'bio':\n",
    "        return tagged_sents  # No conversion needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target scheme: {target_scheme}\")\n",
    "\n",
    "# Convert a sample of the training data to both schemes\n",
    "sample_sents = train_es[:5]  # First 5 sentences\n",
    "io_sents = convert_tagging_scheme(sample_sents, 'bio', 'io')\n",
    "biow_sents = convert_tagging_scheme(sample_sents, 'bio', 'biow')\n",
    "\n",
    "print(f\"Converted {len(sample_sents)} sentences to IO and BIOW schemes\")\n",
    "\n",
    "# Count tag occurrences in each scheme\n",
    "def count_tags(tagged_sents):\n",
    "    tag_counts = {}\n",
    "    for sent in tagged_sents:\n",
    "        for token in sent:\n",
    "            tag = token[-1]\n",
    "            tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "    return tag_counts\n",
    "\n",
    "bio_counts = count_tags(sample_sents)\n",
    "io_counts = count_tags(io_sents)\n",
    "biow_counts = count_tags(biow_sents)\n",
    "\n",
    "print(\"\\nBIO tag counts:\")\n",
    "for tag, count in sorted(bio_counts.items()):\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "print(\"\\nIO tag counts:\")\n",
    "for tag, count in sorted(io_counts.items()):\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "print(\"\\nBIOW tag counts:\")\n",
    "for tag, count in sorted(biow_counts.items()):\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d9b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01758096753536569"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Converting all training and test data\n",
    "\n",
    "# You would typically convert all your data like this:\n",
    "def prepare_converted_data(language='spanish', target_scheme='io'):\n",
    "    train, dev, test = data[language]\n",
    "    \n",
    "    # Convert to target scheme\n",
    "    train_converted = convert_tagging_scheme(train, 'bio', target_scheme)\n",
    "    dev_converted = convert_tagging_scheme(dev, 'bio', target_scheme)\n",
    "    test_converted = convert_tagging_scheme(test, 'bio', target_scheme)\n",
    "    \n",
    "    return train_converted, dev_converted, test_converted\n",
    "\n",
    "# For demonstration, we'll just convert a small sample\n",
    "train_io, dev_io, test_io = prepare_converted_data('spanish','io')\n",
    "train_biow, dev_biow, test_biow = prepare_converted_data('spanish','biow')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_io = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "model_io.train(train_io, \"crfTaggerEs_IO.mdl\")\n",
    "model_io.accuracy(test_io)\n",
    "# \n",
    "model_biow = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "model_biow.train(train_biow, \"crfTaggerEs_BIOW.mdl\")\n",
    "model_biow.accuracy(test_biow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bd90d",
   "metadata": {},
   "source": [
    "# Extra: CADEC Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bc6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inconsistent number of columns:\nLIPITOR.408\npain\tB-10033446\tO\tO\tO\tO\nin\tI-10033446\tO\tO\tO\tO\nmy\tI-10033446\tO\tO\tO\tO\nleft\tI-10033446\tO\tO\tO\tO\nleg\tI-10033446\tO\tO\tO\tO\nand\tO\tO\tO\tO\tO\nmost\tO\tO\tO\tO\tO\nof\tO\tO\tO\tO\tO\nmy\tO\tO\tO\tO\tO\njoints\tI-10003239\tO\tO\tO\tO\n.\tO\tO\tO\tO\tO",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m test_tagged_sents \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mtagged_sents()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Print sample information to verify\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of train sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_tagged_sents\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of test sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_tagged_sents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample from training data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\collections.py:471\u001b[0m, in \u001b[0;36mLazyMap.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lists\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\collections.py:471\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m lst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lists)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\reader\\util.py:240\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;66;03m# iterate_from() sets self._len when it reaches the end\u001b[39;00m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;66;03m# of the file:\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_toknum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_len\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\reader\\util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n\u001b[1;32m--> 306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    310\u001b[0m )\n\u001b[0;32m    311\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\reader\\conll.py:231\u001b[0m, in \u001b[0;36mConllCorpusReader._read_grid_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m grid:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(grid[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 231\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent number of columns:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m block)\n\u001b[0;32m    232\u001b[0m     grids\u001b[38;5;241m.\u001b[39mappend(grid)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grids\n",
      "\u001b[1;31mValueError\u001b[0m: Inconsistent number of columns:\nLIPITOR.408\npain\tB-10033446\tO\tO\tO\tO\nin\tI-10033446\tO\tO\tO\tO\nmy\tI-10033446\tO\tO\tO\tO\nleft\tI-10033446\tO\tO\tO\tO\nleg\tI-10033446\tO\tO\tO\tO\nand\tO\tO\tO\tO\tO\nmost\tO\tO\tO\tO\tO\nof\tO\tO\tO\tO\tO\nmy\tO\tO\tO\tO\tO\njoints\tI-10003239\tO\tO\tO\tO\n.\tO\tO\tO\tO\tO"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "# Define column mappings (adjust to your CoNLL format)\n",
    "column_types = ['words', 'pos', 'ignore', 'ne']  # Example for POS/NER\n",
    "\n",
    "# Load the file\n",
    "train = ConllCorpusReader(\n",
    "    './cadec',           # Directory path\n",
    "    'train.conll',       # Filename\n",
    "    columntypes=column_types\n",
    ")\n",
    "test = ConllCorpusReader(\n",
    "    './cadec',           # Directory path\n",
    "    'test.conll',        # Filename\n",
    "    columntypes=column_types\n",
    ")\n",
    "\n",
    "# Access data - Using appropriate methods\n",
    "# Getting tagged sentences (word, tag pairs)\n",
    "train_tagged_sents = train.tagged_sents()\n",
    "test_tagged_sents = test.tagged_sents()\n",
    "\"\"\"\n",
    "# Print sample information to verify\n",
    "print(f\"Number of train sentences: {len(train_tagged_sents)}\")\n",
    "print(f\"Number of test sentences: {len(test_tagged_sents)}\")\n",
    "print(\"\\nSample from training data:\")\n",
    "if train_tagged_sents:\n",
    "    print(train_tagged_sents[0][:10])  # First 10 tokens of first sentence\n",
    "\n",
    "# Get words from the corpus\n",
    "words = train.words()\n",
    "print(f\"\\nNumber of words in training data: {len(words)}\")\n",
    "print(f\"Sample words: {words[:20]}\")  # First 20 words\n",
    "\n",
    "# If you need NER tags specifically, you can extract them from tagged_sents\n",
    "# This assumes your NE tags are in the format you specified in column_types\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
