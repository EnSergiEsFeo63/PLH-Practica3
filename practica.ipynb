{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c47af4",
   "metadata": {},
   "source": [
    "# Pràctica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b928f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\11ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "train_es = conll2002.iob_sents('esp.train') # Train\n",
    "dev_es = conll2002.iob_sents('esp.testa') # Dev\n",
    "test_es =conll2002.iob_sents('esp.testb') # Test\n",
    "\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "dev_ned = conll2002.iob_sents('ned.testa') # Dev\n",
    "test_ned =conll2002.iob_sents('ned.testb') # Test\n",
    "\n",
    "data = {'spanish': (train_es, dev_es, test_es),\n",
    "        'dutch': (train_ned, dev_ned, test_ned)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdbb168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8323\n"
     ]
    }
   ],
   "source": [
    "train = conll2002.tagged_sents('esp.train')\n",
    "test = conll2002.tagged_sents('esp.testb')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f56828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CRFTagger\n",
    "model = nltk.tag.CRFTagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5193f65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "train_sample = train[0:500] #temps d'entrenament còmicament gran amb totes les dades: pillo un sample més petit\n",
    "print(len(train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aecc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train, 'crfTaggerEs.mdl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b0faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549997089243786"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_features(tokens, idx, features_selection):\n",
    "    \"\"\"\n",
    "    Genera una llista de característiques per a un token específic basat en les característiques seleccionades.\n",
    "\n",
    "    features_selection és un diccionari que indica quines característiques incloure (True o False).\n",
    "\n",
    "    Retorna una llista de característiques per al token donat.\n",
    "    \"\"\"\n",
    "    caracteristiques = []\n",
    "    caracteristiques.append('bias')\n",
    "\n",
    "    token_actual = tokens[idx]\n",
    "\n",
    "    # ------ FORMA DE LA PARAULA ------\n",
    "    if features_selection.get('word_form', True):\n",
    "        caracteristiques.extend([\n",
    "            f'paraula={token_actual}',\n",
    "            f'token_minuscula={token_actual.lower()}'\n",
    "        ])\n",
    "\n",
    "    # ------ PREFIXOS I SUFIXOS ------\n",
    "    if features_selection.get('prefix_suffix', True):\n",
    "        caracteristiques.extend([\n",
    "            f'prefix-1={token_actual[0]}',\n",
    "            f'prefix-2={token_actual[:2]}',\n",
    "            f'prefix-3={token_actual[:3]}',\n",
    "            f'sufix-1={token_actual[-1]}',\n",
    "            f'sufix-2={token_actual[-2:]}',\n",
    "            f'sufix-3={token_actual[-3:]}'\n",
    "        ])\n",
    "\n",
    "    # ------ MORFOLOGIA ------\n",
    "    if features_selection.get('morphology', True):\n",
    "        caracteristiques.extend([\n",
    "            f'està_capitalitzat={token_actual[0].isupper()}',\n",
    "            f'es_majuscules={token_actual.isupper()}',\n",
    "            f'es_minuscules={token_actual.islower()}',\n",
    "            f'té_guion={\"-\" in token_actual}',\n",
    "            f'es_numeric={token_actual.isdigit()}',\n",
    "            f'capitals_internes={token_actual[1:].lower() != token_actual[1:]}'\n",
    "        ])\n",
    "\n",
    "    # ------ LONGITUD ------\n",
    "    if features_selection.get('length', True):\n",
    "        caracteristiques.append(f'longitud={len(token_actual)}')\n",
    "\n",
    "    # ------ POSICIÓ ------\n",
    "    if features_selection.get('position', True):\n",
    "        caracteristiques.extend([\n",
    "            f'es_primer={idx == 0}',\n",
    "            f'es_ultim={idx == len(tokens) - 1}'\n",
    "        ])\n",
    "\n",
    "    # ------ CONTEXT ANTERIOR ------\n",
    "    if idx > 0 and features_selection.get('context', True):\n",
    "        token_anterior = tokens[idx - 1]\n",
    "        caracteristiques.extend([\n",
    "            f'token_anterior={token_anterior.lower()}',\n",
    "            f'anterior_capitalitzat={token_anterior[0].isupper()}',\n",
    "            f'anterior_majuscules={token_anterior.isupper()}',\n",
    "            f'anterior_minuscules={token_anterior.islower()}'\n",
    "        ])\n",
    "\n",
    "    # ------ CONTEXT POSTERIOR ------\n",
    "    if idx < len(tokens) - 1 and features_selection.get('context', True):\n",
    "        token_seguent = tokens[idx + 1]\n",
    "        caracteristiques.extend([\n",
    "            f'token_seguent={token_seguent.lower()}',\n",
    "            f'seguent_capitalitzat={token_seguent[0].isupper()}',\n",
    "            f'seguent_majuscules={token_seguent.isupper()}',\n",
    "            f'seguent_minuscules={token_seguent.islower()}'\n",
    "        ])\n",
    "\n",
    "    return caracteristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a61e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = {\n",
    "        'word_form': True,\n",
    "        'lemma_pos_tags': False,\n",
    "        'prefix_suffix': True,\n",
    "        'morphology': True,\n",
    "        'length': True,\n",
    "        'position': True,\n",
    "        'context': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85732d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {}\n",
    "        word = sentence[index][0]  # Always get the first element (word)\n",
    "        pos = sentence[index][1] if len(sentence[index]) > 1 else None  # Get POS if available\n",
    "        \n",
    "        # Características básicas de la palabra\n",
    "        if feature_selection.get(\"word_form\", True):\n",
    "            features.update({\n",
    "                \"word\": word,\n",
    "                \"word.lower\": word.lower(),\n",
    "                \"word.istitle\": word.istitle(),\n",
    "                \"word.isupper\": word.isupper(),\n",
    "                \"word.isdigit\": word.isdigit(),\n",
    "            })\n",
    "        \n",
    "        # POS tagging y lemas\n",
    "        if feature_selection.get(\"lemma_pos_tags\", True) and pos:\n",
    "            features[\"pos\"] = pos\n",
    "        \n",
    "        # Prefijos y sufijos\n",
    "        if feature_selection.get(\"prefix_suffix\", True):\n",
    "            features.update({\n",
    "                \"prefix3\": word[:3],\n",
    "                \"suffix3\": word[-3:],\n",
    "                \"prefix2\": word[:2],\n",
    "                \"suffix2\": word[-2:],\n",
    "            })\n",
    "        \n",
    "        # Características morfológicas\n",
    "        if feature_selection.get(\"morphology\", True):\n",
    "            features.update({\n",
    "                \"hyphen\": \"-\" in word,\n",
    "                \"has_digit\": any(c.isdigit() for c in word),\n",
    "                \"shape\": \"\".join([\n",
    "                    \"X\" if c.isupper() else \n",
    "                    \"x\" if c.islower() else \n",
    "                    \"d\" if c.isdigit() else c \n",
    "                    for c in word\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        # Longitud de la palabra\n",
    "        if feature_selection.get(\"length\", True):\n",
    "            features[\"length\"] = len(word)\n",
    "        \n",
    "        # Posición en la oración\n",
    "        if feature_selection.get(\"position\", True):\n",
    "            features.update({\n",
    "                \"position\": index,\n",
    "                \"is_first\": index == 0,\n",
    "                \"is_last\": index == len(sentence)-1\n",
    "            })\n",
    "        \n",
    "        # Contexto circundante\n",
    "        if feature_selection.get(\"context\", True):\n",
    "            if index > 0:\n",
    "                prev_word = sentence[index-1][0]\n",
    "                features.update({\n",
    "                    \"prev_word\": prev_word,\n",
    "                    \"prev_word.lower\": prev_word.lower(),\n",
    "                    \"prev_word.istitle\": prev_word.istitle(),\n",
    "                })\n",
    "            if index < len(sentence)-1:\n",
    "                next_word = sentence[index+1][0]\n",
    "                features.update({\n",
    "                    \"next_word\": next_word,\n",
    "                    \"next_word.lower\": next_word.lower(),\n",
    "                    \"next_word.istitle\": next_word.istitle(),\n",
    "                })\n",
    "        \n",
    "        # Características adicionales\n",
    "        features.update({\n",
    "            \"bias\": 1.0,  # Término de sesgo\n",
    "            \"word.isalnum\": word.isalnum(),\n",
    "            \"capital_inside\": word[1:].lower() != word[1:] if len(word) > 1 else False\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc763e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = {\n",
    "    \"word_form\": True,\n",
    "    \"prefix_suffix\": True,\n",
    "    \"morphology\": True,\n",
    "    \"context\": True,\n",
    "    # ... otros parámetros\n",
    "}\n",
    "\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b13290d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7088467583878292"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {}\n",
    "        word = sentence[index]\n",
    "        symbols = {'@', '#', '$', '%', '&', '*', '-', '/', ':', '€', '£', '¥'}\n",
    "        \n",
    "        # ========================\n",
    "        # 1. PREFIJOS Y SUFIJOS\n",
    "        # ========================\n",
    "        if feature_selection['prefix_suffix']:\n",
    "            # Prefijos estándar\n",
    "            for n in [2, 3]:\n",
    "                if len(word) >= n:\n",
    "                    features[f'prefix{n}'] = word[:n]\n",
    "                    features[f'suffix{n}'] = word[-n:]\n",
    "            \n",
    "            # Suffijos largos (4+ caracteres)\n",
    "            if feature_selection['suffix_long'] and len(word) >= 4:\n",
    "                features.update({\n",
    "                    'prefix4': word[:4],\n",
    "                    'suffix4': word[-4:],\n",
    "                    'suffix5': word[-5:] if len(word) >=5 else None\n",
    "                })\n",
    "\n",
    "        # ========================\n",
    "        # 2. LONGITUD DE PALABRA\n",
    "        # ========================\n",
    "        if feature_selection['length']:\n",
    "            word_len = len(word)\n",
    "            features.update({\n",
    "                'length': word_len,\n",
    "                'length_bin': 'short' if word_len <4 else 'medium' if 4<=word_len<=7 else 'long',\n",
    "                'is_single_char': word_len == 1\n",
    "            })\n",
    "\n",
    "        # ========================\n",
    "        # 3. PATRONES DE SÍMBOLOS\n",
    "        # ========================\n",
    "        if feature_selection['symbol_patterns']:\n",
    "            symbol_features = {\n",
    "                'contains_symbol': any(c in symbols for c in word),\n",
    "                'symbol_count': sum(1 for c in word if c in symbols),\n",
    "                'is_hashtag': word.startswith('#') and len(word) > 1,\n",
    "                'is_mention': word.startswith('@') and len(word) > 1,\n",
    "                'is_url': re.match(r'^https?://', word) is not None\n",
    "            }\n",
    "            features.update(symbol_features)\n",
    "\n",
    "            # Detección de combinaciones especiales\n",
    "            if '-' in word:\n",
    "                features['hyphen_combinations'] = word.count('-')\n",
    "            if '/' in word:\n",
    "                features['slash_pattern'] = True\n",
    "        \n",
    "        # 1. Características léxicas básicas\n",
    "        if feature_selection['word_form']:\n",
    "            features.update({\n",
    "                'word': word,\n",
    "                'word.lower': word.lower(),\n",
    "                'word.istitle': word.istitle(),\n",
    "                'word.isupper': word.isupper(),\n",
    "                'word.isdigit': word.isdigit(),\n",
    "                'word.isalnum': word.isalnum(),\n",
    "            })\n",
    "        \n",
    "        # 2. Morfología avanzada\n",
    "        if feature_selection['morphology']:\n",
    "            features.update({\n",
    "                'hyphen': '-' in word,\n",
    "                'dot': '.' in word,\n",
    "                'apostrophe': \"'\" in word,\n",
    "                'shape': ''.join([\n",
    "                    'A' if c.isalpha() else \n",
    "                    'N' if c.isdigit() else \n",
    "                    c for c in word\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        # 3. Patrones de mayúsculas complejos\n",
    "        if feature_selection['case_patterns']:\n",
    "            features.update({\n",
    "                'all_caps': word == word.upper(),\n",
    "                'mixed_case': word != word.lower() and word != word.upper(),\n",
    "                'capital_inside': word[1:] != word[1:].lower() if len(word) > 1 else False\n",
    "            })\n",
    "        \n",
    "        # 4. Prefijos y sufijos extendidos\n",
    "        if feature_selection['prefix_suffix']:\n",
    "            for n in [2,3,4]:\n",
    "                if len(word) >= n:\n",
    "                    features[f'prefix{n}'] = word[:n]\n",
    "                    features[f'suffix{n}'] = word[-n:]\n",
    "        \n",
    "        # 5. Detección de patrones temporales/númericos\n",
    "        if feature_selection['date_time_patterns']:\n",
    "            features['is_date'] = any(c in word for c in ['/', '-', ':'])\n",
    "            features['has_year'] = len(word) == 4 and word.isdigit()\n",
    "        \n",
    "        # 6. Contexto extendido (ventana ±2)\n",
    "        if feature_selection['extended_context']:\n",
    "            for i in range(-2, 3):\n",
    "                if i != 0 and 0 <= index+i < len(sentence):\n",
    "                    ctx_word = sentence[index+i][0]\n",
    "                    features[f'ctx_{i}'] = ctx_word.lower()\n",
    "        \n",
    "        # 7. Características posicionales avanzadas\n",
    "        if feature_selection['sentence_position']:\n",
    "            total_words = len(sentence)\n",
    "            features.update({\n",
    "                'position_quartile': int(4 * index / total_words),\n",
    "                'is_first_10%': index < total_words * 0.1\n",
    "            })\n",
    "        \n",
    "        # 8. Características idiomáticas (ejemplo para español)\n",
    "        if feature_selection['language_specific']:\n",
    "            features.update({\n",
    "                'has_accent': any(c in 'áéíóúñ' for c in word.lower()),\n",
    "                'common_ending': word[-2:] in ['os', 'as', 'es']  # Típico en español\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4724835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240156792734753"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection = {\n",
    "    \"word_form\": True,\n",
    "    \"prefix_suffix\": True,\n",
    "    \"morphology\": True,\n",
    "    \"context\": True,\n",
    "    \"extended_context\": True,\n",
    "    \"language_specific\": True,\n",
    "    \"sentence_position\": True,\n",
    "    \"case_patterns\": True,\n",
    "    \"date_time_patterns\": True,\n",
    "    \"common_ending\": True,\n",
    "}\n",
    "\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_selection))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")\n",
    "ct.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = {\n",
    "    'word_form': True,\n",
    "    'prefix_suffix': True,\n",
    "    'case_patterns': True,\n",
    "    'context': True,\n",
    "    'morphology': True,\n",
    "    'date_time_patterns': True,\n",
    "    'numeric_patterns': True,\n",
    "    'orthographic_features': True,\n",
    "    'symbol_patterns': True,\n",
    "    'length': True,\n",
    "    'language_specific': True,  # Esencial para español/neerlandés\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baf7bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_features(feature_selection):\n",
    "    def feature_function(sentence, index):\n",
    "        features = {}\n",
    "        print(sentence[index])\n",
    "        word,pos = sentence[index]\n",
    "        symbols = {'@', '#', '$', '%', '&', '*', '-', '/', ':', '€', '£', '¥'}\n",
    "        total_words = len(sentence)\n",
    "        \n",
    "        # ========================\n",
    "        # 1. COMMON ENDINGS (IDIOMÁTICOS)\n",
    "        # ========================\n",
    "        if feature_selection['language_specific']:\n",
    "            # Español\n",
    "            spanish_endings = {'ción', 'mente', 'dad', 'ero', 'era', 'ado', 'ada', 'ía'}\n",
    "            features['common_ending_es'] = any(word.lower().endswith(e) for e in spanish_endings)\n",
    "            \n",
    "            # Neerlandés\n",
    "            dutch_endings = {'heid', 'ing', 'teit', 'schap', 'lijk', 'baar', 'nis', 'tie'}\n",
    "            features['common_ending_nl'] = any(word.lower().endswith(e) for e in dutch_endings)\n",
    "        \n",
    "        # ========================\n",
    "        # 2. LEMMA Y POS TAGS\n",
    "        # ========================\n",
    "        if feature_selection['lemma_pos_tags']:\n",
    "            features.update({\n",
    "                'pos': pos,\n",
    "                'pos_prefix': pos.split('|')[0] if '|' in pos else pos,  # Para tags compuestos\n",
    "                'pos_coarse': pos[0] if len(pos) > 0 else 'U'  # Primera letra del tag\n",
    "            })\n",
    "        \n",
    "        # ========================\n",
    "        # 3. POSICIÓN EN LA FRASE\n",
    "        # ========================\n",
    "        if feature_selection['sentence_position']:\n",
    "            position_ratio = index / total_words if total_words > 0 else 0\n",
    "            features.update({\n",
    "                'position_ratio': round(position_ratio, 2),\n",
    "                'position_segment': int(position_ratio * 5),  # 0-4 (quintiles)\n",
    "                'is_first_5%': position_ratio < 0.05,\n",
    "                'is_last_5%': position_ratio > 0.95,\n",
    "                'middle_50%': 0.25 <= position_ratio <= 0.75\n",
    "            })\n",
    "        # ========================\n",
    "        # 1. PREFIJOS Y SUFIJOS\n",
    "        # ========================\n",
    "        if feature_selection['prefix_suffix']:\n",
    "            # Prefijos estándar\n",
    "            for n in [2, 3]:\n",
    "                if len(word) >= n:\n",
    "                    features[f'prefix{n}'] = word[:n]\n",
    "                    features[f'suffix{n}'] = word[-n:]\n",
    "            \n",
    "            # Suffijos largos (4+ caracteres)\n",
    "            if feature_selection['suffix_long'] and len(word) >= 4:\n",
    "                features.update({\n",
    "                    'prefix4': word[:4],\n",
    "                    'suffix4': word[-4:],\n",
    "                    'suffix5': word[-5:] if len(word) >=5 else None\n",
    "                })\n",
    "\n",
    "        # ========================\n",
    "        # 2. LONGITUD DE PALABRA\n",
    "        # ========================\n",
    "        if feature_selection['length']:\n",
    "            word_len = len(word)\n",
    "            features.update({\n",
    "                'length': word_len,\n",
    "                'length_bin': 'short' if word_len <4 else 'medium' if 4<=word_len<=7 else 'long',\n",
    "                'is_single_char': word_len == 1\n",
    "            })\n",
    "\n",
    "        # ========================\n",
    "        # 3. PATRONES DE SÍMBOLOS\n",
    "        # ========================\n",
    "        if feature_selection['symbol_patterns']:\n",
    "            symbol_features = {\n",
    "                'contains_symbol': any(c in symbols for c in word),\n",
    "                'symbol_count': sum(1 for c in word if c in symbols),\n",
    "                'is_hashtag': word.startswith('#') and len(word) > 1,\n",
    "                'is_mention': word.startswith('@') and len(word) > 1,\n",
    "                'is_url': re.match(r'^https?://', word) is not None\n",
    "            }\n",
    "            features.update(symbol_features)\n",
    "\n",
    "            # Detección de combinaciones especiales\n",
    "            if '-' in word:\n",
    "                features['hyphen_combinations'] = word.count('-')\n",
    "            if '/' in word:\n",
    "                features['slash_pattern'] = True\n",
    "        \n",
    "        # ====================\n",
    "        # 1. FORMA DE PALABRA\n",
    "        # ====================\n",
    "        if feature_selection['word_form']:\n",
    "            features.update({\n",
    "                'word.lower': word.lower(),\n",
    "                'word.istitle': word.istitle(),\n",
    "                'word.isupper': word.isupper(),\n",
    "                'word.isdigit': word.isdigit(),\n",
    "                'word.isalnum': word.isalnum(),\n",
    "                'word[:4]': word[:4] if len(word) >=4 else word,\n",
    "                'word[-4:]': word[-4:] if len(word) >=4 else word,\n",
    "            })\n",
    "        \n",
    "        # ========================\n",
    "        # 2. MORFOLOGÍA Y MAYÚSCULAS\n",
    "        # ========================\n",
    "        if feature_selection['morphology']:\n",
    "            features.update({\n",
    "                'hyphen': '-' in word,\n",
    "                'dot': '.' in word,\n",
    "                'has_digit': any(c.isdigit() for c in word),\n",
    "                'shape': ''.join([\n",
    "                    'A' if c.isalpha() else \n",
    "                    'N' if c.isdigit() else \n",
    "                    'S' if c in {'-', '/', ':'} else 'O' \n",
    "                    for c in word\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        if feature_selection['case_patterns']:\n",
    "            features.update({\n",
    "                'all_caps': word == word.upper(),\n",
    "                'mixed_case': word != word.lower() and word != word.upper(),\n",
    "                'capital_inside': word[1:] != word[1:].lower() if len(word) >1 else False\n",
    "            })\n",
    "        \n",
    "        # ====================\n",
    "        # 3. PATRONES TEMPORALES/NUMÉRICOS\n",
    "        # ====================\n",
    "        if feature_selection['date_time_patterns']:\n",
    "            features.update({\n",
    "                'is_date': any(c in word for c in {'/', '-', ':'}) and any(c.isdigit() for c in word),\n",
    "                'has_year': (len(word) == 4 and word.isdigit() and 1900 <= int(word) <= 2100)\n",
    "            })\n",
    "        \n",
    "        if feature_selection['numeric_patterns']:\n",
    "            features.update({\n",
    "                'has_currency': any(c in word for c in {'€', '$', '£'}),\n",
    "                'is_percentage': '%' in word,\n",
    "                'is_numeric': any(c.isdigit() for c in word) and any(c in {',', '.', ':'} for c in word)\n",
    "            })\n",
    "\n",
    "\n",
    "                # 4. Prefijos y sufijos extendidos\n",
    "        if feature_selection['prefix_suffix']:\n",
    "            for n in [2,3,4]:\n",
    "                if len(word) >= n:\n",
    "                    features[f'prefix{n}'] = word[:n]\n",
    "                    features[f'suffix{n}'] = word[-n:]\n",
    "        \n",
    "        # ====================\n",
    "        # 4. CONTEXTO INMEDIATO\n",
    "        # ====================\n",
    "        if feature_selection['context']:\n",
    "            context_window = []\n",
    "            for i in [-2, -1, 1, 2]:  # Ventana ampliada\n",
    "                if 0 <= index+i < len(sentence):\n",
    "                    ctx_word = sentence[index+i][0]\n",
    "                    context_window.append(ctx_word.lower())\n",
    "            \n",
    "            features.update({\n",
    "                'prev_word': sentence[index-1][0].lower() if index >0 else '<START>',\n",
    "                'next_word': sentence[index+1][0].lower() if index < len(sentence)-1 else '<END>',\n",
    "                'context_bag': ' '.join(context_window)\n",
    "            })\n",
    "        \n",
    "        # ====================\n",
    "        # 5. CARACTERÍSTICAS IDIOMÁTICAS\n",
    "        # ====================\n",
    "        if feature_selection['language_specific']:\n",
    "            # Para español\n",
    "            features.update({\n",
    "                'has_accent': any(c in 'áéíóúñ' for c in word.lower()),\n",
    "                'common_ending_es': word[-2:] in {'os', 'as', 'es', 'ía'}\n",
    "            })\n",
    "            \n",
    "            # Para neerlandés (ejemplo)\n",
    "            if any(c in {'ij', 'ee', 'uu'} for c in word.lower()):\n",
    "                features['dutch_diacritics'] = True\n",
    "        \n",
    "        # ====================\n",
    "        # 6. ORTOGRAFÍA COMPLEJA\n",
    "        # ====================\n",
    "        if feature_selection['orthographic_features']:\n",
    "            ortho_patterns = {\n",
    "                'ALPHANUM': re.match(r'^(?=.*[A-Za-z])(?=.*\\d)', word),\n",
    "                'UPPER_DIGIT': re.match(r'^[A-Z0-9-]+$', word),\n",
    "                'CAMEL_CASE': re.match(r'^[A-Z][a-z]+([A-Z][a-z]+)+$', word)\n",
    "            }\n",
    "            for pattern, match in ortho_patterns.items():\n",
    "                if match:\n",
    "                    features[f'ortho_{pattern}'] = True\n",
    "        \n",
    "        # ====================\n",
    "        # 7. METADATOS POSICIONALES\n",
    "        # ====================\n",
    "        if feature_selection['position']:\n",
    "            features.update({\n",
    "                'position': index,\n",
    "                'is_first_3': index < 3,\n",
    "                'is_last_3': index > len(sentence)-4\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    return feature_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a34f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melbourne\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_form\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefix_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     19\u001b[0m ct \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mCRFTagger(feature_func\u001b[38;5;241m=\u001b[39mgenerate_features(feature_config))\n\u001b[1;32m---> 20\u001b[0m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnooooooolapolitziiaa.mdl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m ct\u001b[38;5;241m.\u001b[39maccuracy(test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\tag\\crf.py:186\u001b[0m, in \u001b[0;36mCRFTagger.train\u001b[1;34m(self, train_data, model_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m    185\u001b[0m     tokens, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msent)\n\u001b[1;32m--> 186\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens))]\n\u001b[0;32m    187\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mappend(features, labels)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Now train the model, the output should be model_file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m, in \u001b[0;36mgenerate_features.<locals>.feature_function\u001b[1;34m(sentence, index)\u001b[0m\n\u001b[0;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence[index])\n\u001b[1;32m----> 7\u001b[0m word,pos \u001b[38;5;241m=\u001b[39m sentence[index]\n\u001b[0;32m      8\u001b[0m symbols \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m€\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m£\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m¥\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      9\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "feature_config = {\n",
    "    'word_form': True,\n",
    "    'prefix_suffix': True,\n",
    "    'lemma_pos_tags': True,  # Solo si el corpus incluye POS tags\n",
    "    'sentence_position': True,\n",
    "    'language_specific': True,\n",
    "    'case_patterns': True,\n",
    "    'date_time_patterns': True,\n",
    "    'numeric_patterns': True,\n",
    "    'symbol_patterns': True,\n",
    "    'length': True,\n",
    "    'morphology': True,\n",
    "    'context': True,\n",
    "    'extended_context': True,\n",
    "    'orthographic_features': True,\n",
    "    'common_ending': True,\n",
    "    'position': True,\n",
    "}\n",
    "ct = nltk.tag.CRFTagger(feature_func=generate_features(feature_config))\n",
    "ct.train(train[:100], \"nooooooolapolitziiaa.mdl\")\n",
    "ct.accuracy(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
